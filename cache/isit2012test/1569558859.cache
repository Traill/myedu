{"id":"1569558859","paper":{"title":{"text":"Tackling Intracell Variability in TLC Flash Through Tensor Product Codes"},"authors":[{"name":"Ryan Gabrys ∗"},{"name":"Eitan Yaakobi \u2020\u2021"},{"name":"Laura Grupp \u2020"},{"name":"Steven Swanson \u2020"},{"name":"Lara Dolecek ∗"}],"abstr":{"text":"Abstract\u2014Flash memory is a promising new storage technol- ogy. To fully utilize future multi-level cell Flash memories, it is necessary to develop error correction coding schemes attuned to the underlying physical characteristics of Flash. Based on a careful inspection of ﬁne-grained, experimentally-collected error patterns of TLC (three bits per cell) Flash, we propose a mathematical model that captures the intracell variability, which is manifested by certain patterns of bit-errors. Error correction codes are constructed for this model based upon generalized tensor product codes. For ﬁxed levels of redundancy, these codes are shown to exhibit substantially lower bit error rates than existing error correction schemes."},"body":{"text":"Flash memory devices can be found almost everywhere today. They are lighter, faster and more shock resistant than traditional magnetic hard drives. As this technology scales and the storage density increases, data errors become more preva- lent, making error correction coding critical for maintaining data integrity.\nThe storage density of a Flash memory device is dependent on the number of discrete voltage levels the ﬂoating gate cell is capable of representing. In early generations, every memory cell could represent two voltage levels and thus store a single bit (SLC). The demand for increased storage capacity has created the need to store more than a single bit per cell by simply representing more than two voltage levels. In this work, we follow the commonly adopted nomenclature and assume that multiple level cell (MLC) chips store two bits per cell, and that triple level cell (TLC) chips store three bits per cell.\nRecently, the subject of error-correction coding for Flash memory has received signiﬁcant attention. In [5], trellis coded modulation techniques were applied to Flash memory. In [8], the use of non-binary LDPC codes was investigated. In [6], algebraic error-correction codes were used for rewriting as well as correcting errors. In [1], [4], codes that correct limited magnitude asymmetric errors were constructed. In [12], this model was extended to correct graded error patterns.\nThe error model in this work is motivated by data collected from a TLC Flash device. As observed in [13], if the infor- mation from each Flash cell is interpreted as a triple-bit word, then the errors largely cause only a single bit in each word to change. From this observation, we suggest the use of a new class of codes derived from tensor product codes [7], [11] in the context of Flash memory. This work generalizes the result of [13] to correct errors that mostly have only a small number of bits in error for each cell-error. The technique used to address the problem is based on the generalized tensor product (GTP) scheme proposed in [7].\nTensor product codes were ﬁrst introduced in [11] and were generalized to produce efﬁcient binary codes in [7]. More recently, tensor product codes were used in the context of\nmagnetic recording [2], [3]. In a concatenated coding scheme, the use of a tensor product parity code as the inner code was shown to offer the performance advantages of a short length parity code but without the associated rate penalty. In this work, it is shown that generalized tensor product codes can be used to efﬁciently correct the errors that occur within a TLC Flash device, and in turn extend the lifetime of a memory system. The main contributions are construction methods for codes that correct up to t 1 symbol errors with up to l 1 bit errors and t 2 symbol errors with up to l 2 bit errors.\nIn Section II, the data collected from a TLC Flash chip is summarized. In Section III, the error model, motivated by the experimental data, is proposed. In Section IV, code constructions for this model are given. In Section V, these constructions are shown through simulation to be superior to commonly used storage codes. Section VI concludes the paper.\nIn this section, we report on the observed errors measured from a TLC chip provided by an anonymous vendor. A TLC chip is divided into multiple planes. Each plane is divided into a set of blocks and these blocks are further decomposed into pages. For the particular TLC chip measured, there are 384 pages within a block and 8 kilobytes (KB) within a page. The eight discrete voltage levels from the cell are represented as a triple-bit word. We refer to the ﬁrst bit in the word as the most signiﬁcant bit (MSB), the second bit in the word as the center signiﬁcant bit (CSB), and the third bit in the word as the least signiﬁcant bit (LSB). For more details on the structure of a TLC chip, see [13].\nThe errors were measured from sixteen blocks evenly divided across two planes. The following testing procedure was repeatedly performed. On the ﬁrst cycle of every 100 program/erase (P/E) cycles, a block was erased, and random data was then written and ﬁnally read back for errors. On the other 99 cycles, the block was simply erased and the memory was programmed to simulate the aging of the device.\nIn Figure 1, the Bit Error Rate (BER) is illustrated for the TLC chip tested over the course of its lifetime. It can be seen that over time, the BER increases dramatically but at different rates depending on which bit is programmed. The \u2019Symbol Error Rate\u2019 plot refers to the symbol error rate when each cell is represented as a symbol over GF (8).\nThe dominant trend from Figure 1 is that the \u2019Symbol Error Rate\u2019 appears to be roughly the sum of the individual BERs of the MSB, CSB, and LSB. This suggests that whenever a cell-error occurs, with high probability only one of the three bits in the cell errs. More speciﬁcally, 96.17% of cell-errors only had a single bit in error. This is a result of the special\nprogramming property of the bits where the three bits are not programmed all at once. More details on this phenomena are reported in [13]. Note that this error model is considerably different than the one of asymmetric limited magnitude errors, studied in many previous works, e.g., [1] and [4].\nThe new codes introduced in this paper correct errors that mostly affect a single bit within each cell-error. In addition, these new codes also have the special property that they can correct the remaining few cell-errors with two or three bit- errors. \t III. M\nIn this section, the relevant error models as well as code deﬁnitions are given.\nDeﬁnition 1. A linear code C of length n and dimension k over an alphabet of size q that can correct t errors is referred to as an [n, k, t] q code.\nAll codes considered in this work have alphabets whose cardinality is q = 2 m where m is some positive integer. Each cell can take on 2 m possible values and is displayed as an m- bit vector. Thus, a word of length n is represented as a length- nm binary vector where bits mi, . . . , m(i + 1) − 1 represent the ith cell for 0 ≤ i ≤ n − 1.\nAccordingly, every cell-error is represented as a length-m vector e i . For a ﬁxed , if wt(e i ) ≤ then such an error is called an -bit-cell-error, where the Hamming weight of a vector x is denoted by wt(x). Motivated by the nature of the errors observed, it is useful to deﬁne the following class of error-vectors and codes.\nDeﬁnition 2. Given the parameters t and , an error-vector e = (e 0 , e 1 , . . . , e n−1 ) over (GF (2) m ) n is called a [t; ] 2 m - bit-error-vector if the following holds:\nDeﬁnition 3. A 2 m -ary linear code C that can correct any [t; ] 2 m -bit-error-vector is called a [t; ] 2 m -bit-error- correcting code.\nFrom the data collected from the TLC ﬂash device, it was observed that while most cell-errors suffered a single bit-error, only a small number of cells had double or triple bit-errors. Therefore, to correct all observed errors, it is useful to deﬁne the following reﬁned error-vectors and corresponding codes.\nDeﬁnition 4. Let 0 < 1 < 2 ≤ m, t 1 , t 2 > 0. Then a vector e = (e 0 , e 1 , . . . , e n−1 ) over (GF (2) m ) n is called a\n[t 1 , t 2 ; 1 , 2 ] 2 m -bit-error-vector if the following holds: 1) wt(e) = |{i : e i = 0}| ≤ t 1 + t 2 . 2) ∀i, wt(e i ) ≤ 2 .\nDeﬁnition 5. Let 0 < 1 < 2 ≤ m, t 1 , t 2 > 0. Then a 2 m -ary code C is said to be a [t 1 , t 2 ; 1 , 2 ] 2 m -bit-error-correcting code if it can correct any [t 1 , t 2 ; 1 , 2 ] 2 m -bit-error-vector.\nThe next deﬁnition is useful in determining the parity-check matrices of bit-error-correcting codes.\nDeﬁnition 6. Let A ∈ GF (q) m×n , B ∈ GF (q) p×r . Then the tensor product of A and B is deﬁned as the matrix\n \na 0,0 B \t . . . \t a 0,n−1 B .. . \t . . . \t .. .\n \nIn this section, code constructions are given for bit-error- correcting codes. The section begins by revisiting a result from [11] that can be used to create [t; ] 2 m -bit-error-correcting codes. This idea is extended to create [t 1 , t 2 ; 1 , 2 ] 2 m -bit- error-correcting codes. Any error pattern a [t 1 , t 2 ; 1 , 2 ] 2 m code can correct is also correctable by a [t 1 +t 2 ; 2 ] 2 m code. It is shown that the former code has better redundancy whenever\nIn [11], it was shown that the tensor product of two parity check matrices results in a code that can correct a prescribed number of errors of a pre-deﬁned type. For example, suppose a code with a parity check matrix H 1 ∈ GF (2) r 1 ×m corrects all burst errors of length 2 and a code with a parity check matrix H 2 ∈ (GF (2) r 1 ) r 2 ×n corrects any 3 symbol errors. Then H 2 ⊗H 1 is a parity check matrix of a code of length nm bits, partitioned into n m-bit blocks. This code corrects any 3 block-errors assuming each block-error is a burst of length 2. In Construction A, this result is stated more formally.\nWe start by presenting a construction of [t; ] 2 m -bit-error- correcting codes.\nConstruction A. (see ﬁrst [11]) Let C 1 be an [m, k 1 , ] 2 code with a parity check matrix H 1 . Let C 2 be an [n, k 2 , t] 2 m−k1 code with a parity check matrix H 2 . Then, the code C A with the parity matrix\nThe correctness of the error-correction capability was proved in [11]. Furthermore, since the parity check matrix of the code C A is the tensor product of the matrices H 1 and H 2 , and rank(H 2 ⊗ H 1 ) = rank(H 2 ) · rank(H 1 ), we get that the redundancy of the code C A is r 1 r 2 , where r 1 = m − k 1 and r 2 = n − k 2 . An example of the encoding of such codes was given in [10]. Suppose c ∈ C A , where c = (c 0 , . . . , c n−1 ) ∈ (GF (2) m ) n . Then,\n \n \n \nh 0,0 \t . . . h 0,n−1 .. . . . . \t .. .\n \n \n \nwhere h i,j represents the symbol in position row i, column j of H 2 . Thus, c ∈ C A if and only if (H 1 · c 0 T , . . . , H 1 · c n−1 T ) ∈ C 2 and the code C A can be expressed as follows:\nC A = {c = (c 0 , . . . , c n−1 ) ∈ (GF (2) m ) n : (H 1 · c T 0 , . . . , H 1 · c T n−1 ) ∈ C 2 }.\nFor completeness and for the subsequent discussion, let us describe here a decoder for the code C A . Let\nThe decoder D A : ({0, 1} m ) n → ({0, 1} m ) n of the code C A gets as an input a word of the form y = c + e, where c ∈ C A and e ∈ (GF (2) m ) n is a [t; ] 2 m -bit-error-vector. The output of the decoder is the estimate of the error vector:\nProof: According to the deﬁnition of the code C A we have (H 1 · c T 0 , . . . , H 1 · c T n−1 ) ∈ C 2 and we can write\nThe vector (H 1 · e T 0 , . . . , H 1 · e T n−1 ) has weight at most t and since C 2 can correct t errors we get that\nand since s i = H 1 · e T i and the weight of e i is at most , we get that D 1 (s i ) = e i , that is, e = e. B. Construction B\nThe codes given in Construction A correct error patterns according to the maximum number of bit-errors in every cell (or m-bit symbol). Construction B extends this idea so that, while most cells suffer a small number of bit-errors, relatively few cell-errors may occur with a larger number of bit-errors. We capture this property in the following construction of [t 1 , t 2 ; 1 , 2 ] 2 m -bit-error-correcting codes.\nConstruction B. Let C 1 be an [m, k, 2 ] 2 code with a parity check matrix H 1 , and let r = m − k be such that the following holds:\n1) There exists 0 ≤ r < r such that the matrix H 1 comprised of the ﬁrst r rows of H 1 is a parity check matrix for an [m, m − r , 1 ] 2 code C 1 .\n2) H 1 is an r × m matrix consisting of the last r rows of H 1 , where r = r − r .\n3) H 2 is a parity check matrix for an [n, k 2 , t 1 +t 2 ] 2 r code C 2 , and r 2 = n − k 2 .\n4) H 3 is a parity check matrix for an [n, k 3 , t 2 ] 2 r code C 3 , and r 3 = n − k 3\nThen, a parity check matrix for a [t 1 , t 2 ; 1 , 2 ] 2 m -bit-error- correcting code C B of length n is\nRemark 1. The parity check matrix H 1 of the 2 -error- correcting code C 1 needs to satisfy the property that it can be decomposed into two matrices, where the ﬁrst matrix is a parity check matrix of an 1 -error-correcting code C 1 . We note this requirement is not hard to satisfy as many codes can follow this structure, and in particular BCH codes.\nWe ﬁrst present an example followed by the decoder for C B before the error-correction ability is proven.\n1 0 1 0 1 1 0 0 1\nr = 2 so that H 1 = 1 0 1 0 1 1 where H 1 is a parity check matrix for a [3, 1, 1] 2 Hamming code and H 1 = ( 0 0 1 ). Let C 2 be a [15, 9, 2] 4 code with a parity check matrix H 2 . Furthermore, let C 3 be a [15, 11, 1] 2 Hamming code with a parity check matrix H 3 . Then using Construction B, the code C B has a parity check matrix\nH B is the parity check matrix for a [1, 1; 1, 3] 2 3 -bit-error- correcting code. The particular choice of C 1 in this example results in the same code that was proposed in [13].\n· c 0 T , . . . , H 1 · c n−1 T T H 3 · H 1 · c 0 T , . . . , H 1 · c n−1 T T\nC B = {c = (c 0 , . . . , c n−1 ) ∈ (GF (2) m ) n : (H 1 · c T 0 , . . . , H 1 · c T n−1 ) ∈ C 2 ,\nand its redundancy is at most r r 2 + r r 3 . Let us denote\nD 1 : {0, 1} r → {0, 1} m , D 1 : {0, 1} r → {0, 1} m , D 2 : ({0, 1} r ) r 2 → ({0, 1} r ) n ,\nto be the decoder of the code C 1 , C 1 , C 2 , C 3 , respectively. As before, the input to all these encoders is the syndrome and the output is the error vector whose weight is no greater than the guaranteed error-correction capability of the corresponding code.\nBefore presenting the decoder\u2019s steps, let us explain the idea behind this construction and its decoding procedure. We start in a similar fashion to the decoder in Construction A, where at most t 1 + t 2 cell-errors each of weight at most 1 are found. Clearly, it may not possible to correct all cell-errors this way. If a cell-error has weight at most 1 then it is corrected. Otherwise, it is miscorrected to a cell-error vector, with weight at most 1 + 2 since the weight of each miscorrection has been restricted to be 1 . This, in turn, guarantees that the new cell-error vector is not a codeword in C 1 , since its minimum distance is at least 2 2 +1. Thus, the next step is to detect these cells which were miscorrected. For cell-errors with more than\n1 bits in error, the remaining part of the syndrome according to the code C 1 is recovered. The decoder D 1 is then used to recover the remaining errors.\nThe decoder D B : ({0, 1} m ) n → ({0, 1} m ) n of the code C B gets as an input a word of the form y = c + e, where c ∈ C B and e ∈ (GF (2) m ) n is a [t 1 , t 2 ; 1 , 2 ] 2 m -bit-error- vector. Its output is an estimate of the error vector D B (y) = e. The decoder D B operates as follows:\n9) e = (e 0 , . . . , e n−1 ) where e i = e ∗ i if i / ∈ I and otherwise e i = D 1 (s 0 i , s 1 i ).\nProof: Let y = c + e be the received word to the decoder D B where c ∈ C B and e ∈ (GF (2) m ) n is a [t 1 , t 2 ; 1 , 2 ] 2 m - bit-error-vector. Then according to the deﬁnition of the code C B , (H 1 · c T 0 , . . . , H 1 · c T n−1 ) ∈ C 2 and\nThe vector (H 1 ·e T 0 , . . . , H 1 ·e T n−1 ) now has weight at most t 1 + t 2 and since the code C 2 can correct this number of errors we get that (s 0 0 , . . . , s 0 n−1 ) = (H 1 · e T 0 , . . . , H 1 · e T n−1 ) after step 1.\nAt step 2 since for every 0 ≤ i ≤ n − 1, H 1 · y T i = H 1 · c T i + H 1 · e T i , if wt(e i ) ≤ 1 ,\nas C 1 corrects 1 errors. However, if the weight of e i is between 1 + 1 and 2 then e ∗ i = D 1 (s 0 i ) = e i . This observation results from the fact that the decoder C 1 can only return a cell-error vector of weight at most 1 . In particular, we get that wt(e ∗ ) ≤ t 2 and for all 0 ≤ i ≤ n − 1, wt(e ∗ i ) ≤ 1 + 2 . Thus, at the end of step 3, y contains\nno cell errors of weight less than 1 and all the remaining (at most t 2 ) cell-errors have weight at most 1 + 2 .\nSteps 4 and 5 compute the syndrome using y as input. Since the minimum distance of the code C 1 is at least 2 2 + 1 > 1 + 2 , we get that for all 0 ≤ i ≤ n − 1, if a miscorrection occurred, then e ∗ i is not a codeword in C 1 . Therefore, (s i , s i ) = (0, 0) and in step 6 the set I is the set of all 0 ≤ i ≤ n − 1 such that 1 < wt(e i ) ≤ 2 . In step 7, the word y is the word of y after removing all cell-errors of weight at most 1 .\nIn step 8 the remaining portion of the syndrome is recovered for all cell-errors with more than 1 bits in error. Lastly in step 9 for every cell-error at position i, if 1 or less bit-errors occurred then e ∗ i is its corresponding cell-error vector and if more bit-errors occurred then the decoder D 1 is used. Since C 1 can correct 2 errors and the syndrome H 1 · e i T = (s 0 i , s 1 i ) is known for all cell-errors with more than 1 bits in error, these errors are corrected as well.\nIt can be shown that C B requires less redundancy than C A approximately whenever log n log m < t 1 t\n( 2 − 1 ). The next construction reduces the required redundancy in certain cases when the ratio t 1 t\nConstruction C extends Construction B by using a combi- nation of codes whose abilities are to correct errors, correct erasures, and detect errors. In particular, the code C 1 in Construction B is modiﬁed such that it corrects 1 errors and detects when there are between 1 + 1 and 2 errors. Accordingly, the code C 3 in Construction B need only correct t 2 erasures instead of t 2 errors.\nConstruction C. Let C C be a code with the following modi- ﬁcations with respect to the code construction of C B :\n1) The code C 1 remains an [m, k, 2 ] 2 code as in Construc- tion B where r = m − k.\n2) The matrix H 1 now consists of the ﬁrst r rows of H 1 where\na) H 1 is a parity check matrix for an [m, m−r , 1 ] 2 - bit-error-correcting code C 1 ,\nb) The minimum distance of C 1 is at least 1 + 2 + 1 so is can detect an error vector of weight between\n3) H 1 is an r × m matrix consisting of the last r rows of H 1 , where r = r − r .\n4) H 2 is a parity check matrix for an [n, k 2 , t 1 +t 2 ] 2 r code C 2 , and r 2 = n − k 2 .\n5) H 3 is a parity check matrix of an [n, k 3 , t 2 2 ] 2 r code C 3 that can correct at least t 2 erasures, and r 3 = n−k 3 .\ndecoders of the codes C 1 and C 3 are also changed while the decoders for C 1 and C 2 remain the same as in Construction B. The decoder D 1 , in addition to correcting 1 errors, also detects if the number of errors is between 1 + 1 and 2 . The decoder is deﬁned\nwhere the symbol E indicates a detected error of weight between 1 + 1 and 2 . Note that now the decoder D 1 never miscorrects when the number of errors in each cell is at most\n2 . The input to the decoder D 3 is no longer a syndrome but a vector x = (x 0 , . . . , x n−1 ) with at most t 2 erasures\n6) Let x satisfy: x i = ? if i ∈ I and x i = H 1 · y i T if i / ∈ I.\n8) e = (e 0 , . . . , e n−1 ) where e i = e ∗ i if i / ∈ I and otherwise e i = D 1 (s 0 i , s 1 i + H 1 · y i T ).\nThe proof of the decoder correctness of Construction C is omitted due to a lack of space. It can be shown that Construction C requires less redundancy than Construction A approximately when log n log m < ( t 1 t\n+ 1 2 )( 2 − 1 ). Furthermore, it requires less redundancy than Construction B roughly when\nIn this section, the performance of various linear error- correcting codes with guaranteed error-correction capability is evaluated for the TLC Flash device. The results of these simulations are shown in Figure 2. All the known codes used were the best known linear codes according to [9] of the longest block length.\nThe ﬁrst code shown in the legend of Fig. 2 is a non-binary [128, 116, 3] 8 code with rate 0.906 where each 8-ary symbol corresponds to an 8-ary cell. The second code in the ﬁgure is the result of applying a [255, 232, 3] 2 BCH code three times to each of the 3 bits in each cell.\nThe rate 0.904 code labeled \u2018Scheme A\u2019 is comprised of a non-binary [256, 227, 5] 4 code applied to the LSB and CSB\nbits for each Flash memory cell. Next, an independent binary [256, 240, 2] 2 code was used to protect the remaining bit of information from each cell. This scheme was designed to target the property observed in Section II where the CSB and LSB were more likely to err than the MSB.\nThe [3, 2; 1, 3] 8 -bit-error-correcting code of length 256 with rate 0.904 was generated according to Construction B. In this case, C 1 is the [3, 1, 1] 2 binary repetition code. The code C 2 is the same [256, 227, 5] 4 code used in Scheme A and C 3 is the same [256, 240, 2] 2 code used in Scheme A as well. For reference, we included a [7; 1] 2 3 bit-error code of length 256 and rate 0.83 (constructed using one tensor product operation). From Fig. 2 this particular tensor product code has the ability to delay the appearance of any errors in the system by a factor of 4 over the naive GF (8) code. In addition, the proposed tensor product code offers a 1.6x lifetime improvement over the popular BCH codes.\nIn this work, data from a TLC Flash device demonstrated that when errors occur within a Flash cell, the vast majority of such errors only affect one of the 3 bits of information. This observation was used to motivate a new error-correction model for Flash memory. Error-correcting code constructions based upon generalized tensor product codes were provided that were analytically and empirically shown to offer a potentially valuable component for future coding schemes in the context of Flash memory.\nResearch supported in part by SMART scholarship, and NSF grants CCF-1029030 and CCF-1150212."},"refs":[{"authors":[{"name":"Y. Cassuto et al"}],"title":{"text":"Codes for asymmetric limited-magnitude errors with application to multi-level ﬂash memories, IEEE Trans"}},{"authors":[{"name":"P. Chaichanavong"},{"name":"P. H. Siegel"}],"title":{"text":"A tensor-product parity code for magnetic recording"}},{"authors":[{"name":"P. Chaichanavong"},{"name":"P. H. Siegel"}],"title":{"text":"Tensor-product parity codes: combi- nation with constrained codes and application to perpendicular recording"}},{"authors":[{"name":"N. Elarief"},{"name":"B. Bose"}],"title":{"text":"Optimal, systematic, q-ary codes correcting all asymmetric and symmetric errors of limited magnitude"}},{"authors":[{"name":"S. Fei et al."}],"title":{"text":"Multilevel Flash memory on-chip error correction based on trellis coded modulation"}},{"authors":[{"name":"Q. Huang"},{"name":"S. Lin"},{"name":"K. A. S. Abdel-Ghaffar"}],"title":{"text":"Error-correcting codes for ﬂash coding"}},{"authors":[{"name":"H. Imai"},{"name":"H. Fujiya"}],"title":{"text":"Generalized tensor product codes"}},{"authors":[{"name":"Y. Maeda"},{"name":"H. Kaneko"}],"title":{"text":"Error control coding for multilevel cell ﬂash memories using nonbinary low-density parity-check codes"}},{"authors":[],"title":{"text":"Magma Computational Algebra System"}},{"authors":[{"name":"J. K. Wolf"}],"title":{"text":"An introduction to tensor product codes and applications to digital storage systems"}},{"authors":[{"name":"J. K. Wolf"}],"title":{"text":"On codes derivable from the tensor product of check matrices"}},{"authors":[{"name":"E. Yaakobi et al."}],"title":{"text":"On codes that correct asymmetric errors with graded magnitude distribution"}},{"authors":[{"name":"E. Yaakobi et al."}],"title":{"text":"Characterization and error-correcting codes for TLC ﬂash memories"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569558859.pdf"},"links":[{"id":"1569559259","weight":19},{"id":"1569559541","weight":11},{"id":"1569559221","weight":16},{"id":"1569558785","weight":18},{"id":"1569559565","weight":43},{"id":"1569558681","weight":11},{"id":"1569559195","weight":21},{"id":"1569566489","weight":13},{"id":"1569558901","weight":76},{"id":"1569559111","weight":25},{"id":"1569558985","weight":19},{"id":"1569558509","weight":14},{"id":"1569565705","weight":21},{"id":"1569551347","weight":28},{"id":"1569559199","weight":16},{"id":"1569559035","weight":6},{"id":"1569558779","weight":19},{"id":"1569559523","weight":12},{"id":"1569559597","weight":21},{"id":"1569559251","weight":23},{"id":"1569550425","weight":19},{"id":"1569564509","weight":5},{"id":"1569558697","weight":14},{"id":"1569559233","weight":20}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T5.3","endtime":"12:30","authors":"Ryan Gabrys, Eitan Yaakobi, Laura M Grupp, Steven Swanson, Lara Dolecek","date":"1341317400000","papertitle":"Tackling Intracell Variability in TLC Flash Through Tensor Product Codes","starttime":"12:10","session":"S6.T5: Coding for Flash Memories","room":"Kresge Little Theatre (035)","paperid":"1569558859"},"cluster":{"jsonClass":"Map$EmptyMap$"}}

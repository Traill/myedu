{"id":"1569551347","paper":{"title":{"text":"Moderate Deviations Analysis of Binary Hypothesis Testing"},"authors":[{"name":"Igal Sason"}],"abstr":{"text":"Abstract\u2014This work refers to moderate-deviations analysis of binary hypothesis testing. It relies on a concentration inequality for discrete-parameter martingales with bounded jumps, which forms a reﬁnement to the Azuma-Hoeffding inequality. Relations of the analysis to the moderate deviations principle for i.i.d. random variables and the relative entropy are considered.\nIndex Terms\u2014Concentration inequalities, hypothesis testing, moderate deviations principle."},"body":{"text":"The moderate deviations analysis in the context of source and channel coding has recently attracted some interest among information theorists (see [1], [4], [11], [16], [19] and [22]). The purpose of this paper is to consider moderate deviations analysis for binary hypothesis testing.\nIn the following, related literature on moderate deviations analysis in information-theoretic aspects is shortly reviewed. Moderate deviations were analyzed in [1, Section 4.3] for a channel model that gets noisier as the block length is increased. Due to the dependence of the channel parameter in the block length, the usual notion of capacity for these channels is zero. Hence, the issue of increasing the block length for the considered type of degrading channels was examined in [1, Section 4.3] via moderate deviations analysis when the number of codewords increases sub-exponentially with the block length. In another recent work [4], the moderate deviations behavior of channel coding for discrete memoryless channels was studied by Altug and Wagner with a derivation of direct and converse results which explicitly characterize the rate function of the moderate deviations principle (MDP). In [4], the authors studied the interplay between the probability of error, code rate and block length when the communication takes place over discrete memoryless channels, having the interest to ﬁgure out how the decoding error probability of the best code scales when simultaneously the block length tends to inﬁnity and the code rate approaches the channel capacity. The novelty in the setup of their analysis was the consideration of the scenario mentioned above, in contrast to the case where the rate is kept ﬁxed below capacity, and the study is reduced to a characterization of the dependence between the two remaining parameters (i.e., the block length n and the average/ maximal error probability of the best code). As opposed to the latter case when the code rate is kept ﬁxed, which then corresponds to large deviations analysis and characterizes the error exponents as a function of the rate, the analysis in [4] (via the introduction of direct and converse theorems) demonstrated a sub-exponential scaling of the maximal error probability in the considered moderate\ndeviations regime. This work was followed by a work by Polynaskiy and Verd´u where they show that a DMC satisﬁes the MDP if and only if its channel dispersion is non-zero, and also that the AWGN channel satisﬁes the MDP with a constant that is equal to the channel dispersion. The approach used in [4] was based on the method of types, whereas the approach used in [17] borrowed some tools from a recent work by the same authors in [16].\nIn [11], the moderate deviations analysis of the Slepian- Wolf problem for lossless source coding was studied. More recently, moderate deviations analysis for lossy source coding of stationary memoryless sources was studied in [22].\nThese works, including this paper, indicate a recent interest in moderate deviations analysis in the context of information- theoretic problems. In the literature on probability theory, the moderate deviations analysis was extensively studied (see, e.g., [10, Section 3.7]), and in particular the MDP was studied in [9] for continuous-time martingales with bounded jumps.\nThis paper has the following structure: Section II introduces brieﬂy some preliminary material related to martingales and Azuma\u2019s inequality. It then follows by introducing a reﬁned version of Azuma\u2019s inequality, and a study of its relation to the moderate deviations principle for i.i.d. random variables. Section III considers the relation of Azuma\u2019s inequality and the reﬁned version of this inequality (from Section II) to moderate deviations analysis of binary hypothesis testing. Section IV concludes the paper, followed by a discussion on the MDP that is relegated to an appendix.\nWe present here some essential material that is related to the martingale approach used in this paper for the moderate- deviations analysis of binary hypothesis testing. A background on martingales is provided in, e.g., [23] where we only rely here on basic knowledge on martingales.\nAzuma\u2019s inequality 1 forms a useful concentration inequality for bounded-difference martingales [5]. In the following, this inequality is introduced. The reader is referred to, e.g., [6], [?, Chapter 2] and [15] for surveys on concentration inequalities for martingales (including a proof of this inequality).\nTheorem 1: [Azuma\u2019s inequality] Let {X k , F k } ∞ k=0 be a discrete-parameter real-valued martingale sequence (where\nF 0 ⊆ F 1 ⊆ . . . is called a ﬁltration). Assume that for every k ∈ N, the condition |X k − X k−1 | ≤ d k holds a.s. for some non-negative constants {d k } ∞ k=1 . Then\nThe concentration inequality stated in Theorem 1 was proved in [12] for independent bounded random variables, and it was later derived in [5] for bounded-difference martingales.\nTheorem 2: Let {X k , F k } ∞ k=0 be a discrete-parameter real- valued martingale. Assume that, for some constants d, σ > 0, the following two requirements are satisﬁed a.s.\nδ + γ 1 + γ\n1 + γ \t (2) where\nand D(p||q) p ln p q + (1 − p) ln 1−p 1−q for p, q ∈ [0, 1] is the divergence (a.k.a. relative entropy or Kullback-Leibler distance) between the two probability distributions (p, 1 − p) and (q, 1 − q). If δ > 1, then the probability on the left-hand side of (2) is equal to zero.\nC. Relation of Theorem 2 with the Moderate Deviations Principle for i.i.d. RVs\nAccording to the moderate deviations theorem (see, e.g., [10, Theorem 3.7.1]) in R, let {X i } n i=1 be a sequence of i.i.d. real-valued RVs such that Λ X (λ) = E[e λX i ] < ∞ in some neighborhood of zero, and also assume that E[X i ] = 0 and σ 2 = Var(X i ) > 0. Let {a n } ∞ n=1 be a non-negative sequence such that a n → 0 and na n → ∞ as n → ∞, and let\na n ln P(Z n ∈ Γ) ≤ lim sup\nwhere Γ 0 and Γ designate, respectively, the interior and closure sets of Γ.\nLet η ∈ ( 1 2 , 1) be an arbitrary ﬁxed number, and let {a n } ∞ n=1 be the non-negative sequence\nso that a n → 0 and na n → ∞ as n → ∞. Let α ∈ R + , and Γ (−∞, −α] ∪ [α, ∞). Note that, from (4),\nIt is demonstrated in Appendix A that, in contrast to Azuma\u2019s inequality, Theorem 2 gives an upper bound on the probability P \t n i=1 X i ≥ αn η (where n ∈ N and α ≥ 0) which coincides with the exact asymptotic limit in (6). The analysis in Appendix A provides another interesting link between Theorem 2 and a classical result in probability theory, which also emphasizes the signiﬁcance of the reﬁnements of Azuma\u2019s inequality.\nBinary hypothesis testing for ﬁnite alphabet models was analyzed via the method of types, e.g., in [7, Chapter 11] and [8]. It is assumed that the data sequence is of a ﬁxed length (n), and one wishes to make the optimal decision based on the received sequence and the Neyman-Pearson ratio test.\nLet the RVs X 1 , X 2 .... be i.i.d. ∼ Q, and consider two hypotheses:\n\u2022 H 1 : Q = P 1 . \u2022 H 2 : Q = P 2 .\nFor the simplicity of the analysis, let us assume that the RVs are discrete, and take their values on a ﬁnite alphabet X where P 1 (x), P 2 (x) > 0 for every x ∈ X .\ndesignate the log-likelihood ratio. By the strong law of large numbers (SLLN), if hypothesis H 1 is true, then a.s.\nwhere the above assumptions on the probability mass functions P 1 and P 2 imply that the relative entropies, D(P 1 ||P 2 ) and D(P 2 ||P 1 ), are both ﬁnite. Consider the case where for some ﬁxed constants λ, λ ∈ R that satisfy\none decides on hypothesis H 1 if L(X 1 , . . . , X n ) > nλ, and on hypothesis H 2 if L(X 1 , . . . , X n ) < nλ. Note that if\nλ = λ λ then a decision on the two hypotheses is based on comparing the normalized log-likelihood ratio (w.r.t. n) to a single threshold (λ), and deciding on hypothesis H 1 or H 2 if this normalized log-likelihood ratio is, respectively, above or below λ. If λ < λ then one decides on H 1 or H 2 if the normalized log-likelihood ratio is, respectively, above the upper threshold λ or below the lower threshold λ. Otherwise, if the normalized log-likelihood ratio is between the upper and lower thresholds, then an erasure is declared and no decision is taken in this case.\nthen α (1) n and β (1) n are the probabilities of either making an error or declaring an erasure under, respectively, hypotheses H 1 and H 2 ; similarly α (2) n and β (2) n are the probabilities of making an error under hypotheses H 1 and H 2 , respectively.\nLet π 1 , π 2 ∈ (0, 1) denote the a-priori probabilities of the hypotheses H 1 and H 2 , respectively, so\nBased on the asymptotic results in (7) and (8), which hold a.s. under hypotheses H 1 and H 2 respectively, the large deviations analysis refers to upper and lower thresholds λ and λ which are kept ﬁxed (i.e., these thresholds do not depend on the block length n of the data sequence) where\nSuppose that instead of having some ﬁxed upper and lower thresholds, one is interested to set these thresholds such that as the block length n tends to inﬁnity, they tend simultaneously to their asymptotic limits in (7) and (8), i.e.,\nSpeciﬁcally, let η ∈ ( 1 2 , 1), and ε 1 , ε 2 > 0 be arbitrary ﬁxed numbers, and consider the case where one decides on hypothesis H 1 if L(X 1 , . . . , X n ) > nλ (n) , and on hypothesis H 2 if L(X 1 , . . . , X n ) < nλ (n) where these upper and lower thresholds are set to\nso that they approach, respectively, the relative entropies D(P 1 ||P 2 ) and −D(P 2 ||P 1 ) in the asymptotic case where the block length n of the data sequence tends to inﬁnity.\nAccordingly, the conditional probabilities in (9)\u2013(12) are modiﬁed so that the ﬁxed thresholds λ and λ are replaced with the above block-length dependent thresholds λ (n) and λ (n) , respectively. The moderate deviations analysis for binary hypothesis testing studies the probability of an error event and the probability of a joint error and erasure event under the two hypotheses, and it studies the interplay between each of these probabilities, the block length n, and the related thresholds that tend asymptotically to the limits in (7) and (8) when the block length tends to inﬁnity.\nIn light of the discussion in Section II-C on the MDP for i.i.d. RVs and the discussion of its relation to Theorem 2 (see Appendix A), and also motivated by the three recent works in [1, Section 4.3], [4] and [11], we proceed to consider in the following moderate deviations analysis for binary hypothesis testing. Our approach for this kind of analysis is different, and it relies on concentration inequalities for martingales.\nIn the following, we analyze the probability of a joint error and erasure event under hypothesis H 1 , i.e., derive an upper bound on α (1) n in (9). The same kind of analysis can be adapted easily for the other probabilities in (10)\u2013(12).\nUnder hypothesis H 1 , let us construct the martingale se- quence {U k , F k } n k=0 where F 0 ⊆ F 1 ⊆ . . . F n is the ﬁltration\nln P 1 (X i ) P 2 (X i )\nln P 1 (X i ) P 2 (X i )\nP 1 (X i ) P 2 (X i )\nso d 1 < ∞ since by assumption the alphabet set X is ﬁnite, and P 1 (x), P 2 (x) > 0 for every x ∈ X . From (18) and (19), |U k − U k−1 | ≤ d 1 a.s. for every k ∈ {1, . . . , n}, and due to the statistical independence of {X i }\nP 1 (x) ln P 1 (x) P 2 (x)\nLet ε 1 > 0 and η ∈ ( 1 2 , 1) be two arbitrarily ﬁxed numbers. Then, under hypothesis H 1 , it follows from Theorem 2 and the above construction of a martingale that\nP n 1 L(X 1 , . . . , X n ) ≤ nλ (n) ) = P n 1 U n − U 0 ≤ −ε 1 n η\n+ γ 1 1 + γ 1\nIn the following, we will make use of the following lemma: Lemma 1:\nwhere at u = −1, the left-hand side is deﬁned to be zero (it is the limit of this function when u → −1 from above).\n+ γ 1 1 + γ 1\nprovided that δ (η,n) 1 \t < 1 (which holds for n ≥ n 0 for some n 0 \t n 0 (η, ε 1 , d 1 ) ∈ N that is determined from (22)). By substituting this lower bound on the divergence into (21), it follows that\nwith σ 2 1 in (20). From the analysis in Section II-C and Appendix A, it follows that the inequality for the asymptotic limit in (25) holds in fact with equality. To verify this, consider the real-valued sequence of i.i.d. RVs\nthat, under hypothesis H 1 , have zero mean and variance σ 2 1 . Since, by assumption, the sequence {X i } n i=1 are i.i.d., then\nand it follows from the one-sided version of the MDP in (6) that indeed (25) holds with equality. Moreover, Theorem 2 provides, via the inequality in (24), a ﬁnite-length result that enhances the asymptotic result for n → ∞.\nIn the considered setting of moderate deviations analysis for binary hypothesis testing, the upper bound on the probability α (1) n in (24), which refers to the probability of either making an error or declaring an erasure (i.e., making no decision) under the hypothesis H 1 , decays to zero sub-exponentially with the length n of the sequence. As mentioned above, based on the analysis in Section II-C and Appendix A, the asymptotic upper bound in (25) is tight. A completely similar moderate- deviations analysis can be also performed under the hypothesis H 2 . Hence, a sub-exponential scaling of the probability β (1) n in (11) of either making an error or declaring an erasure (where the lower threshold λ is replaced with λ (n) ) also holds under the hypothesis H 2 . These two sub-exponential decays to zero for the probabilities α (1) n and β (1) n , under hypothesis H 1 or H 2 respectively, improve as the value of η ∈ ( 1 2 , 1) is increased. On the other hand, the two exponential decays to zero of the probabilities of error (i.e., α (2) n and β (2) n under hypothesis H 1 or H 2 , respectively) improve as the value of η ∈ ( 1 2 , 1) is decreased; this is due to the fact that, for a ﬁxed value of n, the margin which serves to protect us from making an error (either under hypothesis H 1 or H 2 ) is increased by decreasing the value of η as above (note that by reducing the value of η for a ﬁxed n, the upper and lower thresholds λ (n) and λ (n) are made closer to D(P 1 ||P 2 ) from below and to −D(P 2 ||P 1 ) from above, respectively, which therefore increases the margin that is used for protecting one from making an erroneous decision). This shows the existence of a tradeoff, in the choice of the parameter η ∈ ( 1 2 , 1), between the probability of error and the joint probability of error and erasure under either hypothesis H 1 or H 2 (where this tradeoff exists symmetrically for each of the two hypotheses).\nIn [4] and [17], the authors consider moderate deviations analysis for channel coding over memoryless channels. In particular, [4, Theorem 2.2] and [17, Theorem 6] indicate on a tight lower bound (i.e., a converse) to the asymptotic result in (25) for binary hypothesis testing. This tight converse is indeed consistent with the asymptotic result of the MDP in (6) for real-valued i.i.d. random variables, which implies that the asymptotic upper bound in (25), obtained via the martingale approach with the reﬁned version of Azuma\u2019s inequality in Theorem 2, holds indeed with equality. Note that this equality does not follow from Azuma\u2019s inequality, so its reﬁnement was essential for obtaining this equality. The reason is that, due to Appendix A, the upper bound in (25) that is equal to\n(note that, from (19) and (20), σ 1 ≤ d 1 where in general σ 1 may be signiﬁcantly smaller than d 1 ).\nThis paper is focused on the moderate deviations analysis of binary hypothesis testing. The analysis is based on a con- centration inequality for discrete-parameter martingales with bounded jumps, which forms a reﬁned version of Azuma\u2019s inequality (see [10, Corollary 2.4.7]). The relation of this concentration inequality to the moderate deviations principle for i.i.d. random variables is considered. This paper presents in part the work in [19], and it exempliﬁes the use of a reﬁnement of Azuma\u2019s inequality in an information-theoretic aspect. Further information-theoretic applications are considered in, e.g., [20] and [24]. The slides are available in [21].\nAcknowledgment : One of the reviewers pointed out that the moderate deviations analysis in this work can be done alternatively by relying on results, e.g., from [3] or [18]. We thank the reviewer for this note, and we currently study this line of work.\nIt is demonstrated in the following that, in contrast to Azuma\u2019s inequality, Theorem 2 provides an upper bound on P \t n i=1 X i ≥ αn η for α ≥ 0, which coincides with the correct asymptotic result in (6). It is proved under the further assumption that there exists some constant d > 0 such that |X k | ≤ d a.s. for every k ∈ N (since the RVs {X k } are assumed to be i.i.d., it is sufﬁcient to require it for k = 1). Let us deﬁne the martingale sequence {S k , F k } n k=0 where S k \t k i=1 X i and F k \t σ(X 1 , . . . , X k ) for every k ∈ {1, . . . , n} with S 0 = 0 and F 0 = {∅, F }.\n1) Analysis related to Azuma\u2019s inequality: The martingale sequence {S k , F k } n k=0 has uniformly bounded jumps, where |S k − S k−1 | = |X k | ≤ d a.s. for every k ∈ {1, . . . , n}. Hence it follows from Azuma\u2019s inequality that, for every α ≥ 0,\n2d 2 and therefore\n2d 2 . \t (27) This differs from the limit in (6) where σ 2 is replaced by d 2 , so Azuma\u2019s inequality does not provide the correct asymptotic result in (6) (unless σ 2 = d 2 , i.e., |X k | = d a.s. for every k).\n2) Analysis related to Theorem 2: From Theorem 2, it follows that for every α ≥ 0,\nδ + γ 1 + γ\n2σ 2 . \t (29) Hence, this bound coincides with the exact limit in (6)."},"refs":[{"authors":[{"name":"E. A. Abb"},{"name":"D. dissertatio"}],"title":{"text":"Local to Global Geometric Methods in Information Theory, Ph"}},{"authors":[{"name":"N. Alo"},{"name":"J. H. Spence"}],"title":{"text":"The Probabilistic Method, Wiley Series in Discrete Mathematics and Optimization, Third Edition, 2008"}},{"authors":[{"name":"A. N. Arkhangel\u2019skii"}],"title":{"text":"Lower bounds for probabilities of large deviations for sums of independent random variables"}},{"authors":[{"name":"Y. Altuˇg"},{"name":"A. B. Wagner"}],"title":{"text":"Moderate deviations analysis of channel coding: discrete memoryless case"}},{"authors":[{"name":"K. Azuma"}],"title":{"text":"Weighted sums of certain dependent random variables"}},{"authors":[{"name":"F. Chung"},{"name":"L. Lu"}],"title":{"text":"Concentration inequalities and martingale inequal- ities: a survey"}},{"authors":[{"name":"T. M. Cove"},{"name":"J. A. Thoma"}],"title":{"text":"Elements of Information Theory, John Wiley and Sons, second edition, 2006"}},{"authors":[{"name":"I. Csisz´a"},{"name":"P. C. Shield"}],"title":{"text":"Information Theory and Statistics: A Tutorial , Foundations and Trends in Communications and Information Theory, vol"}},{"authors":[{"name":"A. Dembo"}],"title":{"text":"Moderate deviations for martingales with bounded jumps"}},{"authors":[{"name":"A. Demb"},{"name":"O. Zeitoun"}],"title":{"text":"Large Deviations Techniques and Applica- tions , Springer, second edition, 1997"}},{"authors":[{"name":"D. He"},{"name":"L. A. Lastras-Monta˜no"},{"name":"E. Yang"},{"name":"A. Jagmohan"},{"name":"J. Chen"}],"title":{"text":"On the redundancy of Slepian-Wolf coding"}},{"authors":[{"name":"W. Hoeffding"}],"title":{"text":"Probability inequalities for sums of bounded random variables"}},{"authors":[{"name":"F. den Hollande"}],"title":{"text":"Large Deviations, Fields Institute Monographs, Amer- ican Mathematical Society, 2000"}},{"authors":[{"name":"C. McDiarmid"}],"title":{"text":"On the method of bounded differences"}},{"authors":[{"name":"C. McDiarmid"}],"title":{"text":"Concentration"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"H. V. Poor"},{"name":"S. Verd´u"}],"title":{"text":"Channel coding rate in ﬁnite blocklength regime"}},{"authors":[{"name":"Y. Polyanskiy"},{"name":"S. Verd´u"}],"title":{"text":"Channel dispersion and moderate devia- tions limits of memoryless channels"}},{"authors":[{"name":"L. V. Rozovsky"}],"title":{"text":"Estimate from below for large-deviation probabilities of a sum of independent random variables with ﬁnite variances"}},{"authors":[{"name":"I. Sason"}],"title":{"text":"On reﬁned versions of the Azuma-Hoeffding inequality with applications in information theory"}},{"authors":[{"name":"I. Sason"}],"title":{"text":"On the concentration of the crest factor for OFDM signals"}},{"authors":[{"name":"I. Saso"}],"title":{"text":"On Concentration of measures and moderate deviations analysis of binary hypothesis testing,\u201d presentation is online available at http://webee"}},{"authors":[{"name":"V. Y. F. Tan"}],"title":{"text":"Moderate-deviations of lossy source coding for discrete and Gaussian sources"}},{"authors":[{"name":"D. William"}],"title":{"text":"Probability with Martingales, Cambridge University Press, 1991"}},{"authors":[{"name":"K. Xenoulis"},{"name":"N. Kalouptsidis"},{"name":"I. Sason"}],"title":{"text":"New achievable rates for nonlinear Volterra channels via martingale inequalities"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569551347.pdf"},"links":[{"id":"1569559259","weight":15},{"id":"1569559541","weight":11},{"id":"1569559221","weight":12},{"id":"1569558785","weight":17},{"id":"1569559565","weight":49},{"id":"1569558681","weight":23},{"id":"1569559195","weight":7},{"id":"1569558859","weight":28},{"id":"1569566489","weight":15},{"id":"1569558901","weight":16},{"id":"1569559111","weight":21},{"id":"1569558985","weight":29},{"id":"1569558509","weight":10},{"id":"1569565705","weight":7},{"id":"1569559199","weight":12},{"id":"1569559035","weight":7},{"id":"1569558779","weight":20},{"id":"1569559523","weight":13},{"id":"1569559597","weight":22},{"id":"1569559251","weight":14},{"id":"1569550425","weight":24},{"id":"1569564509","weight":3},{"id":"1569558697","weight":7},{"id":"1569559233","weight":9}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T4.2","endtime":"10:30","authors":"Igal Sason","date":"1341310200000","papertitle":"Moderate Deviations Analysis of Binary Hypothesis Testing","starttime":"10:10","session":"S5.T4: Finite Blocklength Analysis","room":"Stratton 20 Chimneys (306)","paperid":"1569551347"},"cluster":{"jsonClass":"Map$EmptyMap$"}}

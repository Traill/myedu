{"id":"1569559251","paper":{"title":{"text":"The Finite Field Multi-Way Relay Channel with Correlated Sources: Beyond Three Users"},"authors":[{"name":"Lawrence Ong \u2020"},{"name":"Roy Timo \u2021"},{"name":"Sarah J. Johnson \u2020"}],"abstr":{"text":"Abstract\u2014The multi-way relay channel (MWRC) models coop- erative communication networks in which many users exchange messages via a relay. In this paper, we consider the ﬁnite ﬁeld MWRC with correlated messages. The problem is to ﬁnd all achievable rates, deﬁned as the number of channel uses required per reliable exchange of message tuple. For the case of three users, we have previously established that for a special class of source distributions, the set of all achievable rates can be found [Ong et al., ISIT 2010]. The class is speciﬁed by an almost balanced conditional mutual information (ABCMI) condition. In this paper, we ﬁrst generalize the ABCMI condition to the case of more than three users. We then show that if the sources satisfy the ABCMI condition, then the set of all achievable rates is found and can be attained using a separate source-channel coding architecture."},"body":{"text":"This paper investigates multi-way relay channels (MWRCs) where multiple users exchange correlated data via a relay. More speciﬁcally, each user is to send its data to all other users. There is no direct link among the users, and hence the users ﬁrst transmit to a relay, which processes its received information and transmits back to the users (refer to Fig. 1). The purpose of this paper is to ﬁnd the set of all achievable rates, which are deﬁned as the number of channel uses required to reliably (in the usual Shannon sense) exchange each message tuple.\nThe joint source-channel coding problem in Fig. 1 includes, as special cases, the source coding work of Wyner et al. [1] and the channel capacity work of Ong et al. [2]. Separately, the Shannon limits of source coding [1] (through noiseless channel) and of channel coding [2] (with independent sources) are well established. However, these limits have not yet been discovered for noisy channels with correlated sources in general. For three users, Ong et al. [3] gave sufﬁcient conditions for reliable communication using the separate source-channel coding paradigm. The key result of [3] was to show that these sufﬁcient conditions are also necessary for a special class of source distributions, hence giving the set of all achievable rates. The class was characterized by an almost balanced conditional mutual information (ABCMI) condition. This paper extends the ABCMI concept to more than three users, and shows that if the sources have ABCMI, then the set of all achievable rates can be found. While the ABCMI condition for the three-user case is expressed in terms of the standard Shannon information measure, we will use I-Measure [4, Ch. 3] for more then three users\u2014using Shannon\u2019s measure is possible but the expressions\nThough the ABCMI condition limits the class of sources for which the set of all achievable rates is found, this paper provides the following insights: (i) As achievability is derived based on a separate source-channel coding architecture, we show that source-channel separation is optimal for ﬁnite ﬁeld MWRC with sources having ABCMI. (ii) Since ABCMI are constraints on the sources, the results in this paper are potentially useful for other channel models (not restricted to the ﬁnite ﬁeld model). (iii) This paper highlights the usefulness of the I-Measure as a complement to the standard Shannon information measure.\n1) Sources: Consider m independent and identically distributed (i.i.d.) drawings of a tuple of correlated discrete ﬁnite random variables (W 1 , W 2 , . . . , W L ) , i.e., {(W 1 [t], W 2 [t], . . . , W L [t]) } m t=1 . The message of user i is given by W i = (W i [1], W i [2], . . . , W i [m]) .\n2) Channel: Each channel use of the ﬁnite ﬁeld MWRC consists of an uplink and L downlinks, characterized by\nDownlinks: Y i = X 0 ⊕ N i , for all i ∈ {1, 2, . . . , L}, (2) where X , Y , N , for all ∈ {0, 1, . . . , L}, each take values in a ﬁnite ﬁeld F, ⊕ denotes addition over F, X is the channel\ninput from node , Y is the channel output received by node , and N is the receiver noise at node . The noise N is\narbitrarily distributed, but is i.i.d. for each channel use. We have used the subscript i to denote a user and the subscript to denote a node (which can be a user or the relay).\n3) Block Codes (joint source-channel codes with feedback): Consider block codes for which the users exchange m message tuples in n channel uses. [Encoding:] The t-th transmitted channel symbol of node is a function of its message and the (t − 1) symbols it previously observed on the downlink: X [t] = f [t](W , Y [1], Y [2], . . . , Y [t − 1]), for all ∈ {0, 1, . . . , L} and for all t ∈ {1, 2, . . . , n}. As the relay has no message, we set W 0 = ∅. [Decoding:] The messages decoded by user i are a function of its message and the symbols it observed on the downlink: ( ˆ W j,i : ∀j ∈ {1, . . . , L}\\{i}) = h i (W i , Y i [1], Y i [2], . . . , Y i [n]) , for all i ∈ {1, 2, . . . , L}.\n4) Achievable Rate: Let P e denote the probability that ˆ W j,i = W i for any i = j. The rate (or bandwidth expansion\nfactor) of the code is the ratio of channel symbols to source symbols, κ = n/m. The rate κ is said to be achievable if the following is true: for any > 0, there exists a block code, with n and m sufﬁciently large and n/m = κ, such that P e < .\nTheorem 1: Consider an L-user ﬁnite ﬁeld MWRC with correlated sources. If the sources (W 1 , W 2 , . . . , W L ) have almost balanced conditional mutual information (ABCMI), then κ is achievable if and only if\n(3) The ABCMI condition used in Theorem 1 is rather technical\nand best deﬁned using the I-measure [4, Ch. 3]. For this reason, we specify this condition later in Section III-B after giving a brief review of the I-measure in Section III-A. For now, it sufﬁces to note that it is a non-trivial constraint placed on the joint distribution of (W 1 , W 2 , . . . , W L ) .\nThe achievability (if assertion) of Theorem 1 is proved using a separate source-channel coding architecture, which involves intersecting a certain Slepian-Wolf source-coding region with the ﬁnite ﬁeld MWRC capacity region [2]. The particular source-coding region of interest is the classic Slepian- Wolf region [5] with the total sum-rate constraint omitted; speciﬁcally, it is the set of all source-coding rate tuples (r 1 , r 2 , . . . , r L ) such that\nholds for all strict subsets S ⊂ {1, 2, . . . , L}. The next theorem will be a critical step in the proof of Theorem 1.\nTheorem 2: If L arbitrarily correlated random variables (W 1 , W 2 , . . . , W L ) have ABCMI, then we can ﬁnd a non- negative real tuple (r 1 , r 2 , . . . , r L ) such that\n[C1] the inequality (4) holds for all subsets S ⊂ {1, 2, . . . , L} for which 1 ≤ |S| ≤ L − 2, [C2] the inequality (4) holds with equality for all subsets S ⊂\nRemark 1: Theorem 1 characterizes a class of sources (on any ﬁnite ﬁeld MWRC) for which (i) the set of all achievable rates is known, and (ii) source-channel separation holds.\nRemark 2: Slepian-Wolf type constraints of the form (4) appear often in multi-terminal information theory. Since Theorem 2 applies directly to such constraints, it might be useful beyond its application here to the ﬁnite ﬁeld MWRC.\nRemark 3: To prove Theorem 2, we need to select L non- negative numbers that satisfy (2 L − 2) equations.\nConsider L jointly distributed random variables (W 1 , W 2 , . . . , W L ) . The Shannon measures of these random variables can be efﬁciently characterized via set operations and the I-measure. For each random variable W i , we deﬁne a (corresponding) set\n˜ W i . Let F L be the ﬁeld generated by { ˜ W i } using the usual set operations union ∪, intersection ∩, complement c , and difference −. The relationship between ˜ W i and W i is described by the I-Measure µ\nfor any non-empty S ⊆ {1, 2, . . . , L}, where ˜ W S = i∈S ˜ W i and W S {W i : i ∈ S}.\nThe atoms of F L are sets of the form L i=1 U i , where U i can either be ˜ W i or ˜ W c i . There are 2 L atoms in F L , and we denote the atoms by\nfor all K ⊆ {1, 2, . . . , L} where K c {1, 2, . . . , L} \\ K. Note that each atom corresponds to a unique K ⊆ {1, 2, . . . , L}. For the atom in (6), we call |K| the weight of the atom.\nRemark 4: The I-measure of the atoms corresponds to the conditional mutual information of the variables. More speciﬁ- cally, µ ∗ (a( K)) is the mutual information among the variables {W i : i ∈ K} conditioning on {W j : j ∈ K c }. For example, if L = 4 , then µ ∗ (a(1, 2)) = I(W 1 ; W 2 |W 3 , W 4 ) , where I(·) is Shannon\u2019s measure of conditional mutual information.\nB. Almost Balanced Conditional Mutual Information For each K ∈ {1, 2, . . . , L − 1}, we deﬁne\ni.e., atoms of weight K with the largest and the smallest measures respectively. With this, we deﬁne the ABCMI condition for L random variables:\nDeﬁnition 1: (W 1 , W 2 , . . . , W L ) are said to have ABCMI if the following conditions hold:\nRemark 5: The ABCMI condition requires that all atoms of the same weight (except for those with weight equal to zero, one, or L) have about the same I-measure.\nRemark 6: For any L, S, and K, such that 2 ≤ K ≤ S ≤ L − 2, it can be shown that α(L, S, K) ≥ β(L, S, K) ≥ 0.\nRemark 7: For L = 3, we have µ 2 ≤ 2µ 2 , i.e., we recover the ABCMI condition for the three-user case [3].\nFor the rest of this paper, we are interested in atoms only with weight between one and (L − 1) inclusive. So, we deﬁne K {K ⊂ {1, 2, . . . , L} : 1 ≤ |K| ≤ L − 1} and refer to A {a(K) : ∀K ∈ K} as the set of all such atoms.\nWe propose to select (r 1 , r 2 , . . . , r L ) in terms of the I- Measure:\nEach r i is chosen as the sum of the weighted (by a coefﬁcient + L−|K| L−1 or − |K|−1 L−1 ) I-measure of all atoms in A. The assignments of r i \u2019s are depicted in Fig. 2. We term J i ( K) the contribution from the atom a(K) to r i . Each contribution is represented by a cell in Fig. 2, and each r i by a column.\nWe now show that conditions C1 and C2 in Theorem 2 hold when the r i \u2019s are chosen as per (11).\nSince r i \u2019s are deﬁned in terms of I-Measure, we link the measure of atoms to the entropies of the corresponding random variables:\nµ ∗ (a( K)), \t (13b) where (13a) follows from [4, eqn. (3.43)], and (13b) is obtained by counting all the atoms in the set ( ˜ W S − ˜ W S c ) .\nWe now evaluate the LHS of (12) for some ﬁxed i. Consider some atom a(K) where i /∈ K. We evaluate the contributions from this atom to r −i {r j : j ∈ {1, 2, . . . , L} \\ {i}}, i.e., one speciﬁc row in Fig. 2 less the cell J i ( K):\n\u2022 |K| of the (L − 1) cells each contribute L−|K| L−1 µ ∗ (a( K)). \u2022 The remaining (L − 1 − |K|) cells each contribute\n= µ ∗ (a( K)). \t (14b) Consider some atom a(K ) where i ∈ K . The contributions\nfrom this atom to r −i are as follows (again, one speciﬁc row in Fig. 2 less the cell J i ( K )):\n  \n  \nConsider some S ⊂ {1, 2, . . . , L} where 1 ≤ |S| ≤ L − 2. We now show that if the ABCMI is satisﬁed, then\nµ ∗ (a( K)). \t (16b) Deﬁne S = |S| and r S {r i : i ∈ S}. The LHS of (16a)\nis the sum of contributions from all atoms in A to all r i ∈ r S . We will divide all atoms in A according to their weight K: (i) K = 1 , (ii) 2 ≤ K ≤ S, and (iii) K ≥ S + 1. So, we have\n∗ (a( K)), if K = {i} 0, \t otherwise.\n(18) Summing the contributions from all atoms with weight one to all r i ∈ r S ,\n2) Atoms with weight 2 ≤ K ≤ S: We ﬁx K. Consider the contributions from all atoms with weight K to a particular r i , i ∈ S (one column in the hashed region in Fig. 2). There are\n[O1] L−1 K−1 contributions with coefﬁcient L−K L−1 [from atoms where i ∈ K; there are L−1 K−1 ways to select the other (K − 1) elements in K from {1, 2, . . . , L} \\ {i}], and\n[O2] L−1 L−K−1 contributions with coefﬁcient − K−1 L−1 [from atoms where i ∈ K c ; there are L−1 L−K−1 ways to select the other (L−K−1) elements in K c from {1, 2, . . . , L}\\{i}].\nSince observations O1 and O2 are true for each r i ∈ r S , we have the following contributions from all atoms with weight K to r S (the hashed region in Fig. 2): There are\nFor an atom a(K), we say that the atom is active if K ⊆ S; otherwise, i.e., K S, the atoms is said to be inactive,\nConsider the contributions from a particular active atom a( K) to r S (one row in the hashed region in Fig. 2). We have the following contributions from this atom to r S :\n[O6] The remaining (S − K) cells each contribute − K−1 L−1 µ ∗ (a( K)).\nFor any ﬁxed S, there are S K active atoms with weight K (different ways of choosing K ⊆ S), and observations O5 and O6 are true for each active atom. Combining O3\u2013O6, we can further categorize the contributions from all atoms with weight K to r S :\n[O7] Out of the S · L−1 K−1 contributions with coefﬁcient L−K L−1 , K · S K of them are from active atoms.\n[O8] Out of the S · L−1 L−K−1 contributions with coefﬁcient − K−1 L−1 , (S − K) S K of them are from active atoms.\nNow, summing the contributions from all (active and inactive) atoms with weight K to r S , we have\nwhere α(L, S, K) is deﬁned in (9) and β(L, S, K) in (10). If the ABCMI condition is satisﬁed, then η ≥ 0.\n3) Atoms with weight S + 1 ≤ K ≤ L − 1: Consider the contributions from all atoms with weight K to r i , for some i. From observations O1 and O2, we know that there are\n\u2022 L−1 K−1 contributions with coefﬁcient L−K L−1 , and \u2022 \t L−1 L−K−1 contributions with coefﬁcient − K−1 L−1 .\nSince α(L, S, K) ≥ β(L, S, K) ≥ 0, the ABCMI condition implies that µ K ≤ 1 + 1 K−1 µ K = K K−1 µ K for all K ∈ {2, 3, . . . , L − 1}. Hence, the inequality in (22) follows.\n4) Combining the contributions of A to r S : Substituting (19), (21b), and (22) into (17), if the sources have ABCMI, then\nLemma 1: Consider a ﬁnite ﬁeld MWRC with correlated sources. A rate κ is achievable only if (3) holds for all i ∈ {1, 2, . . . , L}.\nLemma 1 can be proved by generalizing the converse theorem [6, Appx. A] for L = 3 to arbitrary L. The details are omitted.\n1) Source Coding Region: We have the following source coding result for correlated sources:\nLemma 2: Consider L correlated sources as deﬁned in Section II-A1. Each user i encodes its message W i to an index M i ∈ {1, 2, . . . , 2 mr i }, for all i. It reveals its index to all other users. Using these indices and its own message, each user i can then decode the messages of all other users, i.e., {W j : ∀j ∈ {1, 2, . . . , L} \\ {i}}, if (4) is satisﬁed for all non-empty strict subsets S ⊂ {1, 2, . . . , L}.\nThe above result is obtained by combining the results for (i) source coding for correlated sources [7, Thm. 2], and (ii) the three-user noiseless MWRC [1, Sec. II.B.1]. Note that the relay does not participate in the source code, in contrast to the setup of [1]. Instead, the relay participates in the channel code.\n2) Channel Coding Region: We have the following channel coding result for the ﬁnite ﬁeld MWRC [2]:\nLemma 3: Consider the ﬁnite ﬁeld MWRC deﬁned in (1)\u2013 (2). Let the message of each user i be M i , which is i.i.d. and uniformly distributed on {1, . . . , 2 nR i }. Using n uplink and downlink channel uses, each user i can reliably decode the message of all other users {M j : ∀j ∈ {1, 2, . . . , L} \\ {i}} if\nR j ≤ log 2 |F| − max{H(N 0 ), H(N i ) }, (23) for all i ∈ {1, 2, . . . , L}.\n3) Achievable Rates: We propose the following sepa- rate source-channel coding scheme. Fix mr i = nR i . Let (D 1 , . . . , D L ) , where each D i is i.i.d. and uniformly distributed on {1, . . . , 2 mr i }. We call D i a dither. The dithers are made known to all users. User i performs source coding to compresses its message W i to an index M i ∈ {1, . . . , 2 mr i } and computes M i = M i + D i mod 2 mr i . The random variables (M 1 , . . . , M L ) are independent, with M i being uniformly distributed on {1, . . . , 2 nR i }. The nodes then perform channel coding with M i as inputs. If (23) is satisﬁed, then each user i can recover {M j : ∀j ∈ {1, . . . , L} \\ {i}}, from which it can obtain {M j : ∀j ∈ {1, . . . , L} \\ {i}}. If (4) is satisﬁed, then each user i can recover all other users\u2019 messages {W j : ∀j ∈ {1, . . . , L} \\ {i}}, using the indices M j \u2019s and its own message W i . This means the rate κ = n/m is achievable.\nLemma 4: Consider a ﬁnite ﬁeld MWRC with correlated sources. If there exist a tuple (r 1 , r 2 , . . . , r L ) and a positive real number κ such that (4) is satisﬁed for all non-empty strict subsets S ⊂ {1, 2, . . . , L}, and (23) is satisﬁed for all i ∈ {1, 2, . . . , L} with R i = r i /κ , then the rate κ is achievable. C. Proof of Theorem 1 (Necessary and Sufﬁcient Conditions)\nThe \u201conly if\u201d part follows directly from Lemma 1 (regardless of whether the sources have ABCMI). We now prove the \u201cif\u201d part. Suppose that the sources have ABCMI. From Theorem 2, there exists a tuple (r 1 , . . . , r L ) such that conditions C1 and C2 are satisﬁed. These conditions imply that (4) is satisﬁed for all non-empty strict subsets S ⊂ {1, 2, . . . , L}. Let R i = r i /κ . Condition C2 further implies κ = H(W {1,2,...,L} |W i )\n. So, if (3) is true, then (23) is satisﬁed for all i ∈ {1, 2, . . . , L}. From Lemma 4, κ is achievable. \t .\nRemark 8: For a ﬁxed source correlation structure and a ﬁnite ﬁeld MWRC, one can check if the L-dimensional polytope deﬁned by the source coding region and that by the channel coding region (scaled by κ) intersect when κ equals the RHS of (3). If the regions intersect, then we have the set of all achievable κ for this particular source-channel combination. Theorem 1 characterizes a class of such sources."},"refs":[{"authors":[{"name":"A. D. Wyner"},{"name":"J. K. Wolf"},{"name":"F. M. J. Willems"}],"title":{"text":"Communicating via a processing broadcast satellite"}},{"authors":[{"name":"L. Ong"},{"name":"S. J. Johnson"},{"name":"C. M. Kellett"}],"title":{"text":"The capacity region of multiway relay channels over ﬁnite ﬁelds with full data exchange"}},{"authors":[{"name":"L. Ong"},{"name":"R. Timo"},{"name":"G. Lechner"},{"name":"S. J. Johnson"},{"name":"C. M. Kellett"}],"title":{"text":"The ﬁnite ﬁeld multi-way relay channel with correlated sources: The three-user case"}},{"authors":[{"name":"R. W. Yeun"}],"title":{"text":"Information Theory and Network Coding, Springer, 2008"}},{"authors":[{"name":"T. S. Han"}],"title":{"text":"Slepian-Wolf-Cover theorem for networks of channels"}},{"authors":[{"name":"L. Ong"},{"name":"R. Timo"},{"name":"G. Lechner"},{"name":"S. J. Johnson"},{"name":"C. M. Kellett"}],"title":{"text":"The three-user ﬁnite ﬁeld multi-way relay channel with correlated sources"}},{"authors":[{"name":"T. M. Cover"}],"title":{"text":"A proof of the data compression theorem of Slepian and Wolf for ergodic sources"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569559251.pdf"},"links":[{"id":"1569559259","weight":47},{"id":"1569559541","weight":18},{"id":"1569559221","weight":37},{"id":"1569558785","weight":26},{"id":"1569559565","weight":31},{"id":"1569558681","weight":17},{"id":"1569559195","weight":21},{"id":"1569558859","weight":23},{"id":"1569566489","weight":11},{"id":"1569558901","weight":17},{"id":"1569559111","weight":27},{"id":"1569558985","weight":29},{"id":"1569558509","weight":34},{"id":"1569565705","weight":16},{"id":"1569551347","weight":14},{"id":"1569559199","weight":18},{"id":"1569559035","weight":9},{"id":"1569558779","weight":29},{"id":"1569559523","weight":48},{"id":"1569559597","weight":20},{"id":"1569550425","weight":38},{"id":"1569564509","weight":5},{"id":"1569558697","weight":9},{"id":"1569559233","weight":15}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T2.4","endtime":"11:10","authors":"Lawrence Ong, Roy Timo, Sarah J Johnson","date":"1341312600000","papertitle":"The Finite Field Multi-Way Relay Channel with Correlated Sources: Beyond Three Users","starttime":"10:50","session":"S5.T2: Relay Channels","room":"Kresge Auditorium (109)","paperid":"1569559251"},"cluster":{"jsonClass":"Map$EmptyMap$"}}

{"id":"1569558779","paper":{"title":{"text":"Multiterminal Source Coding under Logarithmic Loss"},"authors":[{"name":"Thomas A. Courtade"},{"name":"Tsachy Weissman"}],"abstr":{"text":"Abstract\u2014We consider the two-encoder multiterminal source coding problem subject to distortion constraints computed un- der logarithmic loss. We provide a single-letter description of the achievable rate distortion region for arbitrarily correlated sources with ﬁnite alphabets. In doing so, we also give the rate distortion region for the CEO problem under logarithmic loss. Notably, the Berger-Tung inner bound is tight in both settings."},"body":{"text":"Characterizing the rate distortion region for the two-encoder lossy source coding problem is perhaps the most well-known, long-standing open problem in the ﬁeld of multiterminal source coding. Indeed, it is commonly referred to as the multiterminal source coding problem (a tradition to which we adhere in the present paper). Although this problem was posed nearly four decades ago, a description of the rate distortion region eluded researchers for any nontrivial choice of source distribution and distortion measure until the seminal work [1] by Wagner et al. in 2008.\nIn [1], the authors characterized the rate distortion region for jointly Gaussian sources subject to quadratic distortion con- straints. Notably, [1] showed that the extension of the single- encoder vector quantization scheme to two encoders (com- monly referred to as the Berger-Tung achievability scheme) sufﬁces to attain any point in the achievable rate distortion region. However, due to the reliance of [1] on the peculiarities of the Gaussian distribution, it was still not clear whether the Berger-Tung achievability scheme would be optimal in other settings of interest.\nIn the present paper, we answer this point in the afﬁrmative for the two-encoder setting. Speciﬁcally, we show that the Berger-Tung achievability scheme is optimal for all ﬁnite- alphabet sources when distortion is measured under logarith- mic loss. To our knowledge, this constitutes the ﬁrst time that the entire rate distortion region has been described for the multiterminal source coding problem with nontrivial ﬁnite- alphabet sources and nontrivial distortion constraints.\nThis paper is organized as follows. In Section II we formally deﬁne the logarithmic loss function and the multiterminal source coding problem we consider. In Section III we deﬁne the CEO problem and give the rate distortion region under\nlogarithmic loss. In Section IV we return to the multiterminal source coding problem and derive the rate distortion region for the two-encoder setting. Section V delivers concluding remarks.\nLet {Y 1 (j), Y 2 (j)} n j=1 be a sequence of independent, iden- tically distributed random variables with ﬁnite alphabets Y 1 and Y 2 , respectively, and joint pmf p(y 1 , y 2 ).\nIn this paper, we take the reproduction alphabet ˆ Y i to be equal to the set of probability distributions over the source alphabet Y i for i = 1, 2. Thus, for a vector ˆ Y n i ∈ ˆ Y n i , we will use the notation ˆ Y i [y i ](j) to mean the j th coordinate (1 ≤ j ≤ n) of ˆ Y n i (which is a probability distribution on Y i ) evaluated for the outcome y i ∈ Y i . In other words, the decoder generates \u201csoft\u201d estimates of the source sequences.\nWe consider the logarithmic loss distortion measure deﬁned as follows:\nUsing this deﬁnition for symbol-wise distortion, it is standard to deﬁne the distortion between sequences as\nd(y n i , ˆ y n i ) = 1 n\nWe remark that logarithmic loss is a widely used penalty function in the theory of learning and has natural interpreta- tions and applications in gambling and portfolio theory (cf. [2, Chapter 9]). Several applications of logarithmic loss in the context of source coding are discussed in the full manuscript [3]. To the best of our knowledge, logarithmic loss ﬁrst appeared explicitly as a distortion measure in the context of multiterminal source coding in [4].\nA rate distortion code (of blocklength n) consists of encod- ing functions:\nA rate distortion vector (R 1 , R 2 , D 1 , D 2 ) is strict-sense achievable if there exists a blocklength n, encoding functions g (n) 1 , g (n) 2 , and a decoder (ψ (n) 1 , ψ (n) 2 ) such that\nD i ≥ 1 n\nDeﬁnition 1. Let RD denote the set of strict-sense achiev- able rate distortion vectors and deﬁne the set of achievable rate distortion vectors to be its closure, RD .\nOur ultimate goal in the present paper is to give a single- letter characterization of the region RD . However, in order to do this, we ﬁrst consider an associated CEO problem. In this sense, the roadmap for our argument is similar to that of [1]. Speciﬁcally, both arguments couple the multiterminal source coding problem to a parametrized family of CEO problems. Then, the parameter in the CEO problem is \u201ctuned\u201d to yield the converse result. Despite this apparent similarity, the proofs are quite different since the results in [1] depend heavily on the properties of the Gaussian sources.\nIn order to attack the general multiterminal problem, we begin by studying the CEO problem (See [5] for an introduc- tion.). To this end, let {X(j), Y 1 (j), Y 2 (j)} n j=1 be a sequence of independent, identically distributed random variables with joint pmf p(x, y 1 , y 2 ) = p(x)p(y 1 |x)p(y 2 |x). That is, Y 1 ↔ X ↔ Y 2 form a Markov chain.\nIn this section, we consider the reproduction alphabet ˆ X to be equal to the set of probability distributions over the source alphabet X . As before, for a vector ˆ X n ∈ ˆ X n , we will use the notation ˆ X[x](j) to mean the j th coordinate of ˆ X n (which is a probability distribution on X ) evaluated for the outcome x ∈ X . As in the rest of this paper, d(·, ·) is the logarithmic loss distortion measure.\nA rate distortion CEO code (of blocklength n) consists of encoding functions g (n) 1 , g (n) 2 as in (1), and a decoding function\nA rate distortion vector (R 1 , R 2 , D) is strict-sense achiev- able for the CEO problem if there exists a blocklength n, encoding functions g (n) 1 , g (n) 2 and a decoder ψ (n) such that\nR i ≥ 1 n\nDeﬁnition 2. Let RD CEO denote the set of strict-sense achievable rate distortion vectors and deﬁne the set of achiev- able rate distortion vectors to be its closure, RD CEO .\nDeﬁnition 3. Let (R 1 , R 2 , D) ∈ RD i CEO if and only if there exists a joint distribution of the form\nR 1 ≥ I(U 1 ; Y 1 |U 2 , Q) R 2 ≥ I(U 2 ; Y 2 |U 1 , Q)\nR 1 + R 2 ≥ I(U 1 , U 2 ; Y 1 , Y 2 |Q) D ≥ H(X|U 1 , U 2 , Q).\nBefore proceeding with the proof, we cite the following variant of a well-known inner bound:\nProposition 1 (Berger-Tung Inner Bound [6]). The rate dis- tortion vector (R 1 , R 2 , D) is achievable if\nR 1 ≥ I(U 1 ; Y 1 |U 2 , Q) R 2 ≥ I(U 2 ; Y 2 |U 1 , Q)\nProof of Theorem 1: Apply Proposition 1 with the repro- duction function f (U 1 , U 2 , Q) := Pr [X = x|U 1 , U 2 , Q]. Then simply note that E [d(X, f (U 1 , U 2 , Q)] = H(X|U 1 , U 2 , Q), which yields the desired result.\nThus, we note that our inner bound is merely the Berger- Tung inner bound specialized to the case of logarithmic loss.\nA particularly useful property of the logarithmic loss dis- tortion measure is that the expected distortion is lower- bounded by a conditional entropy, a property also enjoyed by quadratic distortion for Gaussian random variables. The following lemma is a key tool in the proof of the converse.\nLemma 1. Let Z = (g (n) 1 (Y n 1 ), g (n) 2 (Y n 2 )) be the argument of the reproduction function ψ (n) . Then nEd(X n , ˆ X n ) ≥ H(X n |Z).\nProof: By deﬁnition of the reproduction alphabet, we can consider the reproduction ˆ X n to be a probability distribution on X n conditioned on the argument Z. In particular, if ˆ x n = ψ (n) (z), deﬁne s(x n |z) = n j=1 ˆ x[x(j)](j), which is\na probability measure on X n . Then, we obtain the following lower bound on the expected distortion, conditioned on Z = z:\n1 n\nwhere p(x n |z) = Pr (X n = x n |Z = z) is the true conditional distribution. Averaging both sides over all values of Z, we obtain the desired result.\nDeﬁnition 4. Let (R 1 , R 2 , D) ∈ RD o CEO if and only if there exists a joint distribution of the form\np(x)p(y 1 |x)p(y 2 |x)p(u 1 |y 1 , q)p(u 2 |y 2 , q)p(q), which satisﬁes\nR 1 ≥ I(Y 1 ; U 1 |X, Q) + H(X|U 2 , Q) − D \t (2) R 2 ≥ I(Y 2 ; U 2 |X, Q) + H(X|U 1 , Q) − D\nTheorem 2. If (R 1 , R 2 , D) is strict-sense achievable for the CEO problem, then (R 1 , R 2 , D) ∈ RD o CEO .\nProof: Suppose the triple (R 1 , R 2 , D) is strict-sense achievable. Let A be a nonempty subset of {1, 2}, and let F j = g (n) j (Y n j ) be the message sent by encoder j. Deﬁne U j (i) = (F j , Y j (1 : i−1)) and Q(i) = (X(1 : i−1), X(i+1 : n)) and observe that:\n= I(X n , Y n A ; F A |F A c ) \t (4) = I(X n ; F A |F A c ) +\n\u2022 (5) follows since F k is a function of Y n k and hence F 1 ↔ X n ↔ F 2 form a Markov chain (since Y n 1 ↔ X n ↔ Y n 2 form a Markov chain).\n\u2022 (7) follows by the chain rule and also from the Markov condition Y k (i) ↔ X n ↔ Y k (1 : i − 1) resulting from the i.i.d. nature of the source sequences.\n\u2022 (8) follows since conditioning reduces entropy. Therefore, dividing both sides by n, we have:\n1 n\nAlso, using Lemma 1 and the fact that conditioning reduces entropy, we have:\nObserve that Q(i) is independent of (X(i), Y 1 (i), Y 2 (i)) and, conditioned on Q(i), we have the Markov chain U 1 (i) ↔ Y 1 (i) ↔ X(i) ↔ Y 2 (i) ↔ U 2 (i). Thus, a standard time- sharing argument proves the theorem.\nProof: We ﬁrst remark that the cardinality bounds in the deﬁnition of RD i CEO can be imposed without any loss of generality. This is a consequence of [7, Lemma 2.2] and is discussed in detail in the full manuscript [3].\nFix p(q), p(u 1 |y 1 , q), and p(u 2 |y 2 , q) and consider the extreme points of polytope deﬁned by the inequalities (2)-(3):\nP 1 = (0, 0, I(Y 1 ; U 1 |X, Q) + I(Y 2 ; U 2 |X, Q) + H(X)) P 2 = (I(Y 1 ; U 1 |Q), 0, I(U 2 ; Y 2 |X, Q) + H(X|U 1 , Q)) P 3 = (0, I(Y 2 ; U 2 |Q), I(U 1 ; Y 1 |X, Q) + H(X|U 2 , Q)) P 4 = (I(Y 1 ; U 1 |Q), I(Y 2 ; U 2 |U 1 , Q), H(X|U 1 , U 2 , Q))\nwhere the point P j is a triple (R (j) 1 , R (j) 2 , D (j) ). We say a point (R (j) 1 , R (j) 2 , D (j) ) is dominated by a point in RD i CEO if there exists some (R 1 , R 2 , D) ∈ RD i CEO for which R 1 ≤ R (j) 1 , R 2 ≤ R (j) 2 , and D ≤ D (j) . Observe that each of these extreme points is dominated by a point in RD i CEO :\n\u2022 First, \t observe \t that \t (R (4) 1 , R (4) 2 , D (4) ) \t and (R (5) 1 , R (5) 2 , D (5) ) are both in RD i CEO , so these points are not problematic.\n\u2022 Next, observe that the point (0, 0, H(X)) is in RD i CEO , which can be seen by setting all auxiliary ran- dom variables to be constant. This point dominates\n\u2022 By using auxiliary random variables ( ˆ U 1 , ˆ U 2 , Q) = (U 1 , ∅, Q), the point (I(Y 1 ; U 1 |Q), 0, H(X|U 1 , Q)) is in RD i CEO , and dominates the point (R (2) 1 , R (2) 2 , D (2) ). By a symmetric argument, the point (R (3) 1 , R (3) 2 , D (3) ) is also dominated by a point in RD i CEO .\nSince RD o CEO is the convex hull of all such extreme points (i.e., the convex hull of the union of extreme points over all appropriate joint distributions), the theorem is proved.\nRemark 1. Theorem 3 can be extended to the general case of m-encoders. Moreover, the converse of the theorem continues to hold when the reproduction alphabet ˆ X n is not restricted to the set of product distributions. The key observation is that Lemma 1 continues to hold. The reader is directed to the complete manuscript [3] for details.\nWith Theorem 3 in hand, we are now in a position to characterize the achievable rate distortion region for the mul- titerminal source coding problem under logarithmic loss.\nDeﬁnition 5. Let (R 1 , R 2 , D 1 , D 2 ) ∈ RD i if and only if there exists a joint distribution of the form\nR 1 ≥ I(U 1 ; Y 1 |U 2 , Q) R 2 ≥ I(U 2 ; Y 2 |U 1 , Q)\nR 1 + R 2 ≥ I(U 1 , U 2 ; Y 1 , Y 2 |Q) D 1 ≥ H(Y 1 |U 1 , U 2 , Q)\nAgain, we require an appropriate version of the Berger-Tung inner bound:\nProposition 2 (Berger-Tung Inner Bound [6]). The rate dis- tortion vector (R 1 , R 2 , D 1 , D 2 ) is achievable if\nR 1 ≥ I(U 1 ; Y 1 |U 2 , Q) R 2 ≥ I(U 2 ; Y 2 |U 1 , Q)\nProof of Theorem 4: Similar to the proof of Theo- rem 1, apply Proposition 2 with the reproduction functions f i (U 1 , U 2 , Q) := Pr [Y i = y i |U 1 , U 2 , Q].\nB. A Matching Outer Bound Theorem 5. RD i = RD .\nProof: Assume (R 1 , R 2 , D 1 , D 2 ) is strict-sense achiev- able. We ﬁrst note that the cardinality bounds in the deﬁnition of RD i can be imposed without any loss of generality. This is a consequence of [7, Lemma 2.2] and is discussed in detail in the full manuscript [3]. Thus, it sufﬁces to show that (R 1 , R 2 , D 1 , D 2 ) ∈ RD i , ignoring the cardinality constraints.\nIn other words, X = (Y B , B), where B is a Bernoulli random variable independent of Y 1 , Y 2 . Observe that we have the Markov chain Y 1 ↔ X ↔ Y 2 , and thus, we are able to apply Theorem 3.\nSince (R 1 , R 2 , D 1 , D 2 ) is strict-sense achievable, there exist reproductions ˆ Y n i satisfying\n1 n\nFix the encoding operations and set ˆ X[(y 1 , 1)](j) = t ˆ Y 1 [y 1 ](j) and ˆ X[(y 2 , 2)](j) = (1 − t) ˆ Y 2 [y 2 ](j). Then for the CEO problem deﬁned by (X, Y 1 , Y 2 ):\n1 n\nt n\nwhere h 2 (t) is the binary entropy function. Hence, for this CEO problem, distortion h 2 (t) + tD 1 + (1 − t)D 2 is achievable and Theorem 3 yields a joint distribution 1 p(y 1 , y 2 )p t (u 1 |y 1 , q)p t (u 2 |y 2 , q)p t (q) satisfying\n  \n \nNext, ﬁx > 0, and partition the interval [0, 1] as 0 = t 1 < t 2 < · · · < t m = 1, such that |t j+1 − t j | < H(Y\n. We have just proven that, for each t j in the partition, there exist distributions p t j (u 1 |y 1 , q), p t j (u 2 |y 2 , q), p t j (q) for which the corresponding joint distribution satisﬁes (10)-(11).\nNow, suppose t satisﬁes t j < t < t j+1 . Then we can express t as a convex combination t = θt j + (1 − θ)t j+1 . By timesharing between the distributions {p t j (u 1 |y 1 , q), p t j (u 2 |y 2 , q), p t j (q)} with probability θ and the distributions {p t j+1 (u 1 |y 1 , q), p t j+1 (u 2 |y 2 , q), p t j+1 (q)} with probability (1 − θ), we obtain a set of distributions 2 {p t (u 1 |y 1 , q), p t (u 2 |y 2 , q), p t (q)} for which the corresponding joint distri- bution satisﬁes\n+ (1 − θ)I(U (t j+1 ) 2 \t ; Y 2 |U (t j+1 ) 1 \t , Q (t j+1 ) ) R 1 + R 2 ≥ I(U (t) 1 , U (t) 2 ; Y 1 , Y 2 |Q (t) )\nBy repeating this procedure for each interval in the partition, we obtain a family of such distributions parametrized by t ∈ [0, 1]. Next, we show that the following holds for any t:\nTo simplify notation, deﬁne f i (t) H(Y i |U (t) 1 , U (t) 2 , Q (t) ) for i = 1, 2 and t ∈ [0, 1]. By construction, we have that\nwhenever t = t j for some t j in the partition. Next, observe that if t = θt j + (1 − θ)t j+1 , then f i (t) = θf i (t j ) + (1 − θ)f i (t j+1 ) by virtue of the time-sharing scheme. Furthermore, f i is piecewise-linear (and therefore continuous) and bounded from above by H(Y 1 , Y 2 ). Now, suppose t = θt j + (1 − θ)t j+1 for some j and θ. Then some straightforward algebra yields:\n+ (1 − θt j − (1 − θ)t j+1 ) (θf 2 (t j ) + (1 − θ)f 2 (t j+1 )) ≤ θ 2 [t j f 1 (t j ) + (1 − t j )f 2 (t j )]\n+t j f 1 (t j ) + t j+1 f 1 (t j+1 )] + \t (14) ≤ θ 2 [t j D 1 + (1 − t j )D 2 ]\n+t j D 1 + t j+1 D 1 ] + \t (15) = tD 1 + (1 − t)D 2 + , \t (16)\nwhere (14) follows since |t j+1 − t j | is small, and (15) follows from (13).\nThis proves (12) and implies that it is impossible to have f 1 (t) > D 1 + and f 2 (t) > D 2 +\nsimultaneously for any t ∈ [0, 1], else we would contradict (16). Also, we have the following two inequalities at the endpoints of the interval [0, 1]:\nTherefore, distributions {p t ∗ (u 1 |y 1 , q), p t ∗ (u 2 |y 2 , q), p t ∗ (q)} corresponding to t ∗ yield a joint distribution which satisﬁes the rate constraints (10) and the distortion constraints\nD i ≥ H(Y i |U (t ∗ ) 1 , U (t ∗ ) 2 , Q (t ∗ ) ) − for i = 1, 2. Since was arbitrary, this proves the converse.\nRemark 2. Like Theorem 3, the converse of Theorem 5 continues to hold when the reproduction alphabets are not restricted to the set of product distributions. Details are in the complete manuscript [3].\nGeneralizing Theorem 5 to three or more encoders rep- resents a formidable challenge. Indeed, an extension of the converse alone would not be sufﬁcient since this would imply optimality of the Berger-Tung inner bound for more than two encoders. This is known to be false since the Berger-Tung achievability scheme is suboptimal for the lossless modulo- sum problem studied by K¨orner and Marton in [8].\nWe refer the reader to the full manuscript [3] for a detailed discussion of the relationship between the results presented herein and the general multiterminal source coding problem. Also, several applications and examples are given in [3] which have necessarily been omitted here due to space constraints."},"refs":[{"authors":[{"name":"A. Wagner"},{"name":"S. Tavildar"},{"name":"P. Viswanath"}],"title":{"text":"Rate region of the quadratic gaussian two-encoder source-coding problem"}},{"authors":[{"name":"N. Cesa-Bianch"},{"name":"G. Lugos"}],"title":{"text":"Prediction, Learning, and Games"}},{"authors":[{"name":"T. A. Courtade"},{"name":"T. Weissman"}],"title":{"text":"Multiterminal source coding under logarithmic loss"}},{"authors":[{"name":"T. Courtade"},{"name":"R. Wesel"}],"title":{"text":"Multiterminal source coding with an entropy- based distortion measure"}},{"authors":[{"name":"T. Berger"},{"name":"Z. Zhang"},{"name":"H. Viswanathan"}],"title":{"text":"The ceo problem [multitermi- nal source coding]"}},{"authors":[{"name":"T. Berge"},{"name":"G. Longo (Ed"}],"title":{"text":"Multiterminal Source Coding"}},{"authors":[{"name":"S. Jana"}],"title":{"text":"Alphabet sizes of auxiliary random variables in canonical inner bounds"}},{"authors":[{"name":"J. Korner"},{"name":"K. Marton"}],"title":{"text":"How to encode the modulo-two sum of binary sources (corresp.)"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569558779.pdf"},"links":[{"id":"1569559259","weight":37},{"id":"1569559541","weight":29},{"id":"1569559221","weight":16},{"id":"1569558785","weight":32},{"id":"1569559565","weight":58},{"id":"1569558681","weight":30},{"id":"1569559195","weight":12},{"id":"1569558859","weight":19},{"id":"1569566489","weight":36},{"id":"1569558901","weight":17},{"id":"1569559111","weight":51},{"id":"1569558985","weight":35},{"id":"1569558509","weight":27},{"id":"1569565705","weight":15},{"id":"1569551347","weight":20},{"id":"1569559199","weight":17},{"id":"1569559035","weight":8},{"id":"1569559523","weight":13},{"id":"1569559597","weight":44},{"id":"1569559251","weight":29},{"id":"1569550425","weight":82},{"id":"1569558697","weight":18},{"id":"1569559233","weight":17}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T1.2","endtime":"10:30","authors":"Thomas Courtade, Tsachy Weissman","date":"1341310200000","papertitle":"Multiterminal Source Coding under Logarithmic Loss","starttime":"10:10","session":"S5.T1: Multiterminal Source Coding","room":"Kresge Rehearsal B (030)","paperid":"1569558779"},"cluster":{"jsonClass":"Map$EmptyMap$"}}

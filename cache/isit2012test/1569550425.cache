{"id":"1569550425","paper":{"title":{"text":"Distributed and Cascade Lossy Source Coding with a Side Information \"Vending Machine\""},"authors":[{"name":"Behzad Ahmadi"},{"name":"Osvaldo Simeone"}],"abstr":{"text":"Abstract\u2014Source coding with a side information \"vending ma- chine\" is a recently proposed framework in which the statistical relationship between the side information available at the decoder and the source sequence can be controlled by the decoder based on the message received from the encoder. In this paper, the characterization of the optimal rate-distortion performance as a function of the cost associated with the control actions is extended from the previously studied point-to-point set-up to two multiterminal models. First, a distributed source coding model is studied, in which two encoders communicate over rate-limited links to a decoder, whose side information can be controlled based on the control actions selected by one of the encoders. The rate- distortion-cost region is characterized under the assumption of lossless reconstruction of the source encoded by the node that does not control the side information. Then, a three-node cascade scenario is investigated, in which the last node has controllable side information. The rate-distortion-cost region is derived for general distortion requirements and under the assumption of \"causal\" availability of side information at the last node.\nKeywords: Distributed source coding, cascade source coding, observation costs, side information, rate-distortion theory."},"body":{"text":"Reference [1] introduced the notion of a side information \"vending machine\". In this framework, unlike the conventional Wyner-Ziv set-up, the joint distribution of the side information Y available at the decoder and of the source X observed at the encoder can be controlled through the selection of an \"action\" A. Action A is selected by the decoder based on the message M , of R bits per source symbol, received from the encoder, and is subject to a cost constraint. The performance of the system is thus expressed in terms of the interplay among three metrics, namely the rate R, the cost budget Γ on the action A, and the distortion D of the reconstruction ˆ X at the decoder. The rate-distortion-cost function R(D, Γ) is derived in [1] for the case in which the side information Y is available \"non- causally\" to the decoder, as in the standard Wyner-Ziv model, and in the case in which it is available \"causally\", as introduced in [2].\nRecent works [3] and [4] generalized the characterization of the rate-distortion-cost function in [1] to a multi-terminal set-up analogous to the so called Heegard-Berger problem [5], in which the side information vending machine may or may not be available at the decoder. This entails the presence of two decoders, one with access to the vending machine and\none without any side information. Reference [4] also solved the more general case in which both decoders have access to the same vending machine, and either the side informations produced by the vending machine at the two decoders satisfy a degradedness condition, or lossless source reconstructions are required at the decoders. Instead, the work [3] considered a set-up that generalizes the Heegard-Berger problem mentioned above by allowing for functional reconstructions of the source X and of an additional sequence measured only through the vending machine at the decoder. A further related work is [6], where the model in [1] is extended to include secrecy constraints.\nContributions: In this paper, we study two multi-terminal extensions of the set-up of [1], namely the distributed source coding setting of Fig. 1 and the cascade model of Fig. 2. In the distributed source coding setting of Fig. 1, two encoders (Node 1 and Node 2), which measure correlated sources X 1 and X 2 , respectively, communicate over rate-limited links, of rates R 1 and R 2 , to a single decoder (Node 3). The action sequence controlling the side information at Node 3 is selected by Node 3 based on the message M 1 (of rate R 1 ) received from Node 1. In Sec. II, we characterize the set R(D 1 , Γ) of all achievable rates ( R 1 , R 2 ) for a given distortion constraint D 1 on the reconstruction 1 ˆ X 1 and an action cost constraint of Γ , under the requirement that source X 2 must be decoded (near)\nlosslessly at the decoder. We also provide a numerical example to obtain further insight into the role of control information in achieving optimal performance. This result generalizes the classical result of [7] for distributed source coding with \"one distortion criterion\".\nIn the cascade model of Fig. 2, Node 1 is connected via a rate-limited link, of rate R 12 , to Node 2, which is in turn communicates with Node 3 with rate R 23 . Source X 1 is measured by Node 1 and the correlated source X 2 by both Node 1 and Node 2. Node 3 has side information Y , which can be controlled via an action A selected by Node 3 based on the message received from Node 2. In Sec. III, we derive the set R(D 1 , D 2 , Γ) of all achievable rates (R 12 , R 23 ) for given distortion constraints ( D 1 , D 2 ) on the reconstructions ˆ\nX 1 and ˆ X 2 at Node 2 and Node 3, respectively, 2 and for cost constraint Γ . This characterization is obtained under the assumption that the side information Y be available causally at Node 3. This result extends the characterization of the rate-distortion achievable performance for the cascade model studied in [9] to the set-up at hand with a side information vending machine at Node 3.\nIn this section, we ﬁrst detail the system model for the setting of Fig. 1 of distributed source coding with a vending machine. We then present the characterization of the corre- sponding rate-distortion-cost performance in Sec. II-B. An example follows in Sec. II-C.\nThe problem of distributed lossy source coding with a vending machine and non-causal side information is deﬁned by the probability mass functions (pmfs) p X 1 X 2 (x 1 , x 2 ) and p Y |AX 1 X 2 (y|a, x 1 , x 2 ) and discrete alphabets X 1 , X 2 , Y, A and ˆ X 1 as follows. The source sequences X n 1 and X n 2 with X n 1 ∈ X n 1 and X n 2 ∈ X n 2 , respectively, are such that the pairs ( X 1i , X 2i ) for i ∈ [1, n] are independent and identically distributed (i.i.d.) with joint pmf p X 1 X 2 (x 1 , x 2 ). Node 1 measures sequences X n 1 and encodes it into message M 1 of nR 1 bits, while Node 2\nmeasures sequences X n 2 and encodes it into message M 2 of nR 2 bits. Node 3 wishes to reconstruct the two sources within given distortion requirements, to be discussed below, as ˆ X n 1 ∈ ˆ X n 1 and ˆ X n 2 ∈ ˆ X n 2 .\nTo this end, Node 3 selects an action sequence A n , where A n ∈ A n , based on the message M 1 received from Node 1 . The side information sequence Y n is then realized as the output of a memoryless channel with inputs ( A n , X n 1 , X n 2 ). Speciﬁcally, given A n , X n 1 and X n 2 , the sequence Y n is distributed as\nThe overall cost of an action sequence a n is deﬁned by a per- symbol cost function Λ: A →[0, Λ max ] with 0 ≤ Λ max < ∞. The estimated sequences ˆ X n 1 and ˆ X n 2 are obtained as a func- tion of both messages M 1 and M 2 and of the side information Y n . The estimates ˆ X n 1 and ˆ X n 2 are constrained to satisfy distortion constraints deﬁned by two per-symbol distortion metrics, namely d 1 (x 1 , x 2 , y, ˆ x 1 ): X 1 × X 2 × Y × ˆ X 1 → [0, D max ] with 0 ≤ D max < ∞ and d 2 (x 2 , ˆ x 2 ) = d H (x 2 , ˆ x 2 ): X 2 × X 2 → {0, 1}. Note that, while metric d 1 (x 1 , x 2 , y, ˆ x 1 ) is arbitrary, metric d 2 (x 2 , ˆ x 2 ) is assumed to be the Hamming distortion d H (x 2 , ˆ x 2 ), where d H (x 2 , ˆ x 2 ) = 0 if x 2 = ˆx 2 and d H (x 2 , ˆ x 2 ) = 1 otherwise.\nMore formally, an ( n, R 1 , R 2 , D 1 , D 2 , Γ) code for the set- up of Fig. 1 consists of two source encoders\nand g 2 : X n 2 → [1, 2 nR 2 ], \t (2) which map the sequences X n 1 and X n 2 into messages M 1 and M 2 , respectively; an \u201caction\u201d function\n: [1 , 2 nR 1 ] → A n , \t (3) which maps the message M 1 into an action sequence A n ; and a decoding function\nwhich maps the messages M 1 and M 2 , and the side informa- tion sequence Y n into the estimated sequences ˆ X n 1 and ˆ X n 2 ; such that the action cost constraint Γ is satisﬁed as\n1 n\nand the distortion constraints D 1 and D 2 hold, namely 1 n\n1 n\n(6b) Given a distortion-cost tuple ( D 1 , D 2 , Γ), a rate pair\n(R 1 , R 2 ) is said to be achievable if, for any > 0 and sufﬁ- ciently large n, there exists a (n, R 1 , R 2 , D 1 + , D 2 + , Γ+ )\ncode. The rate-distortion-cost region R(D 1 , D 2 , Γ) is deﬁned as closure of all rate pairs ( R 1 , R 2 ) that are achievable given the distortion-cost tuple ( D 1 , D 2 , Γ). We focus on charac- terizing the rate-distortion-cost function R(D 1 , Γ), which is deﬁned as R(D 1 , Γ) = R(D 1 , 0, Γ), that is, we impose the constraint 1 n n\nIn this section, a single-letter characterization of the rate- distortion-cost region R(D 1 , Γ) is derived.\nProposition 1. The rate-distortion-cost region R(D 1 , Γ) for the model in Fig. 1 is given by union of the set of all of rate tuples (R 1 , R 2 ) that satisfy the inequalities\nR 1 ≥ I(X 1 ; A|Q) + I(X 1 ; V |A, X 2 , Y, Q) (7a) R 2 ≥ H(X 2 |A, Y, V, Q) \t (7b)\nand R 1 + R 2 ≥ I(X 1 ; A|Q) + H(X 2 |A, Y, Q) \t (7c) + I(X 1 ; V |A, X 2 , Y, Q),\n(8) with pmfs p(q) and p(a, v|x 1 , q) and deterministic function ˆx 1 (v, y, q), such that the action cost and the distortion con- straints\nE [Λ(A)] ≤ Γ \t (9) and E d 1 (X 1 , X 2 , Y, ˆ X 1 ) ≤ D 1 \t (10)\nhold. Finally, Q and V are auxiliary random variables whose alphabet cardinality can be constrained as |Q| ≤ 6 and |V| ≤ 6 |X 1 | |A| + 3 without loss of optimality.\nRemark 2. If we set p(y|a, x 1 , x 2 ) = p(y|x 1 , x 2 ), Proposition 1 reduces to [7, Theorem 1]. If, instead, X 1 is independent of X 2 , the minimum rate R 1 , given by the right-hand side of (7a), recovers [1, Theorem 1].\nFor the proof of converse, we refer to [8]. As for achiev- ability, the scheme at hand combines the distributed Wyner-Ziv approach of [10, Theorem II] with the scheme proposed in [1, Sec. II-B]. Speciﬁcally, Node 1 ﬁrst maps the input sequence X n 1 into an action sequence A n , so that the two sequences are jointly typical. Conveying sequence A n to the receiver requires I(X 1 ; A) bits per source sample, as follows easily from standard rate-distortion theory results. The sequences (A n , Y n ) are now regarded as side information available at the decoder. Based on this, the distributed Wyner-Ziv scheme proposed in [10, Theorem 2] is used to convey an auxiliary codeword V n from Node 1 and sequence X n 2 from Node 2 3 . Note that the fact that sequences ( A n , Y n ) are not i.i.d. does not affect achievability of the rate region derived in [10].\nFinally, the decoder estimates ˆ X n 1 sample by sample by using function ˆx 1 (v, y, q) as ˆ X 1i = ˆx 1 (V i , Y i , Q i ).\nWe remark that an extension of Proposition 1 to any number of encoders can be found in [8].\nWe now focus on a speciﬁc example in order to illustrate the result derived in Proposition 1. Speciﬁcally, we assume that all alphabets are binary and that ( X 1 , X 2 ) is a doubly symmetric binary source (DSBS) characterized by probability p, with 0 ≤ p ≤ 1/2, so that p(x 1 ) = p(x 2 ) = 1/2 for x 1 , x 2 ∈ {0, 1} and Pr[X 1 = X 2 ] = p. Moreover, the decoder wishes to reconstruct both X 1 and X 2 losslessly in the sense discussed above. This implies that we set d 1 (x 1 , x 2 , y, ˆ x 1 ) = d H (x 1 , ˆ x 1 ) and D 1 = 0. The side information Y i is such that\nwhere f( x 1 , x 2 ) is a deterministic function to be speciﬁed. Therefore, when a unitary action, A i = 1, is selected, then Y i = f(X 1i , X 2i ) is measured at the receiver, while with A i = 0 no useful information is collected by the decoder. The action sequence A n must satisfy the cost constraint (5), where the cost function is deﬁned as Λ( A i ) = 1 if A i = 1 and Λ(A i ) = 0 if A i = 0. It follows that, given (11), a cost Γ implies that the decoder can observe f( X 1i , X 2i ) only for at most nΓ symbols. As for the function f( x 1 , x 2 ), we consider two cases, namely f( x 1 , x 2 ) = x 1 ⊕ x 2 , where ⊕ is the binary sum and f(x 1 , x 2 ) = x 1 x 2 , where is the binary product.\nUnder the requirement of lossless reconstruction for both X 1 and X 2 (i.e., D 1 = 0 along with D 2 = 0), it can be easily shown from Proposition 1 that the minimum sum-rate R 1 +R 2 for a given cost constraint Γ , which we denote as R sum (Γ) is given by the right-hand side of (7c) with V = X 1 , namely 4\nwhere the mutual informations are calculated with respect to the distribution p(x 1 , x 2 , y, a) = p(x 1 , x 2 )p(a|x 1 )p(y|a, x 1 , x 2 ), and the minimum is taken over all distributions p(a|x 1 ) such that E [Λ(A)] = E [A] ≤ Γ. Note that, by its deﬁnition, function R sum (Γ) is non- increasing for all Γ ≥ 0 (and constant for Γ ≥ 1) so that in particular R sum (1) = min Γ≥0 R sum (Γ). Given the function f(x 1 , x 2 ), evaluation of (12) requires solving a simple convex optimization problem. We do not provide a more explicit expression here, as it can be easily derived. Instead, we discuss some numerical results for the two functions f( x 1 , x 2 ) mentioned above , namely (binary) sum and product.\nTo start with, we evaluate the sum-rate (12) R sum (1), which provides the minimum value of R sum (Γ) over Γ, as discussed above . With Γ = 1, it is clearly optimal to set A = 1, irrespective of the value of X 1 . It is not difﬁcult to see that\nwe have R ⊕ sum (1) = 1 for the sum side information, while for the product side information we obtain\nwhere we have used the deﬁnition H (p 1 , p 2 , ..., p k ) = − k i =1 p i log 2 p i . By comparing these two expressions, it can be seen that, if p is sufﬁciently small, namely if p 0.33, we have R sum (1) < R ⊕ sum (1) and thus product side information is more informative than the sum, while for p 0.33 the opposite is true (and for p = 1, they are equally informative).\nWe now evaluate the sum-rate (12) for a general cost budget 0 ≤ Γ ≤ 1 for both sum and product side information. We are interested in emphasizing the role of data and control information in achieving the optimal sum-rate (12). To this end, we compare (12) with the performance attainable by imposing that the action A be selected by Node 3 a priori, that is, without any control from Node 1. The sum-rate attainable by such a scheme, which is referred to as \"greedy\", following [1], can be easily seen to be given by 5\n(14) We use, as above, the notations R ⊕ sum, greedy (Γ) and R sum, greedysum (Γ) for (14) as evaluated with sum and prod- uct side informations.\nA ﬁrst observation is that, with sum side information, we have that (see [8] for details)\nR ⊕ sum, greedy (Γ) = R ⊕ sum (Γ). \t (15) This shows that a \"greedy\" approach, in which only data information is conveyed by Node 1, is optimal. Instead, this is not the case with product side information and we generally have R sum, greedy (Γ) ≥ R sum (Γ), where inequality can be strict (unless, of course, Γ = 1) . The reason is that, if X 1 = 0,\nthe value of the side information is always Y = X 1 X 2 = 0 irrespective of the value of X 2 . Therefore, if X 1 = 0, the side information is less informative than if X 1 = 1 and thus it may be advantageous to save on the action cost by setting A = 0. Consequently, choosing actions based on the message received from Node 1 can result in a lower sum-rate.\nTo illustrate the discussion above, Fig. 3 depicts the sum- rates (12) and (14) versus the action cost Γ for p = 0.4. It can be seen that, for sufﬁciently large probability p (here, p = 0.4), while a product side information is less advantageous that sum side information for Γ = 1 , as per the discussion above, this may not be the case for smaller costs. Moreover, in this case, the greedy approach suffers from a signiﬁcant performance loss for product side information.\nIn this section, we ﬁrst describe the system model for the setting of Fig. 2 of cascade source coding with a side infor- mation vending machine. We recall that side information Y is here assumed to be available causally at the decoder (Node 3). We then present the characterization of the corresponding rate-distortion-cost performance in Sec. III-B.\nThe problem of cascade lossy computing with causal observation costs at second user is deﬁned by the pmfs p X 1 X 2 (x 1 , x 2 ) and p Y |AX 1 X 2 (y|a, x 1 , x 2 ) and discrete alpha- bets X 1 , X 2 , Y, A, ˆ X 1 , ˆ X 2 . The source sequences X n 1 and X n 2\nwith X n 1 ∈ X n 1 and X n 2 ∈ X n 2 , respectively , are such that the pairs ( X 1i , X 2i ) for i ∈ [1, n] are i.i.d. with joint pmf p X 1 X 2 (x 1 , x 2 ). Node 1 measures sequences X n 1 and X n 2 and encodes them in a message M 12 of nR 12 bits, which is delivered to Node 2. Node 2 estimates a sequence ˆ\nX n 1 ∈ ˆ X n 1 within given distortion requirements. Moreover, Node 2 encodes the message M 12 , received from Node 1, and the locally available sequence X n 2 in a message M 23 of nR 23 bits, which is delivered to node 3.\nNode 3 wishes to estimate a sequence ˆ X n 2 ∈ ˆ X n 2 within given distortion requirements. To this end, Node 3 receives message M 23 and based on this, selects an action sequence A n , where A n ∈ A n . The action sequence affects the quality of the measurement Y n of sequence X n 1 and X n 2 obtained at the Node 3. Speciﬁcally, given A n , X n 1 and X n 2 , the sequence Y n is distributed as in (1). The estimated symbol ˆ X 2i with ˆ\nX 2i ∈ ˆ X 2 is then obtained as a function of M 23 and Y i for i ∈ [1, n]. Estimated sequences ˆ X n j for j = 1, 2 must satisfy distortion constraints deﬁned by functions d j (x 1 , x 2 , y, ˆ x j ): X 1 × X 2 × Y × ˆ X j → [0, D max ] with 0 ≤ D max < ∞ for j = 1, 2, respectively.\nThe formal description of an ( n, R 12 , R 23 , D 1 , D 2 , Γ) code for the set-up of Fig. 2 can be constructed similar to Sec. 2 following the discussion above and is fully detailed in [8]. Here we remark that the \u201caction\u201d function at Node 3\nmaps the message M 23 into an action sequence A n ; and that we have the decoding function at Node 2\nwhich maps the message M 12 and the measured sequence X n 2 into the estimated sequence ˆ X n 1 ; and a sequence of decoding functions at Node 3\nfor i ∈ [1, n] which maps the message M 23 and the mea- sured sequence Y i into the ith estimated symbol ˆ X 2i = h 2i (M 23 , Y i ). We also note that the action cost constraint (5) and distortion constraints D j\n1 n\nmust be satisﬁed. Achievability and the rate-distortion-cost region R(D 1 , D 2 , Γ) are deﬁned similar to Sec. 2.\nWe have the following characterization of the rate- distortion-cost region.\nProposition 3. The rate-distortion-cost region R(D 1 , D 2 , Γ) for the set-up of Fig. 2 is given by the union of all rate pairs (R 12 , R 23 ) satisfying the inequalities\nR 12 ≥ I(X 1 ; U, A, ˆ X 1 |X 2 ) \t (20a) and R 23 ≥ I(X 1 , X 2 ; U, A), \t (20b)\n(21) with pmf p(a, u, ˆ x 1 |x 1 , x 2 ) and deterministic function ˆx 2 (u, y), such that the action and the distortion constraints\nand E[d j (X 1 , X 2 , Y, ˆ X j )] ≤ D j , for j = 1, 2, \t (23) respectively, hold. Finally, U is an auxiliary random variable whose alphabet cardinality can be constrained as |U| ≤ |X 1 | |X 2 | + 4, without loss of optimality.\nRemark 4. If p(y|a, x 1 , x 2 ) = p(y|x 1 , x 2 ), Proposition 3 reduces to [9, Theorem 1].\nThe proof of converse is provided in [8]. The coding strategy that proves achievability is a combination of the techniques proposed in [1] and [9, Theorem 1]. Speciﬁ- cally, Node 1 ﬁrst maps sequences X n 1 and X n 2 into the action sequence A n and an auxiliary codeword U n using the standard joint typicality criterion. This mapping operation requires a codebook of rate I(X 1 , X 2 ; U, A). Then, given the so obtained sequences A n and U n , source sequences X n 1 and X n 2 are further mapped into the estimate ˆ X n 1 for Node 2 so that the sequences ( X n 1 , X n 2 , A n , U n , ˆ X n 1 ) are\njointly typical. This requires rate I(X 1 , X 2 ; ˆ X 1 |U, A). Lever- aging the side information X n 2 available at Node 2, con- veying the codewords A n , ˆ X n 1 and U n to Node 2 requires rate I(X 1 , X 2 ; U, A)+I(X 1 , X 2 ; ˆ X 1 |U, A)−I(U, A, ˆ X 1 ; X 2 ),\nwhich equals the right-hand side of (20a). Node 2 conveys U n and A n to Node 3 by simply forwarding the index received from Node 1 (of rate I(X 1 , X 2 ; U, A)). Finally, Node 3 estimates ˆ X n 2 through a symbol-by-symbol function as ˆ\nAs a concluding remark, one aspect worth emphasizing of the two problems solved in this paper concerns the way the side information is assumed to be available at the decoder. For distributed source coding, we have in fact assumed that side information is available in a non-causal fashion in the conventional sense of the Wyner-Ziv problem. Adaptation of the results given here to a model where side information is available only causally, in the sense of [2], proved chal- lenging and is left open by this work. On the contrary, for the cascade/triangular model, we have assumed causal side information at the decoder. In this case, adaption of the given results to the set-up of non-causal side information proved difﬁcult, and is again left as an open problem.\nThis work was supported in part by the U.S. National Science Foundation under Grant No. 0914899."},"refs":[{"authors":[{"name":"H. Permuter"},{"name":"T. Weissman"}],"title":{"text":"Source coding with a side information \u201cvending machine\u201d"}},{"authors":[{"name":"T. Weissman"},{"name":"A. El Gamal"}],"title":{"text":"Source coding with limited-look-ahead side information at the decoder"}},{"authors":[{"name":"B. Ahmadi"},{"name":"O. Simeone"}],"title":{"text":"Robust coding for lossy computing with receiver-side observation costs"}},{"authors":[{"name":"Y. Chia"},{"name":"H. Asnani"},{"name":"T. Weissman"}],"title":{"text":"Multi-terminal source coding with action dependent side information"}},{"authors":[{"name":"C. Heegard"},{"name":"T. Berger"}],"title":{"text":"Rate distortion when side information may be absent"}},{"authors":[{"name":"K. Kittichokechai"},{"name":"T. J. Oechtering"},{"name":"M. Skoglund"}],"title":{"text":"Secure Source Coding with Action-dependent Side Information"}},{"authors":[{"name":"T. Berger"},{"name":"R. Yeung"}],"title":{"text":"Multiterminal source encoding with one distortion criterion"}},{"authors":[{"name":"B. Ahmadi"},{"name":"O. Simeone"}],"title":{"text":"Distributed and Cascade Lossy Source Coding with a Side Information \"Vending Machine\""}},{"authors":[{"name":"Y.-K. Chia"},{"name":"T. Weissman"}],"title":{"text":"Cascade and triangular source coding with causal side information"}},{"authors":[{"name":"M. Gastpar"}],"title":{"text":"The Wyner\u2013Ziv problem with multiple sources"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012test/1569550425.pdf"},"links":[{"id":"1569559259","weight":37},{"id":"1569559541","weight":61},{"id":"1569559221","weight":13},{"id":"1569558785","weight":25},{"id":"1569559565","weight":41},{"id":"1569558681","weight":44},{"id":"1569559195","weight":18},{"id":"1569558859","weight":19},{"id":"1569566489","weight":17},{"id":"1569558901","weight":20},{"id":"1569559111","weight":34},{"id":"1569558985","weight":26},{"id":"1569558509","weight":18},{"id":"1569565705","weight":11},{"id":"1569551347","weight":24},{"id":"1569559199","weight":30},{"id":"1569559035","weight":14},{"id":"1569558779","weight":82},{"id":"1569559523","weight":27},{"id":"1569559597","weight":47},{"id":"1569559251","weight":38},{"id":"1569564509","weight":68},{"id":"1569558697","weight":21},{"id":"1569559233","weight":28}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S2.T1.3","endtime":"12:30","authors":"Behzad Ahmadi, Osvaldo Simeone","date":"1341231000000","papertitle":"Distributed and Cascade Lossy Source Coding with a Side Information \"Vending Machine\"","starttime":"12:10","session":"S2.T1: Network Source Coding with Side Information","room":"Kresge Rehearsal B (030)","paperid":"1569550425"},"cluster":{"jsonClass":"Map$EmptyMap$"}}

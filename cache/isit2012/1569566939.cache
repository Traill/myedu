{"id":"1569566939","paper":{"title":{"text":"Penalized Maximum Likelihood Methods for Finite Memory Estimators of Inﬁnite Memory Processes"},"authors":[{"name":"Zsolt Talata"}],"abstr":{"text":"Abstract\u2014Stationary ergodic processes with ﬁnite alphabets are estimated by ﬁnite memory processes from a sample, an n-length realization of the process. Both the transition prob- abilities and the memory depth of the estimator process are estimated from the sample using penalized maximum likelihood (PML). Under some assumptions on the continuity rate and the assumption of non-nullness, a rate of convergence in ¯ d-distance is obtained, with explicit constants. The results show an optimality of the PML Markov order estimator for not necessarily ﬁnite memory processes. Moreover, the notion of consistent Markov order estimation is generalized for inﬁnite memory processes using the concept of oracle order estimates, and generalized consistency of the PML Markov order estimator is presented."},"body":{"text":"This paper is concerned with the problem of estimating stationary ergodic processes with ﬁnite alphabet from a sam- ple, an observed length n realization of the process, with the ¯ d-distance being considered between the process and the estimated one. The ¯ d-distance was introduced by Ornstein [17] and became one of the most widely used metrics over stationary processes. Two stationary processes are close in ¯ d- distance if there is a joint distribution whose marginals are the distributions of the processes such that the marginal processes are close with high probability (see Section IV for the formal deﬁnition). The class of ergodic processes is ¯ d-closed and entropy is ¯ d-continuous, which properties do not hold for the weak topology [22].\nOrnstein and Weiss [18] proved that for stationary processes isomorphic to i.i.d. processes, the empirical distribution of the k(n)-length blocks is a strongly consistent estimator of the k(n)-length parts of the process in ¯ d-distance if and only if k(n) ≤ (log n)/h, where h denotes the entropy of the process.\nCsisz´ar and Talata [11] estimated the n-length part of a stationary ergodic process X by a Markov process of order k n . The transition probabilities of this Markov estimator process are the empirical conditional probabilities, and the order k n → +∞ does not depend on the sample. They obtained a rate of convergence of the Markov estimator to the process X in ¯ d- distance, which consists of two terms. The ﬁrst one is the bias due to the error of the approximation of the process by a Markov chain. The second term is the variation due to the\nerror of the estimation of the parameters of the Markov chain from a sample.\nModel selection methods in various settings seek a tradeoff between the bias and the variation. There are classical results aiming at identifying the balance, see for instance the indices of resolvability in the work by Barron [2], [3], [4].\nIn this paper, the order k n of the Markov estimator pro- cess is estimated from the sample. The penalized maximum likelihood (PML) is a natural generalization of the Bayesian information criterion, that is often regarded as an approxi- mation of the criteria derived from the minimum description length principle (see Section III for the formal deﬁnition). For the order estimation, PML with general penalty term is used. The resulted Markov estimator process ﬁnds a tradeoff between the bias and the variation as it uses shorter memory for faster memory decays of the process X. If the process X is a Markov chain, the PML order estimation recovers its order asymptotically with a wide range of penalty terms.\nNot only an asymptotic rate of convergence result is ob- tained but also an explicit bound on the probability that the ¯ d-distance of the above Markov estimator from the process X is greater than ε. It is assumed that the process X is non- null, that is, the conditional probabilities of the symbols given the pasts are separated from zero, and that the continuity rate of the process X is summable and the restricted continuity rate is uniformly convergent. These conditions are usually assumed in this area [7], [12], [13], [16]. The summability of the continuity rate implies that the process is isomorphic to an i.i.d. process [5].\nTo provide additional insight into the asymptotics of Markov order estimators, the notion of consistent Markov order estima- tion is generalized for inﬁnite memory processes. A Markov order estimator is compared to its oracle version, which is calculated based on the true distribution of the process instead of the empirical distribution. The oracle concept is used in various problems, see, e.g., [2], [6]. If the decay of the continuity rate of the process is faster than exponential, the ratio of the PML Markov order estimator with sufﬁciently large penalty term to its oracle version is shown to converge to 1 in probability.\nLet X = {X i , −∞ < i < +∞} be a stationary er- godic stochastic process with ﬁnite alphabet A. We write X j i = X i , . . . , X j and x j i = x i , . . . , x j ∈ A j−i+1 for j ≥ i. If j < i, x j i is the empty string. For two strings x i 1 ∈ A i and y j 1 ∈ A j , x i 1 y j 1 denotes their concatenation x 1 , . . . , x i , y 1 , . . . , y j ∈ A i+j . Write\nIf ∞ k=1 γ(k) < +∞, then the process X is said to have summable continuity rate .\nRemark 1. Since for any x −1 −k ∈ A k and z −k−1 −m ∈ A m−k , m ≥ k,\nSimilarly to Remark 1, note that the above deﬁnition is equivalent to\nHence, lim m→+∞ γ(k|m) = γ(k) for any ﬁxed k. We say that the process X has uniformly convergent restricted continuity rate with parameters θ 1 , θ 2 , k θ if\nLogarithms are to the base 2. It is well-known for stationary processes that the conditional entropy h k is a non-negative\ndecreasing function of k, therefore its limit exists as k → +∞. The entropy rate of the process is\n1 k\nThe process X is a Markov chain of order k if for each n > k and x n 1 ∈ A n\nwhere P (x k 1 ) is called initial distribution and P (a|a k 1 ), a ∈ A, a k 1 ∈ A k is called transition probability\nmatrix. The case k = 0 corresponds to i.i.d. processes. The process X is of inﬁnite memory if it is not a Markov chain for any order k < +∞. For inﬁnite memory processes, h k − ¯ H > 0 for any k ≥ 0.\nIn this paper, we consider statistical estimates based on a sample X n 1 , an n-length part of the process. Let N n (a k 1 ) denote the number of occurrences of the string a k 1 in the sample X n 1\nand the empirical conditional probability of a ∈ A given a k 1 is\nFor k = 0, ˆ P (a k+1 | a k 1 ) = ˆ P (a k+1 ). The k-order empirical entropy is\nAn information criterion assigns a score to each hypothetical model (here, Markov chain order) based on a sample, and the estimator will be that model whose score is minimal.\nthe Markov order estimator bounded by r n < n, r n ∈ N, is ˆ k IC (X n 1 | r n ) = arg min\nRemark 3. Here, the number of candidate Markov chain orders based on a sample is ﬁnite, therefore the minimum is attained. If the minimizer is not unique, the smallest one will be taken as arg min.\nA popular approach to choosing information criteria is the minimum description length (MDL) principle [19], [4]. In particular, the normalized maximum likelihood (NML) [23] and the Krichevsky\u2013Troﬁmov (KT) [15] code lengths are natural information criteria because the former minimizes the worst case maximum redundancy for the model class of k- order Markov chains, while the latter does so, up to an additive constant, with the average redundancy. The Bayesian informa- tion criterion (BIC) [20] can be regarded as an approximation of the NML and KT code lengths. The family of penalized maximum likelihood (PML) is a generalization of BIC.\nThe likelihood of the sample X n 1 with respect to a k- order Markov chain model of the process X with some tran- sition probability matrix Q(a k+1 |a k 1 ), a k+1 ∈ A, a k 1 ∈ A k , by (1), is\nFor 0 ≤ k < n, the maximum likelihood is the maximum in Q(a k+1 |a k 1 ) of the second factor above, which equals\nDeﬁnition 4. Given a penalty function pen(n), a non- decreasing function of the sample size n, for a candidate order 0 ≤ k < n the PML criterion is\nPML X n 1 (k) = − log ML k (X n 1 ) + (|A| − 1)|A| k pen (n) = (n − k) ˆ h k (X n 1 ) + (|A| − 1)|A| k pen (n).\nThe k-order Markov chain model of the process X is described by the conditional probabilities\nQ(a k+1 |a k 1 ), a k+1 ∈ A, a k 1 ∈ A k , and (|A| − 1)|A| k of these are free parameters.\nThe second term of the PML criterion, which is proportional to the number of free parameters of the k-order Markov chain model, is increasing in k. The ﬁrst term, for a given sample, is known to be decreasing in k. Hence, minimizing the criterion yields a tradeoff between the goodness of ﬁt of the sample to the model and the complexity of the model.\nRemark 5. If pen(n) = 1 2 log n, the PML criterion is called Bayesian information criterion (BIC), and if pen(n) = 1, Akaike information criterion (AIC) [1].\nThe problem of statistical estimation of stationary ergodic processes by ﬁnite memory processes is considered, and the following distance is used. The per-letter Hamming distance between two strings x n 1 and y n 1 is\nand the ¯ d-distance between two random sequences X n 1 and Y n 1 is deﬁned by\nwhere the minimum is taken over all the joint distributions P of ˜ X n 1 and ˜ Y n 1 whose marginals are equal to the distributions of X n 1 and Y n 1 .\nThe process X is estimated by a Markov chain of order k = k n from the sample in the following way.\nDeﬁnition 6. The empirical k-order Markov estimator of a process X based on the sample X n 1 is the stationary Markov chain, denoted by ˆ X[k], of order k with transition probability matrix ˆ P (a k+1 | a k 1 ), a k+1 ∈ A, a k 1 ∈ A k . If the initial distribution of a stationary Markov chain with these transition probabilities is not unique, then any of these initial distributions can be taken.\nThe order k of the empirical Markov estimator ˆ X[k] is estimated from the sample, using the PML criterion. The estimated order needs to be bounded to guarantee an accurate assessment of the memory decay of the process.\nThe optimal order can be smaller than the upper bound if the memory decay of the process is sufﬁciently fast. Deﬁne\nwhere f (n) \t 0 and r n \t ∞. Since γ is a decreasing function, K n increases in n but does not exceed r n . It is less than r n if γ vanishes sufﬁciently fast, and then the faster γ vanishes, the slower K n increases.\nTheorem 7. For any non-null stationary ergodic process with summable continuity rate and uniformly convergent restricted continuity rate with parameters θ 1 , θ 2 , k θ , and for any µ n > 0, the empirical Markov estimator of the process with the order estimated by the bounded PML Markov order estimator ˆ k n = ˆ k PML (X n\nif n ≥ n 0 , where c > 0 is an arbitrary constant, s n → ∞ and β 2 , c 4 , c 5 , n 0 > 0 are constants depending only on the distribution of the process.\nRemark 8. If the process X is a Markov chain of order k, then the restricted continuity rate is uniformly convergent with parameters θ 1 = 1, θ 2 > 1 arbitrary (arbitrarily close to 1), k θ = k + 1, and if n is sufﬁciently large, K n = k and\nAn application of the Borel\u2013Cantelli lemma in Theorem 7 yields the following asymptotic result.\nCorollary 9. For any non-null stationary ergodic process with summable continuity rate and uniformly convergent restricted continuity rate with parameters θ 1 , θ 2 , k θ , the empirical Markov estimator of the process with the order estimated by the bounded PML Markov order estimator ˆ k n = ˆ k PML (X n 1 | r n ) with 1 2 log n ≤ pen(n) ≤ O(\neventually almost surely as n → +∞, where c > 0 is an arbitrary constant, and β 2 , c 6 > 0 are constants depending only on the distribution of the process.\nRemark 10. In Corollary 9, in the upper bound the ﬁrst term is the bias due to the error of the approximation of the process by a Markov chain. The second term is the variation due to the error of the estimation of the order and the parameters of the Markov chain based on a sample. If the memory decay of the process is slow, the bias is essentially γ( r n /θ 2 ), and the variance is maximal. If the memory decay is sufﬁciently fast, then the rate of the estimated order ˆ k n and the rate of K n are smaller, therefore the variance term is smaller while the\nbias term is smaller as well. The result, however, shows the optimality of the PML Markov order estimator in the sense that it selects an order which is small enough to allow the variance to decrease but large enough to keep the bias below a polynomial threshold.\nThe BIC Markov order estimator is strongly consistent [8], that is, if the process is a Markov chain of order k, then ˆ k BIC (X n 1 ) = k eventually almost surely as n → +∞. Increas- ing the penalty term, up to cn, where c > 0 is a sufﬁciently small constant, does not affect the strong consistency. It is not known whether or not the strong consistency holds for smaller penalty terms but it is known that if the candidate orders are upper bounded by c log n, where c > 0 is a sufﬁciently small constant, that is, the estimator minimizes the PML over the orders 0 ≤ k ≤ c log n only, then pen(n) = C log log n still provides the strong consistency, where C > 0 is a sufﬁciently large constant [14].\nThe NML and KT Markov order estimators fail to be strongly consistent because for i.i.d. processes with uniform distribution, they converge to inﬁnity at a rate O(log n) [8]. However, if the candidate orders are upper bounded by o(log n), the strong consistency holds true [9].\nIf the process is of inﬁnite memory, the BIC and KT Markov order estimators diverge to inﬁnity [10], [26]. In [24], results on the divergence rate of the PML, NML and KT Markov order estimators are presented. Bounds on the probability that the estimators are greater and less, respectively, than some order are obtained, with explicit constants. The ﬁrst implies that under mild conditions, the estimators do not exceed the O(log n) rate eventually almost surely as n → +∞. The second bound implies that the rate O(log n) is attained eventually almost surely as n → +∞ for the processes whose continuity rates decay in some exponential range.\nIn this section, the notion of consistent Markov order estimation is generalized for inﬁnite memory processes. If the continuity rates decay faster than exponential, the PML Markov order estimator is shown to be consistent with the oracle-type order estimate.\nIn the previous section, non-nullness is assumed for the process. In this section the process X is assumed to be only weakly non-null , that is,\nMoreover, the PML Markov order estimator does not need to be bounded. Therefore, let\nDeﬁnition 11. For a candidate order 0 ≤ k < n the oracle PML criterion is\nRemark 12. For Markov chains of order k, k PML,n = k if n is sufﬁciently large, with any pen(n) = o(n).\nTheorem 13. For any weakly non-null stationary ergodic process with\nthe PML Markov order estimator ˆ k PML (X n 1 ) with pen(n) = n κ , 1 2 < κ < 1, is consistent in the sense that\nThe proof of Theorem 7, the process estimation result, relies on two bounds. First, an upper bound on the probability that the bounded PML Markov order estimator is less than some order is derived using non-nullness, summability of the continuity rate and uniform convergence of the restricted con- tinuity rate. Then, for any non-null stationary ergodic process, an upper bound on the probability that the bounded PML Markov order estimator is greater than some order is derived using the relationship between the maximum likelihood and the Krichevsky\u2013Troﬁmov distribution. The proof is completed by the result established in [11].\nThe proof of Theorem 13, the consistency result, is based on the following observation. As the order of the estimator process increases, a penalty term growing sufﬁciently fast can control the increasing variance to prevent overestimation. On the other hand, a large penalty term requires fast decay of the continuity rate to prevent underestimation.\nThe proofs provide explicit constants in all bounds. The full proofs can be found in [25].\nIn this paper, stationary ergodic processes have been es- timated by ﬁnite memory processes from a sample, where the memory depth of the estimator process is also estimated from the sample using PML. Under some assumptions on the process, a rate of convergence in ¯ d-distance has been obtained. The results show an optimality of the PML Markov order estimator for not necessarily ﬁnite memory processes. Moreover, the PML Markov order estimator has been shown to be consistent with the oracle-type order estimate under some assumptions on the process. Notice that the consistency result requires larger penalty terms for PML than the process esti- mation result. This reﬂects the expectation that the estimation of the structure parameter needs larger penalty terms than the estimation of the sampling distribution; see, for example, [21] and [20].\nIn this research, Talata is supported by the NSF grant DMS 0906929."},"refs":[{"authors":[{"name":"H. Akaike"}],"title":{"text":"Information theory and an extension of the maximum like- lihood principle"}},{"authors":[{"name":"A. R. Barron"},{"name":"L. Birg´e"},{"name":"P. Massart"}],"title":{"text":"Risk bounds for model selection via penalization"}},{"authors":[{"name":"A. R. Barron"},{"name":"T. M. Cover"}],"title":{"text":"Minimum complexity density estimation"}},{"authors":[{"name":"A. R. Barron"},{"name":"J. Rissanen"},{"name":"B. Yu"}],"title":{"text":"The minimum description length principle in coding and modeling"}},{"authors":[{"name":"H. Berbee"}],"title":{"text":"Chains with inﬁnite connections: uniqueness and Markov representation"}},{"authors":[{"name":"L. Birg´e"},{"name":"P. Massart"}],"title":{"text":"Gaussian model selection"}},{"authors":[{"name":"X. Bressaud"},{"name":"R. Fernandez"},{"name":"A. Galves"}],"title":{"text":"Speed of ¯ d-convergence for Markov approximations of chains with complete connections. A coupling approach"}},{"authors":[{"name":"I. Csisz´ar"},{"name":"P. C. Shields"}],"title":{"text":"The consistency of the BIC Markov order estimator"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Large-scale typicality of Markov sample paths and consistency of MDL order estimators"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"Context tree estimation for not necessarily ﬁnite memory processes, via BIC and MDL"}},{"authors":[{"name":"I. Csisz´ar"}],"title":{"text":"On Rate of Convergence of Statistical Estimation of Stationary Ergodic Processes"}},{"authors":[{"name":"D. Duarte"},{"name":"A. Galves"},{"name":"N. Garcia"}],"title":{"text":"Markov approximation and consistent estimation of unbounded probabilistic sufﬁx trees"}},{"authors":[{"name":"R. Fern´andez"},{"name":"A. Galves"}],"title":{"text":"Markov Approximations of Chains of Inﬁnite Order"}},{"authors":[{"name":"R. van Handel"}],"title":{"text":"On the minimal penalty for Markov order estimation"}},{"authors":[{"name":"R. E. Krichevsky"},{"name":"V. K. Troﬁmov"}],"title":{"text":"The performance of universal encoding"}},{"authors":[{"name":"K. Marton"}],"title":{"text":"Measure Concentration for a Class of Random Processes"}},{"authors":[{"name":"D. S. Ornstein"}],"title":{"text":"An Application of Ergodic Theory to Probability Theory"}},{"authors":[{"name":"D. S. Ornstein"},{"name":"B. Weiss"}],"title":{"text":"How sampling reveals a process"}},{"authors":[{"name":"J. Rissane"}],"title":{"text":"Stochastic Complexity in Statistical Inquiry"}},{"authors":[{"name":"G. Schwarz"}],"title":{"text":"Estimating the dimension of a model"}},{"authors":[{"name":"R. Shibata"}],"title":{"text":"Asymptotically efﬁcient selection of the order of the model for estimating parameters of a linear process"}},{"authors":[{"name":"P. Shield"}],"title":{"text":"The ergodic theory of discrete sample paths"}},{"authors":[{"name":"J. Shtarkov"}],"title":{"text":"Coding of discrete sources with unknown statistics"}},{"authors":[],"title":{"text":"Divergence of information-criterion based Markov order estimators for inﬁnite memory processes"}},{"authors":[],"title":{"text":"Divergence Rates of Markov Order Estimators and Their Application to Statistical Estimation of Stationary Ergodic Processes"}},{"authors":[{"name":"E. Duncan"}],"title":{"text":"BIC context tree estimation for stationary ergodic processes"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569566939.pdf"},"links":[{"id":"1569566567","weight":2},{"id":"1569564843","weight":4},{"id":"1569566527","weight":3},{"id":"1569566485","weight":2},{"id":"1569565383","weight":8},{"id":"1569565883","weight":2},{"id":"1569564889","weight":2},{"id":"1569565223","weight":2},{"id":"1569566725","weight":3},{"id":"1569565663","weight":2},{"id":"1569565377","weight":7},{"id":"1569566385","weight":4},{"id":"1569564635","weight":4},{"id":"1569565867","weight":2},{"id":"1569566799","weight":3},{"id":"1569565067","weight":3},{"id":"1569559665","weight":2},{"id":"1569561021","weight":2},{"id":"1569564669","weight":2},{"id":"1569566815","weight":7},{"id":"1569566875","weight":2},{"id":"1569559617","weight":2},{"id":"1569566981","weight":9},{"id":"1569566433","weight":2},{"id":"1569566321","weight":3},{"id":"1569566605","weight":5},{"id":"1569566683","weight":2},{"id":"1569566855","weight":4},{"id":"1569560629","weight":18},{"id":"1569566869","weight":5},{"id":"1569565097","weight":2},{"id":"1569566227","weight":4},{"id":"1569566091","weight":4},{"id":"1569559259","weight":2},{"id":"1569566697","weight":2},{"id":"1569566597","weight":6},{"id":"1569565551","weight":2},{"id":"1569566761","weight":2},{"id":"1569566943","weight":3},{"id":"1569565091","weight":4},{"id":"1569566591","weight":11},{"id":"1569566571","weight":4},{"id":"1569552245","weight":4},{"id":"1569565607","weight":2},{"id":"1569565495","weight":3},{"id":"1569567045","weight":2},{"id":"1569565227","weight":3},{"id":"1569564481","weight":6},{"id":"1569560833","weight":2},{"id":"1569566415","weight":3},{"id":"1569564805","weight":3},{"id":"1569567005","weight":9},{"id":"1569566081","weight":3},{"id":"1569565613","weight":2},{"id":"1569565355","weight":5},{"id":"1569564469","weight":2},{"id":"1569565931","weight":5},{"id":"1569566373","weight":10},{"id":"1569566647","weight":3},{"id":"1569551535","weight":8},{"id":"1569566765","weight":3},{"id":"1569565775","weight":2},{"id":"1569565547","weight":33},{"id":"1569566871","weight":5},{"id":"1569565461","weight":5},{"id":"1569564731","weight":16},{"id":"1569565171","weight":2},{"id":"1569566207","weight":3},{"id":"1569564227","weight":6},{"id":"1569558325","weight":3},{"id":"1569566671","weight":9},{"id":"1569566303","weight":2},{"id":"1569566119","weight":2},{"id":"1569564233","weight":4},{"id":"1569566459","weight":5},{"id":"1569563411","weight":5},{"id":"1569564401","weight":5},{"id":"1569564849","weight":3},{"id":"1569559541","weight":3},{"id":"1569565317","weight":2},{"id":"1569566363","weight":2},{"id":"1569565123","weight":4},{"id":"1569566941","weight":2},{"id":"1569566033","weight":2},{"id":"1569555811","weight":3},{"id":"1569565609","weight":2},{"id":"1569565291","weight":7},{"id":"1569564203","weight":3},{"id":"1569566821","weight":3},{"id":"1569556713","weight":4},{"id":"1569562685","weight":4},{"id":"1569566467","weight":2},{"id":"1569565771","weight":3},{"id":"1569560613","weight":3},{"id":"1569566903","weight":2},{"id":"1569566999","weight":14},{"id":"1569565859","weight":2},{"id":"1569564249","weight":3},{"id":"1569566843","weight":3},{"id":"1569566579","weight":8},{"id":"1569558483","weight":2},{"id":"1569566563","weight":2},{"id":"1569566089","weight":2},{"id":"1569559221","weight":3},{"id":"1569556091","weight":7},{"id":"1569565347","weight":4},{"id":"1569566925","weight":3},{"id":"1569564387","weight":2},{"id":"1569565455","weight":2},{"id":"1569566497","weight":4},{"id":"1569566963","weight":2},{"id":"1569566709","weight":5},{"id":"1569564989","weight":4},{"id":"1569566787","weight":2},{"id":"1569560721","weight":2},{"id":"1569566523","weight":2},{"id":"1569565897","weight":4},{"id":"1569551763","weight":2},{"id":"1569565953","weight":3},{"id":"1569566269","weight":5},{"id":"1569564189","weight":3},{"id":"1569564195","weight":3},{"id":"1569561513","weight":7},{"id":"1569566985","weight":5},{"id":"1569567009","weight":3},{"id":"1569566865","weight":2},{"id":"1569566095","weight":2},{"id":"1569565907","weight":2},{"id":"1569566343","weight":2},{"id":"1569565785","weight":7},{"id":"1569566167","weight":2},{"id":"1569566679","weight":4},{"id":"1569565989","weight":6},{"id":"1569566575","weight":3},{"id":"1569563981","weight":9},{"id":"1569561085","weight":5},{"id":"1569566617","weight":3},{"id":"1569559565","weight":3},{"id":"1569566905","weight":5},{"id":"1569563307","weight":7},{"id":"1569566063","weight":3},{"id":"1569558681","weight":21},{"id":"1569566759","weight":6},{"id":"1569565589","weight":2},{"id":"1569566149","weight":5},{"id":"1569559995","weight":2},{"id":"1569566657","weight":8},{"id":"1569558859","weight":3},{"id":"1569565199","weight":2},{"id":"1569565213","weight":3},{"id":"1569565365","weight":2},{"id":"1569566511","weight":5},{"id":"1569566531","weight":3},{"id":"1569567665","weight":4},{"id":"1569561143","weight":5},{"id":"1569566581","weight":4},{"id":"1569566489","weight":2},{"id":"1569564611","weight":2},{"id":"1569565535","weight":3},{"id":"1569562867","weight":3},{"id":"1569565667","weight":3},{"id":"1569561795","weight":2},{"id":"1569566845","weight":9},{"id":"1569566325","weight":2},{"id":"1569566423","weight":3},{"id":"1569564795","weight":6},{"id":"1569567015","weight":5},{"id":"1569566437","weight":4},{"id":"1569566851","weight":7},{"id":"1569558901","weight":3},{"id":"1569565735","weight":3},{"id":"1569553909","weight":3},{"id":"1569559111","weight":4},{"id":"1569553537","weight":13},{"id":"1569565427","weight":10},{"id":"1569566403","weight":3},{"id":"1569565839","weight":2},{"id":"1569565915","weight":2},{"id":"1569552251","weight":6},{"id":"1569553519","weight":3},{"id":"1569567051","weight":2},{"id":"1569564441","weight":3},{"id":"1569566231","weight":2},{"id":"1569564209","weight":6},{"id":"1569566513","weight":2},{"id":"1569566425","weight":2},{"id":"1569554881","weight":2},{"id":"1569554971","weight":6},{"id":"1569565501","weight":4},{"id":"1569566899","weight":5},{"id":"1569566209","weight":7},{"id":"1569566791","weight":3},{"id":"1569565655","weight":3},{"id":"1569566127","weight":2},{"id":"1569558985","weight":7},{"id":"1569563763","weight":2},{"id":"1569566473","weight":4},{"id":"1569566913","weight":3},{"id":"1569566629","weight":5},{"id":"1569566257","weight":5},{"id":"1569565033","weight":4},{"id":"1569566447","weight":10},{"id":"1569565817","weight":2},{"id":"1569565847","weight":5},{"id":"1569564353","weight":3},{"id":"1569565887","weight":3},{"id":"1569565929","weight":2},{"id":"1569566141","weight":3},{"id":"1569566721","weight":2},{"id":"1569565055","weight":2},{"id":"1569565633","weight":3},{"id":"1569555879","weight":3},{"id":"1569565521","weight":2},{"id":"1569566115","weight":3},{"id":"1569565219","weight":3},{"id":"1569558509","weight":3},{"id":"1569565595","weight":2},{"id":"1569565185","weight":2},{"id":"1569566773","weight":2},{"id":"1569566223","weight":2},{"id":"1569566553","weight":3},{"id":"1569564973","weight":3},{"id":"1569565469","weight":2},{"id":"1569564969","weight":4},{"id":"1569566043","weight":7},{"id":"1569565029","weight":3},{"id":"1569565357","weight":2},{"id":"1569561245","weight":2},{"id":"1569566505","weight":6},{"id":"1569565393","weight":7},{"id":"1569565933","weight":2},{"id":"1569562207","weight":2},{"id":"1569566191","weight":3},{"id":"1569567033","weight":6},{"id":"1569565527","weight":3},{"id":"1569566603","weight":4},{"id":"1569567029","weight":2},{"id":"1569565363","weight":3},{"id":"1569566051","weight":4},{"id":"1569561379","weight":4},{"id":"1569561123","weight":4},{"id":"1569565467","weight":2},{"id":"1569566655","weight":3},{"id":"1569566673","weight":3},{"id":"1569567235","weight":4},{"id":"1569566233","weight":5},{"id":"1569566893","weight":2},{"id":"1569566317","weight":3},{"id":"1569564097","weight":8},{"id":"1569560997","weight":4},{"id":"1569563845","weight":5},{"id":"1569560349","weight":2},{"id":"1569566501","weight":4},{"id":"1569565741","weight":5},{"id":"1569566275","weight":2},{"id":"1569566481","weight":2},{"id":"1569565545","weight":4},{"id":"1569566857","weight":4},{"id":"1569566245","weight":2},{"id":"1569560503","weight":2},{"id":"1569565463","weight":3},{"id":"1569566219","weight":2},{"id":"1569565439","weight":11},{"id":"1569566229","weight":3},{"id":"1569566949","weight":4},{"id":"1569562551","weight":3},{"id":"1569563395","weight":2},{"id":"1569566901","weight":2},{"id":"1569551347","weight":3},{"id":"1569565415","weight":3},{"id":"1569555367","weight":3},{"id":"1569561623","weight":9},{"id":"1569566383","weight":4},{"id":"1569566631","weight":2},{"id":"1569565571","weight":3},{"id":"1569565885","weight":2},{"id":"1569566177","weight":4},{"id":"1569557633","weight":2},{"id":"1569564411","weight":2},{"id":"1569559199","weight":2},{"id":"1569566293","weight":2},{"id":"1569565665","weight":3},{"id":"1569566831","weight":16},{"id":"1569565549","weight":3},{"id":"1569565523","weight":3},{"id":"1569565611","weight":3},{"id":"1569557715","weight":2},{"id":"1569564175","weight":2},{"id":"1569566983","weight":8},{"id":"1569566779","weight":2},{"id":"1569566097","weight":4},{"id":"1569556361","weight":2},{"id":"1569565397","weight":3},{"id":"1569566873","weight":6},{"id":"1569565765","weight":4},{"id":"1569565435","weight":3},{"id":"1569557275","weight":3},{"id":"1569566261","weight":2},{"id":"1569565093","weight":2},{"id":"1569565385","weight":4},{"id":"1569565575","weight":3},{"id":"1569565919","weight":6},{"id":"1569565181","weight":3},{"id":"1569566711","weight":4},{"id":"1569566927","weight":2},{"id":"1569566887","weight":3},{"id":"1569565273","weight":5},{"id":"1569565319","weight":2},{"id":"1569566267","weight":3},{"id":"1569564131","weight":3},{"id":"1569552037","weight":2},{"id":"1569564919","weight":6},{"id":"1569565511","weight":4},{"id":"1569566737","weight":9},{"id":"1569561221","weight":2},{"id":"1569564595","weight":3},{"id":"1569566917","weight":6},{"id":"1569566253","weight":2},{"id":"1569565353","weight":2},{"id":"1569564683","weight":3},{"id":"1569564305","weight":4},{"id":"1569564283","weight":2},{"id":"1569564291","weight":2},{"id":"1569566691","weight":2},{"id":"1569565421","weight":39},{"id":"1569566547","weight":4},{"id":"1569566651","weight":2},{"id":"1569566823","weight":2},{"id":"1569566595","weight":14},{"id":"1569566677","weight":3},{"id":"1569552025","weight":2},{"id":"1569566237","weight":4},{"id":"1569565645","weight":2},{"id":"1569565375","weight":3},{"id":"1569566715","weight":4},{"id":"1569565237","weight":2},{"id":"1569566639","weight":4},{"id":"1569566819","weight":2},{"id":"1569565041","weight":2},{"id":"1569564703","weight":7},{"id":"1569566713","weight":2},{"id":"1569566813","weight":2},{"id":"1569565293","weight":2},{"id":"1569566771","weight":3},{"id":"1569564201","weight":2},{"id":"1569562277","weight":2},{"id":"1569566641","weight":6},{"id":"1569565425","weight":5},{"id":"1569564247","weight":4},{"id":"1569564437","weight":5},{"id":"1569551905","weight":3},{"id":"1569564861","weight":2},{"id":"1569565457","weight":5},{"id":"1569564787","weight":42},{"id":"1569566487","weight":4},{"id":"1569565529","weight":7},{"id":"1569556759","weight":5},{"id":"1569566619","weight":2},{"id":"1569561185","weight":3},{"id":"1569558779","weight":4},{"id":"1569565669","weight":2},{"id":"1569565233","weight":3},{"id":"1569563721","weight":2},{"id":"1569565593","weight":2},{"id":"1569566817","weight":2},{"id":"1569567483","weight":2},{"id":"1569564923","weight":2},{"id":"1569565367","weight":10},{"id":"1569566299","weight":4},{"id":"1569564281","weight":2},{"id":"1569564769","weight":2},{"id":"1569565769","weight":2},{"id":"1569566171","weight":5},{"id":"1569565805","weight":7},{"id":"1569561713","weight":2},{"id":"1569566933","weight":4},{"id":"1569563919","weight":2},{"id":"1569566577","weight":2},{"id":"1569557851","weight":4},{"id":"1569565389","weight":11},{"id":"1569559919","weight":6},{"id":"1569565861","weight":2},{"id":"1569566147","weight":5},{"id":"1569565537","weight":2},{"id":"1569566057","weight":7},{"id":"1569562367","weight":7},{"id":"1569560213","weight":2},{"id":"1569566457","weight":2},{"id":"1569555891","weight":6},{"id":"1569565997","weight":2},{"id":"1569563425","weight":2},{"id":"1569565035","weight":3},{"id":"1569559597","weight":7},{"id":"1569564961","weight":2},{"id":"1569559251","weight":2},{"id":"1569565089","weight":10},{"id":"1569567013","weight":2},{"id":"1569566583","weight":2},{"id":"1569561861","weight":2},{"id":"1569565337","weight":4},{"id":"1569565737","weight":4},{"id":"1569560459","weight":2},{"id":"1569564463","weight":2},{"id":"1569565853","weight":2},{"id":"1569550425","weight":4},{"id":"1569566273","weight":2},{"id":"1569564123","weight":3},{"id":"1569566341","weight":3},{"id":"1569565889","weight":4},{"id":"1569566635","weight":2},{"id":"1569566611","weight":3},{"id":"1569563725","weight":3},{"id":"1569551539","weight":4},{"id":"1569564505","weight":2},{"id":"1569565565","weight":6},{"id":"1569565635","weight":6},{"id":"1569566797","weight":2},{"id":"1569566125","weight":2},{"id":"1569566413","weight":2},{"id":"1569565707","weight":3},{"id":"1569565113","weight":4},{"id":"1569566375","weight":4},{"id":"1569564257","weight":6},{"id":"1569564931","weight":2},{"id":"1569565373","weight":3},{"id":"1569566973","weight":2},{"id":"1569561579","weight":4},{"id":"1569566449","weight":5},{"id":"1569566987","weight":2},{"id":"1569565031","weight":5},{"id":"1569551541","weight":6},{"id":"1569566839","weight":7},{"id":"1569551751","weight":2},{"id":"1569565139","weight":4},{"id":"1569566663","weight":2},{"id":"1569564419","weight":13},{"id":"1569566067","weight":7},{"id":"1569566825","weight":6},{"id":"1569564807","weight":2},{"id":"1569563007","weight":6},{"id":"1569566113","weight":4},{"id":"1569566443","weight":5},{"id":"1569566727","weight":7},{"id":"1569565315","weight":2},{"id":"1569560581","weight":2},{"id":"1569559233","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S5.T8.1","endtime":"10:10","authors":"Zsolt Talata","date":"1341309000000","papertitle":"Penalized Maximum Likelihood Methods for Finite Memory Estimators of Infinite Memory Processes","starttime":"09:50","session":"S5.T8: Prediction and Estimation","room":"Stratton (491)","paperid":"1569566939"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"5","spectral43":"9","spectral28":"4","spectral32":"22","spectral14":"13","spectral20":"5","spectral9":"6","spectral25":"4","spectral42":"32","spectral3":"1","spectral47":"33","spectral17":"16","louvain":"512","spectral36":"22","spectral39":"19","spectral10":"4","spectral15":"5","spectral33":"32","spectral5":"4","spectral21":"10","spectral44":"9","spectral26":"22","spectral40":"22","spectral8":"1","spectral11":"4","spectral4":"2","spectral37":"19","spectral48":"26","spectral22":"4","spectral23":"5","spectral12":"9","spectral50":"41","spectral19":"13","spectral34":"3","spectral45":"33","spectral7":"5","spectral49":"45","spectral38":"30","spectral24":"21","spectral13":"1","spectral31":"27","spectral29":"10","spectral35":"4","spectral30":"14","spectral41":"8","spectral27":"25","spectral18":"9","spectral46":"11","spectral2":"0","spectral16":"5"}}

{"id":"1569561513","paper":{"title":{"text":"q -Gaussian based Smoothed Functional Algorithms for Stochastic Optimization"},"authors":[{"name":"Debarghya Ghoshdastidar"},{"name":"Ambedkar Dukkipati"},{"name":"Shalabh Bhatnagar"}],"abstr":{"text":"Abstract\u2014The q-Gaussian distribution results from maximiz- ing certain generalizations of Shannon entropy under some constraints. The importance of q-Gaussian distributions stems from the fact that they exhibit power-law behavior, and also generalize Gaussian distributions. In this paper, we propose a Smoothed Functional (SF) scheme for gradient estimation using q-Gaussian distribution, and also propose an algorithm for optimization based on the above scheme. Convergence results of the algorithm are presented. Performance of the proposed algorithm is shown by simulation results on a queuing model."},"body":{"text":"Stochastic optimization algorithms play an important role in optimization problems involving objective functions that can- not be computed analytically. These schemes are extensively used in discrete event systems, such as queuing systems, for obtaining optimal or near-optimal performance measures.\nGradient descent algorithms are used for stochastic opti- mization by estimating the gradient of average cost in the long run. Methods for gradient estimation by random perturbation of parameters have been proposed in [1]. The Smoothed Func- tional (SF) scheme, described in [2], approximates the gradient of expected cost by its convolution with a multivariate normal distribution. Based on all the above schemes, two-timescale stochastic approximation algorithms have been presented in [3], which simultaneously perform cost averaging and param- eter updation using different step-size schedules. The main issue with such algorithms is that, although convergence to a local optimum is guaranteed, the global optimum cannot achieved in practice. Hence, new methods are sought.\nIn this paper, we propose a new SF technique based on q-Gaussian distribution, which is a generalization of the Gaussian distribution. We show that q-Gaussian satisﬁes all the conditions for smoothing kernels proposed by Rubin- stein [4]. We illustrate a method for gradient estimation using q-Gaussian. We also present a two-timescale algorithm for stochastic optimization using q-Gaussian based SF, and show the convergence of the proposed algorithm.\nThe rest of the paper is organized as follows. The framework for the optimization problem and some of the preliminaries are presented in Section II. Gradient estimation using q-Gaussian SF has been derived in Section III. Section IV presents the proposed algorithm. Numerical experiments comparing our algorithm with a previous algorithm is presented in Sec- tion V. An outline of convergence analysis of our algorithm\nis discussed in Section VI. Finally, Section VII provides the concluding remarks.\nMost of the distributions, like normal, uniform, exponen- tial etc., can be obtained by maximizing Shannon entropy functional deﬁned as H(p) =\np(x)lnp(x)dx, where p is a pdf deﬁned on the sample space X . Other entropy functions have also been proposed as generalized information measures. One of the most popular among them is nonextensive entropy, ﬁrst introduced in [5], and later studied by Tsallis [6]. Its continuous form entropy functional, which is consistent with the discrete case [7], is deﬁned as\nq − 1 \t , \t q ∈ R. \t (1) This entropy functional produces Shannon entropy as q → 1. Corresponding to this generalized measure, q-expectation of a function f (.) can be deﬁned as\nwhere y + = max(y, 0) is called Tsallis cut-off condition, and K q is the normalizing constant, which depends on the value of q. The function deﬁned in (4) is not integrable for q 3, and hence, q-Gaussian is a probability density function only for q < 3. Multivariate form of the q-Gaussian distribution [9] is deﬁned as\nwhere K q,N is the normalizing constant. It is easy to verify that the multivariate normal distribution is a special case of (5) as q → 1. A similar distribution can also be obtained by maximizing R´enyi entropy [10].\nLet {Y n } n∈N ⊂ R d be a parameterized Markov process, depending on a tunable parameter θ ∈ C, where C is a compact and convex subset of R N . Let P θ (x, dy) denote the transition kernel of {Y n } when the operative parameter is θ ∈ C. Let h : R d → R + {0} be a Lipschitz continuous cost function associated with the process.\nAssumption I. The process {Y n } is ergodic for any given θ as the operative parameter, i.e.,\n1 L\n1 L\nby choosing an appropriate θ ∈ C. The existence of the above limit is given by Assumption I. In addition, we assume that the average cost J (θ) satisﬁes the following condition.\nAssumption II. J (θ) is continuously differentiable with re- spect to any θ ∈ C.\nWe also assume the existence of a stochastic Lyapunov function through the following assumption.\nAssumption III. Let {θ(n)} be a sequence of random pa- rameters, obtained using an iterative scheme, controlling the process {Y n }, and F n = σ(θ(m), Y m , m n), n 0 denote the sequence of associated σ-ﬁelds.\nThere exists 0 > 0, K ⊂ R d compact, and a continuous R d -valued function V , with lim x →∞ V (x) = ∞, such that under any non-anticipative {θ(n)},\nAssumption II is a technical requirement, whereas As- sumption III is used to show the stability of the scheme. Assumption III will not be required, for instance, if the single- stage cost function h is bounded in addition.\nGiven any function f : C → R, its smoothed functional is deﬁned as\n(7) where G β : R N → R is a kernel function.\nThe idea behind using smoothed functionals is that if f (θ) is not well-behaved, i.e., it has a ﬂuctuating character, then S β [f (θ)] has less ﬂuctuations for appropriate values of β. This ensures that any optimization algorithm with objective function f (θ) does not get stuck at any local minimum, but converges to the global minimum. The parameter β controls the degree of smoothness. Rubinstein [4] has shown that the SF algorithm achieves these properties if the kernel function satisﬁes the following sufﬁcient conditions:\nwhere G( η β ) = G 1 ( η β ) = G 1 ( η (1) β , η (2) β , . . . , η (N ) β ). (P2) G β (η) is piecewise differentiable in η.\n(P3) G β (η) is a probability distribution function, i.e. , S β [f (θ)] = E G β (η) [f (θ − η)].\nThe normal distribution satisﬁes the above conditions, and has been used as a kernel by Katkovnik [2].\nBased on (7), a form of gradient estimator has been derived in [3] which is given by\nfor large M , L and small β. The process {Y m } is governed by parameter (θ(n) + βη(n)), where θ(n) ∈ C ⊂ R N is obtained through an iterative scheme. η(n) is a N -dimensional vector composed of i.i.d. N (0, 1)-distributed random variables.\nProposition 3.1. The q-Gaussian distribution satisﬁes the kernel properties (P1) \u2013 (P5) for all q < 3, q = 1.\nThus, G q,β (η) is differentiable for q > 1, and piecewise differentiable for q < 1.\n(P3) G q,β (η) is a distribution for q < 3 and hence, the corresponding SF S q,β (.), which is parameterized by both q and β can be written as\nOur objective is to estimate θ J (θ) using the SF approach. The existence of θ J (θ) is due to Assumption II. Now,\nLet us deﬁne, Ω q = η ∈ R N : η 2 < (3−q)β 2 (1−q) for q < 1, and Ω q = R N for 1 < q < 3. It is evident that Ω q is the support set for the q-Gaussian distribution with q-variance β 2 . Deﬁne the SF for gradient of average cost as\nWe ﬁrst state the following lemma which will be required to prove the result in Proposition 3.3.\nLemma 3.2. Let f : R N → R be a function deﬁned over a standard q-Gaussian distributed random variable X ∈ R N ,\n[G q (x)] q dx , K q,N being the normalizing constant for N-variate q-Gaussian.\nProposition 3.3. For a given q < 3, q = 1, as β → 0, SF for the gradient converges to a scaled version of the gradient,\nAs a consequence of the Proposition 3.3, for large M and small β, the form of gradient estimate suggested by (10) is\nUsing an approximation of (6), for large L, we can write the above equation as\nHowever, since Λ q > 0, Λ q need not be explicitly deter- mined as estimating [Λ q θ J (θ)] instead of θ J (θ) does not affect the gradient descent approach. As a special case, for q = 1, we have Λ q = 1 from deﬁnition. Hence, we obtain the same form as in (8).\nIn this section, we propose a two-timescale algorithm cor- responding to the estimate obtained in (12).\nThe q-Gaussian distributed parameters (η) have been gen- erated in the algorithm using the method proposed in [11]. Let {a(n)}, {b(n)} be two step-size sequences satisfying\nFor θ = (θ (1) , . . . , θ (N ) ) T ∈ R N , let Γ(θ) = Γ(θ (1) ), . . . , Γ(θ (N ) ) T represent the projection of θ onto the set C. {Z (i) (n), i = 1, . . . , N } n∈N are quantities used to estimate [Λ q θ J (θ)] via the recursions below.\n5: \t Generate i.i.d. standard q-Gaussian distributed random variables η (1) (n), . . . , η (N ) (n) and set η(n) = (η (1) (n), . . . , η (N ) (n)) T .\n7: \t Generate the simulation Y nL+m governed with pa- rameter (θ(n) + βη(n)).\nWe consider a two-node network of M/G/1 queues with feedback. The setting here is somewhat similar to that consid- ered in [3]. Nodes 1 and 2 are fed with independent Poisson external arrival processes with rates λ 1 = 0.2 and λ 2 = 0.1, respectively. After departing from Node-1, customers enter Node-2. Once the service at Node-2 is completed, a cus- tomer either leaves the system with probability p = 0.4 or joins Node-1. The service time processes of the two nodes, {S 1 n (θ 1 )} n 1 and {S 2 n (θ 2 )} n 1 , respectively, are deﬁned as\nwhere R 1 = 10 and R 2 = 20 are constants. Here, U 1 (n) and U 2 (n) are independent samples drawn from uniform distribution on (0,1). Service time of each node depends on the N i -dimensional tunable parameter vector θ i , whose individual components lie in a certain interval [(θ (j) i ) min , (θ (j) i ) max ], j = 1, . . . , N i , i = 1, 2. θ i (n) represents the n th update of parameter vector at Node-i, and ¯ θ i represents the target vector.\nThe cost function is chosen to be the sum of the two queue lengths at any instant. For the cost to be minimum, S i n (θ i ) should be minimum, and hence, we should have θ i (n) = ¯ θ i , i = 1, 2. We denote θ = (θ (1) 1 , .., θ (N 1 ) 1 , θ (1) 2 , .., θ (N 2 ) 2 ) ∈ R N , and ¯ θ = (¯ θ (1) 1 , .., ¯ θ (N 1 ) 1 , ¯ θ 2 , .., ¯ θ (N 2 ) 2 ) ∈ R N , where N =N 1 +N 2 . For the simulations, we use the following values of parameters: (1) N 1 = N 2 = 2,\nSimulations are performed by varying the parameters q and β. We compare the performance of our algorithm with the SF algorithm proposed in [3], which uses Gaussian smoothing. The Euclidian distance between θ(n) and ¯ θ is chosen as the performance measure as this gives the proximity of the updates to the global optimum. For each case, the results are averaged over 20 independent trials. Figure 1 shows that with same β, q-SF converges faster than SF algorithm for some q\u2019s. Table I presents a detailed comparison for different values of q and β.\nThe cases where q-SF outperforms SF are highlighted, and for each β, the best result is underlined. It can be observed that for smaller β, q-SF with q > 1 performs better than SF, but for larger β, better performance can be obtained with q < 1. So, as β increases, smaller q\u2019s prove to be better. As per observations, q = 0.9 performs better than Gaussian in 63% cases, and also gives the least distance in most of the cases (50%).\nThe results show that there are some values of q = 1 for which we can reach closer proximity of the global minimum with the proposed algorithm than the SF case. This can be contributed to the power-law tail of q-Gaussian which allows better control over the level of smoothing. There is an additional improvement provided by Λ q , which can be expressed as\nFor q > 1, the term inside bracket is always less than 1, which implies Λ q < 1, whereas Λ q > 1 for q < 1. Thus the gradient descent is faster for q < 1, which leads to faster convergence.\nWe also note that for high q, the algorithm does not converge for larger β. So we may claim that the region of stability of q-SF, given by β 0 (see Theorem 6.5), decreases as q increases.\nHere, we give a sketch of the proof of convergence of the proposed algorithm. We just state the important results. The proofs will be given in a longer version of the paper.\nLet F (l) = σ ˜ θ (i) (k), ˜ η (i) (k), Y k , k \t l, i = 1, . . . , N , l 1 denote the σ-ﬁelds generated by the above mentioned quantities, where ˜ θ (i) (k) = θ (i) (n) and ˜ η (i) (k) = η (i) (n) for i = 1, . . . N , nL k < (n + 1)L. Deﬁne {˜ b(n)} n 0 such that ˜ b(n) = b( n L ), where [x] is the integer part of x. Thus,\nWith the above notation, substituting p = nL + m we can rewrite Step 9 of our algorithm in terms of ˜ b(p), ˜ θ (i) (p) and ˜ η (i) (p). We deﬁne the sequences {M (i) (p)} p 1 , i = 1, . . . N ,\nLemma 6.1. The sequences {M (i) (p), F (p)} p 1 , i = 1, 2, . . . N are almost surely convergent martingale sequences.\nLemma 6.2. The sequence of updates {Z(p)} is uniformly bounded with probability 1.\nLemma 6.3. For a given q < 3, q = 1, with probability 1 Z(nL) − (3−q) 2 D q,β [J (θ(n))] → 0 as n → ∞.\nThe following corollary follows directly from Proposi- tion 3.3 and Lemma 6.3 by triangle inequality.\nCorollary 6.4. Given a particular q < 3, with probability 1, as n → ∞ and β → 0, Z(nL) − Λ q θ J (θ) → 0\nNow, ﬁnally considering the ODE corresponding to the slowest timescale recursion:\nwhere ˜ Γ(f (x)) = lim →0 Γ(x+ f (x))−x for any bounded, continuous function f : R N → R N . The stable points of (18) lie in the set S = θ ∈ C : ˜ Γ − Λ q θ J (θ(t)) = 0 . Given δ > 0, we deﬁne S δ = θ ∈ C : θ − θ 0 < δ, θ 0 ∈ S .\nTheorem 6.5. Under Assumptions II \u2013 IV, given q < 3, q = 1 and δ > 0, ∃β 0 > 0 such that for all β ∈ (0, β 0 ], the sequence {θ(n)} obtained using the q-SF algorithm converges to a point in S δ with probability 1 as n → ∞.\nThe q-Gaussian exhibits power-law behavior, which gives a better control over smoothing of functions as compared to normal distribution. We have extended the Gaussian smoothed functional gradient estimation approach to q-Gaussians, and developed an optimization algorithm based on this. We have also presented results illustrating that for some values of q, our algorithm performs better than the SF algorithm [3]."},"refs":[{"authors":[{"name":"E. Kiefer"},{"name":"J. Wolfowitz"}],"title":{"text":"Stochastic estimation of a maximum regression function"}},{"authors":[{"name":"V. Y. A. Katkovnik"},{"name":"Y. U. Kulchitsky"}],"title":{"text":"Convergence of a class of random search algorithms"}},{"authors":[{"name":"S. Bhatnagar"},{"name":"V. S. Borkar"}],"title":{"text":"Multiscale chaotic SPSA and smoothed functional algorithms for simulation optimization"}},{"authors":[{"name":"R. Y. Rubinstei"}],"title":{"text":"Simulation and Monte-Carlo Method, John Wiley, New York, 1981"}},{"authors":[{"name":"J. Havrda"},{"name":"F. Charv´at"}],"title":{"text":"Quantiﬁcation method of classiﬁcation processes: Concept of structural a-entropy"}},{"authors":[{"name":"C. Tsallis"}],"title":{"text":"Possible generalization of Boltzmann-Gibbs statistics"}},{"authors":[{"name":"A. Dukkipati"},{"name":"S. Bhatnagar"},{"name":"M. N. Murty"}],"title":{"text":"On measure-theoretic aspects of nonextensive entropy functionals and corresponding maxi- mum entropy prescriptions"}},{"authors":[{"name":"D. Prato"},{"name":"C. Tsallis"}],"title":{"text":"Nonextensive foundation of L´evy distributions"}},{"authors":[{"name":"C. Vignat"},{"name":"A. Plastino"}],"title":{"text":"Central limit theorem and deformed exponentials"}},{"authors":[{"name":"J. Costa"},{"name":"A. Hero"},{"name":"C. Vignat"}],"title":{"text":"On solutions to multivariate maximum α-entropy problems"}},{"authors":[{"name":"W. J. Thistleton"},{"name":"J. A. Marsh"},{"name":"K. Nelson"},{"name":"C. Tsallis"}],"title":{"text":"Generalized Box-Muller method for generating q-Gaussian random deviates"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569561513.pdf"},"links":[{"id":"1569566567","weight":3},{"id":"1569564843","weight":5},{"id":"1569566527","weight":4},{"id":"1569566485","weight":2},{"id":"1569565383","weight":6},{"id":"1569565883","weight":5},{"id":"1569564889","weight":3},{"id":"1569565223","weight":2},{"id":"1569566725","weight":5},{"id":"1569565663","weight":4},{"id":"1569565377","weight":7},{"id":"1569566385","weight":6},{"id":"1569564635","weight":3},{"id":"1569565867","weight":3},{"id":"1569566799","weight":4},{"id":"1569559665","weight":2},{"id":"1569561021","weight":7},{"id":"1569564669","weight":7},{"id":"1569565691","weight":3},{"id":"1569566815","weight":4},{"id":"1569566875","weight":5},{"id":"1569559617","weight":2},{"id":"1569566981","weight":4},{"id":"1569566433","weight":2},{"id":"1569566321","weight":13},{"id":"1569566605","weight":3},{"id":"1569565489","weight":3},{"id":"1569566683","weight":5},{"id":"1569566855","weight":3},{"id":"1569560629","weight":7},{"id":"1569566869","weight":5},{"id":"1569565097","weight":9},{"id":"1569566227","weight":10},{"id":"1569566091","weight":5},{"id":"1569559259","weight":3},{"id":"1569566697","weight":4},{"id":"1569566597","weight":5},{"id":"1569565551","weight":5},{"id":"1569565711","weight":3},{"id":"1569566761","weight":5},{"id":"1569566943","weight":5},{"id":"1569565091","weight":10},{"id":"1569566591","weight":6},{"id":"1569556029","weight":4},{"id":"1569566571","weight":6},{"id":"1569552245","weight":8},{"id":"1569565607","weight":7},{"id":"1569565495","weight":3},{"id":"1569559967","weight":6},{"id":"1569567045","weight":5},{"id":"1569565227","weight":6},{"id":"1569564481","weight":9},{"id":"1569560833","weight":10},{"id":"1569566415","weight":4},{"id":"1569564805","weight":7},{"id":"1569567005","weight":7},{"id":"1569566469","weight":3},{"id":"1569566081","weight":6},{"id":"1569565613","weight":2},{"id":"1569565355","weight":6},{"id":"1569564469","weight":4},{"id":"1569565931","weight":4},{"id":"1569566373","weight":7},{"id":"1569566647","weight":3},{"id":"1569551535","weight":15},{"id":"1569566765","weight":3},{"id":"1569564897","weight":2},{"id":"1569565775","weight":4},{"id":"1569565547","weight":7},{"id":"1569566871","weight":4},{"id":"1569566653","weight":2},{"id":"1569565461","weight":5},{"id":"1569564731","weight":8},{"id":"1569565171","weight":2},{"id":"1569566207","weight":2},{"id":"1569564227","weight":3},{"id":"1569558325","weight":5},{"id":"1569565837","weight":4},{"id":"1569566671","weight":6},{"id":"1569566303","weight":4},{"id":"1569564233","weight":4},{"id":"1569566459","weight":6},{"id":"1569567535","weight":2},{"id":"1569563411","weight":8},{"id":"1569560427","weight":2},{"id":"1569564401","weight":5},{"id":"1569564849","weight":3},{"id":"1569559541","weight":5},{"id":"1569565317","weight":2},{"id":"1569566363","weight":3},{"id":"1569565123","weight":6},{"id":"1569566941","weight":2},{"id":"1569566033","weight":2},{"id":"1569566739","weight":3},{"id":"1569555811","weight":6},{"id":"1569558459","weight":2},{"id":"1569565609","weight":2},{"id":"1569565291","weight":5},{"id":"1569564203","weight":7},{"id":"1569566821","weight":3},{"id":"1569556713","weight":3},{"id":"1569562685","weight":2},{"id":"1569566751","weight":5},{"id":"1569566467","weight":3},{"id":"1569565771","weight":7},{"id":"1569566157","weight":2},{"id":"1569560613","weight":6},{"id":"1569566903","weight":3},{"id":"1569566999","weight":6},{"id":"1569565859","weight":2},{"id":"1569565809","weight":5},{"id":"1569566843","weight":4},{"id":"1569566579","weight":5},{"id":"1569558483","weight":6},{"id":"1569566563","weight":9},{"id":"1569566089","weight":2},{"id":"1569559221","weight":3},{"id":"1569556091","weight":3},{"id":"1569565347","weight":5},{"id":"1569566925","weight":3},{"id":"1569564387","weight":4},{"id":"1569565455","weight":3},{"id":"1569566497","weight":9},{"id":"1569566795","weight":2},{"id":"1569566963","weight":3},{"id":"1569561679","weight":2},{"id":"1569566709","weight":3},{"id":"1569564989","weight":2},{"id":"1569560721","weight":5},{"id":"1569566015","weight":6},{"id":"1569566523","weight":2},{"id":"1569565897","weight":6},{"id":"1569551763","weight":2},{"id":"1569565953","weight":3},{"id":"1569566895","weight":3},{"id":"1569566889","weight":2},{"id":"1569565709","weight":3},{"id":"1569566749","weight":2},{"id":"1569566269","weight":6},{"id":"1569564189","weight":7},{"id":"1569564195","weight":2},{"id":"1569566985","weight":6},{"id":"1569564613","weight":2},{"id":"1569567009","weight":5},{"id":"1569566865","weight":2},{"id":"1569565321","weight":4},{"id":"1569558785","weight":2},{"id":"1569564647","weight":3},{"id":"1569566095","weight":3},{"id":"1569566193","weight":2},{"id":"1569564271","weight":2},{"id":"1569564337","weight":4},{"id":"1569565907","weight":5},{"id":"1569566343","weight":2},{"id":"1569564311","weight":3},{"id":"1569565803","weight":4},{"id":"1569565785","weight":4},{"id":"1569566239","weight":3},{"id":"1569566167","weight":2},{"id":"1569566679","weight":4},{"id":"1569565989","weight":7},{"id":"1569566575","weight":3},{"id":"1569563981","weight":5},{"id":"1569561085","weight":7},{"id":"1569566419","weight":3},{"id":"1569566617","weight":3},{"id":"1569559565","weight":9},{"id":"1569566905","weight":6},{"id":"1569566753","weight":2},{"id":"1569566311","weight":6},{"id":"1569563307","weight":12},{"id":"1569566063","weight":4},{"id":"1569558681","weight":8},{"id":"1569555999","weight":3},{"id":"1569566759","weight":5},{"id":"1569565589","weight":4},{"id":"1569559195","weight":2},{"id":"1569566149","weight":10},{"id":"1569559995","weight":2},{"id":"1569566657","weight":7},{"id":"1569565199","weight":6},{"id":"1569565365","weight":3},{"id":"1569566643","weight":4},{"id":"1569566511","weight":6},{"id":"1569566719","weight":4},{"id":"1569566991","weight":2},{"id":"1569566975","weight":3},{"id":"1569566369","weight":2},{"id":"1569566531","weight":4},{"id":"1569567665","weight":4},{"id":"1569561143","weight":3},{"id":"1569566581","weight":6},{"id":"1569565833","weight":2},{"id":"1569566489","weight":13},{"id":"1569564611","weight":2},{"id":"1569565535","weight":12},{"id":"1569562867","weight":12},{"id":"1569566395","weight":2},{"id":"1569565667","weight":3},{"id":"1569566845","weight":4},{"id":"1569566325","weight":7},{"id":"1569566423","weight":2},{"id":"1569564795","weight":5},{"id":"1569567015","weight":11},{"id":"1569559805","weight":4},{"id":"1569566437","weight":5},{"id":"1569566851","weight":4},{"id":"1569565735","weight":5},{"id":"1569553909","weight":3},{"id":"1569559111","weight":4},{"id":"1569566687","weight":3},{"id":"1569564881","weight":3},{"id":"1569566939","weight":7},{"id":"1569553537","weight":6},{"id":"1569565427","weight":15},{"id":"1569566403","weight":2},{"id":"1569565839","weight":2},{"id":"1569565915","weight":2},{"id":"1569552251","weight":7},{"id":"1569566139","weight":2},{"id":"1569553519","weight":3},{"id":"1569567051","weight":2},{"id":"1569566885","weight":2},{"id":"1569564441","weight":4},{"id":"1569566231","weight":3},{"id":"1569564209","weight":2},{"id":"1569566513","weight":3},{"id":"1569566425","weight":5},{"id":"1569554881","weight":2},{"id":"1569554971","weight":9},{"id":"1569565501","weight":6},{"id":"1569566899","weight":6},{"id":"1569566445","weight":3},{"id":"1569566209","weight":3},{"id":"1569566649","weight":2},{"id":"1569566791","weight":6},{"id":"1569565559","weight":5},{"id":"1569566371","weight":2},{"id":"1569565655","weight":6},{"id":"1569566909","weight":2},{"id":"1569566127","weight":3},{"id":"1569565151","weight":2},{"id":"1569558985","weight":5},{"id":"1569563763","weight":12},{"id":"1569566473","weight":9},{"id":"1569564857","weight":5},{"id":"1569564333","weight":4},{"id":"1569566913","weight":11},{"id":"1569566809","weight":4},{"id":"1569566629","weight":4},{"id":"1569566257","weight":6},{"id":"1569565033","weight":3},{"id":"1569566447","weight":4},{"id":"1569566357","weight":3},{"id":"1569565817","weight":6},{"id":"1569565847","weight":7},{"id":"1569564353","weight":5},{"id":"1569563897","weight":3},{"id":"1569557083","weight":2},{"id":"1569565887","weight":2},{"id":"1569565929","weight":5},{"id":"1569566141","weight":4},{"id":"1569566721","weight":2},{"id":"1569565055","weight":2},{"id":"1569564677","weight":2},{"id":"1569563231","weight":5},{"id":"1569565633","weight":6},{"id":"1569566661","weight":5},{"id":"1569565279","weight":3},{"id":"1569555879","weight":6},{"id":"1569566115","weight":4},{"id":"1569565219","weight":3},{"id":"1569558509","weight":4},{"id":"1569565595","weight":4},{"id":"1569565185","weight":5},{"id":"1569566773","weight":7},{"id":"1569564985","weight":3},{"id":"1569566223","weight":3},{"id":"1569558401","weight":9},{"id":"1569566553","weight":4},{"id":"1569564969","weight":4},{"id":"1569565029","weight":7},{"id":"1569565357","weight":3},{"id":"1569561245","weight":3},{"id":"1569566505","weight":5},{"id":"1569565393","weight":3},{"id":"1569565933","weight":5},{"id":"1569562207","weight":6},{"id":"1569566191","weight":2},{"id":"1569567033","weight":10},{"id":"1569565527","weight":4},{"id":"1569566853","weight":3},{"id":"1569566603","weight":4},{"id":"1569567029","weight":3},{"id":"1569565363","weight":4},{"id":"1569566159","weight":3},{"id":"1569566695","weight":3},{"id":"1569566051","weight":5},{"id":"1569561379","weight":2},{"id":"1569565909","weight":2},{"id":"1569561123","weight":3},{"id":"1569565467","weight":3},{"id":"1569566655","weight":4},{"id":"1569566673","weight":3},{"id":"1569567235","weight":6},{"id":"1569565441","weight":2},{"id":"1569565739","weight":2},{"id":"1569565311","weight":2},{"id":"1569566233","weight":13},{"id":"1569566667","weight":2},{"id":"1569566297","weight":4},{"id":"1569566893","weight":6},{"id":"1569566317","weight":5},{"id":"1569564097","weight":2},{"id":"1569560997","weight":5},{"id":"1569563845","weight":2},{"id":"1569566407","weight":3},{"id":"1569560349","weight":2},{"id":"1569566501","weight":3},{"id":"1569565741","weight":5},{"id":"1569566275","weight":7},{"id":"1569566481","weight":5},{"id":"1569565545","weight":5},{"id":"1569566857","weight":6},{"id":"1569566387","weight":2},{"id":"1569566245","weight":9},{"id":"1569560503","weight":3},{"id":"1569565463","weight":5},{"id":"1569566219","weight":3},{"id":"1569565439","weight":4},{"id":"1569566229","weight":4},{"id":"1569566949","weight":2},{"id":"1569566133","weight":2},{"id":"1569562551","weight":6},{"id":"1569566901","weight":3},{"id":"1569551347","weight":3},{"id":"1569565415","weight":5},{"id":"1569555367","weight":2},{"id":"1569561623","weight":5},{"id":"1569566383","weight":2},{"id":"1569564485","weight":9},{"id":"1569565155","weight":2},{"id":"1569566631","weight":8},{"id":"1569565571","weight":4},{"id":"1569565885","weight":4},{"id":"1569566177","weight":3},{"id":"1569565493","weight":5},{"id":"1569557633","weight":5},{"id":"1569566805","weight":3},{"id":"1569559199","weight":3},{"id":"1569566293","weight":9},{"id":"1569565665","weight":4},{"id":"1569566831","weight":5},{"id":"1569565549","weight":5},{"id":"1569565523","weight":6},{"id":"1569565611","weight":5},{"id":"1569557715","weight":3},{"id":"1569564175","weight":3},{"id":"1569566983","weight":11},{"id":"1569566779","weight":2},{"id":"1569566097","weight":4},{"id":"1569566479","weight":5},{"id":"1569556361","weight":3},{"id":"1569565397","weight":2},{"id":"1569566873","weight":5},{"id":"1569565765","weight":4},{"id":"1569565925","weight":8},{"id":"1569565435","weight":4},{"id":"1569557275","weight":3},{"id":"1569566261","weight":7},{"id":"1569565215","weight":2},{"id":"1569565093","weight":2},{"id":"1569565385","weight":2},{"id":"1569565575","weight":3},{"id":"1569565919","weight":6},{"id":"1569565181","weight":3},{"id":"1569566711","weight":4},{"id":"1569565241","weight":3},{"id":"1569566927","weight":4},{"id":"1569565661","weight":3},{"id":"1569565865","weight":2},{"id":"1569566887","weight":5},{"id":"1569565273","weight":5},{"id":"1569566267","weight":4},{"id":"1569552037","weight":5},{"id":"1569564919","weight":4},{"id":"1569566737","weight":7},{"id":"1569566429","weight":4},{"id":"1569561221","weight":3},{"id":"1569566917","weight":8},{"id":"1569566035","weight":11},{"id":"1569565353","weight":6},{"id":"1569564683","weight":5},{"id":"1569564305","weight":6},{"id":"1569564283","weight":5},{"id":"1569566691","weight":3},{"id":"1569565421","weight":7},{"id":"1569566547","weight":3},{"id":"1569566651","weight":4},{"id":"1569565177","weight":5},{"id":"1569566823","weight":3},{"id":"1569566595","weight":4},{"id":"1569566677","weight":2},{"id":"1569565349","weight":4},{"id":"1569552025","weight":3},{"id":"1569566137","weight":3},{"id":"1569565013","weight":2},{"id":"1569565829","weight":3},{"id":"1569566283","weight":5},{"id":"1569566529","weight":4},{"id":"1569565375","weight":4},{"id":"1569566715","weight":4},{"id":"1569565237","weight":5},{"id":"1569566639","weight":5},{"id":"1569566819","weight":5},{"id":"1569565041","weight":6},{"id":"1569564703","weight":6},{"id":"1569565541","weight":2},{"id":"1569566813","weight":5},{"id":"1569565293","weight":2},{"id":"1569566771","weight":3},{"id":"1569564201","weight":5},{"id":"1569562277","weight":5},{"id":"1569566641","weight":7},{"id":"1569565425","weight":2},{"id":"1569564247","weight":2},{"id":"1569564437","weight":5},{"id":"1569566533","weight":3},{"id":"1569563975","weight":3},{"id":"1569551905","weight":2},{"id":"1569564861","weight":5},{"id":"1569565457","weight":4},{"id":"1569564787","weight":5},{"id":"1569566487","weight":5},{"id":"1569565529","weight":11},{"id":"1569556759","weight":4},{"id":"1569566619","weight":5},{"id":"1569565271","weight":2},{"id":"1569561185","weight":5},{"id":"1569566075","weight":2},{"id":"1569566397","weight":3},{"id":"1569566301","weight":4},{"id":"1569558779","weight":8},{"id":"1569565669","weight":2},{"id":"1569565233","weight":2},{"id":"1569563721","weight":2},{"id":"1569566001","weight":5},{"id":"1569565593","weight":6},{"id":"1569560235","weight":2},{"id":"1569566817","weight":2},{"id":"1569566389","weight":2},{"id":"1569566435","weight":3},{"id":"1569567483","weight":3},{"id":"1569564923","weight":7},{"id":"1569565367","weight":4},{"id":"1569566299","weight":5},{"id":"1569564281","weight":3},{"id":"1569564769","weight":4},{"id":"1569565769","weight":3},{"id":"1569566171","weight":8},{"id":"1569566601","weight":4},{"id":"1569565805","weight":5},{"id":"1569561713","weight":2},{"id":"1569566933","weight":3},{"id":"1569563919","weight":5},{"id":"1569566577","weight":3},{"id":"1569557851","weight":4},{"id":"1569565389","weight":12},{"id":"1569559919","weight":10},{"id":"1569565861","weight":3},{"id":"1569566147","weight":4},{"id":"1569565537","weight":4},{"id":"1569566057","weight":4},{"id":"1569562367","weight":4},{"id":"1569560785","weight":6},{"id":"1569565561","weight":2},{"id":"1569565631","weight":2},{"id":"1569560213","weight":3},{"id":"1569555891","weight":2},{"id":"1569565997","weight":5},{"id":"1569563425","weight":3},{"id":"1569565035","weight":9},{"id":"1569559597","weight":5},{"id":"1569564961","weight":8},{"id":"1569559251","weight":2},{"id":"1569565089","weight":2},{"id":"1569567013","weight":4},{"id":"1569566583","weight":3},{"id":"1569561861","weight":3},{"id":"1569565337","weight":3},{"id":"1569564253","weight":2},{"id":"1569565737","weight":4},{"id":"1569560459","weight":3},{"id":"1569564463","weight":9},{"id":"1569565853","weight":4},{"id":"1569550425","weight":10},{"id":"1569566273","weight":4},{"id":"1569564123","weight":11},{"id":"1569566341","weight":3},{"id":"1569565889","weight":3},{"id":"1569566635","weight":3},{"id":"1569566611","weight":8},{"id":"1569551539","weight":5},{"id":"1569564505","weight":3},{"id":"1569565165","weight":2},{"id":"1569565565","weight":9},{"id":"1569565635","weight":7},{"id":"1569561397","weight":2},{"id":"1569565731","weight":2},{"id":"1569566797","weight":12},{"id":"1569566125","weight":6},{"id":"1569566413","weight":5},{"id":"1569565707","weight":9},{"id":"1569565113","weight":3},{"id":"1569566375","weight":6},{"id":"1569565143","weight":2},{"id":"1569564257","weight":6},{"id":"1569565583","weight":2},{"id":"1569566555","weight":3},{"id":"1569565373","weight":5},{"id":"1569564141","weight":3},{"id":"1569566973","weight":6},{"id":"1569561579","weight":7},{"id":"1569566449","weight":4},{"id":"1569566987","weight":2},{"id":"1569565031","weight":8},{"id":"1569564755","weight":2},{"id":"1569551541","weight":6},{"id":"1569565619","weight":3},{"id":"1569566839","weight":11},{"id":"1569551751","weight":2},{"id":"1569558697","weight":2},{"id":"1569565139","weight":6},{"id":"1569565895","weight":3},{"id":"1569566663","weight":2},{"id":"1569564419","weight":2},{"id":"1569565579","weight":3},{"id":"1569566067","weight":5},{"id":"1569566825","weight":9},{"id":"1569566241","weight":4},{"id":"1569564807","weight":4},{"id":"1569563007","weight":3},{"id":"1569566113","weight":4},{"id":"1569566443","weight":4},{"id":"1569566727","weight":12},{"id":"1569565315","weight":3},{"id":"1569565515","weight":3},{"id":"1569566417","weight":3},{"id":"1569560581","weight":6},{"id":"1569559233","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S6.T8.3","endtime":"12:30","authors":"Debarghya Ghoshdastidar, Ambedkar Dukkipati, Shalabh Bhatnagar","date":"1341317400000","papertitle":"q-Gaussian based Smoothed Functional Algorithms for Stochastic Optimization","starttime":"12:10","session":"S6.T8: Probability and Estimation","room":"Stratton (491)","paperid":"1569561513"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"3","spectral43":"2","spectral28":"5","spectral32":"22","spectral14":"13","spectral20":"14","spectral9":"6","spectral25":"19","spectral42":"32","spectral3":"1","spectral47":"25","spectral17":"3","louvain":"632","spectral36":"7","spectral39":"37","spectral10":"4","spectral15":"5","spectral33":"10","spectral5":"4","spectral21":"10","spectral44":"9","spectral26":"22","spectral40":"21","spectral8":"1","spectral11":"10","spectral4":"2","spectral37":"30","spectral48":"5","spectral22":"4","spectral23":"5","spectral12":"9","spectral50":"41","spectral19":"8","spectral34":"22","spectral45":"4","spectral7":"5","spectral49":"27","spectral38":"30","spectral24":"22","spectral13":"1","spectral31":"3","spectral29":"14","spectral35":"33","spectral30":"19","spectral41":"8","spectral27":"25","spectral18":"5","spectral46":"0","spectral2":"0","spectral16":"5"}}

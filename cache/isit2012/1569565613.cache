{"id":"1569565613","paper":{"title":{"text":"Codes Can Reduce Queueing Delay in Data Centers"},"authors":[{"name":"Longbo Huang"},{"name":"Sameer Pawar"},{"name":"Hao Zhang"},{"name":"Kannan Ramchandran"}],"abstr":{"text":"Abstract\u2014In this paper, we quantify how much codes can reduce the data retrieval latency in storage systems. By combining a simple linear code with a novel request scheduling algorithm, which we call Blocking-one Scheduling (BoS), we show analyti- cally that it is possible to use codes to reduce data retrieval delay by up to 17% over currently popular replication-based strategies. Although in this work we focus on a simpliﬁed setting where the storage system stores a single content, the methodology developed can be applied to more general settings with multiple contents. The results also offer insightful guidance to the design of storage systems in data centers and content distribution networks."},"body":{"text":"In today\u2019s data centers, one of the most demanding tasks (in terms of latency) is \u201cdisk-read,\u201d e.g., fetching the data for performing analytics such as MapReduce, or to serve the data to the end consumer. In many cases, this task is greatly complicated by the highly non-uniform data popularity, where the most popular data objects can be accessed ten times more frequently than the bottom third [1]. This skewed demand leads to high contention for read tasks of the most popular data. To meet the demand and reduce data retrieval latency, current systems often introduce data redundancy by replicating many copies of each content, e.g., Hadoop replicates each content three or more times, to make the popular data more available and relieve the hot spots of read contentions, thereby reducing the average request latency.\nThis motivates us to investigate the fundamental role of redundancy in improving the system latency . In particular, we compare two systems, one using erasure codes to introduce redundancy and the other using simple replication. Heuris- tically, codes offer more ﬂexibility when retrieving the data from the servers, thus may improve content retrieval latency. In this paper, we use a non-asymptotic analytical approach to quantify this intuition.\nTo make the problem more concrete, consider an abstracted example of retrieving a ﬁle in a data center shown in Fig. 1. The storage system consists of 4 servers, called storage units (SU), each capable of storing 1 packet of the desired ﬁle that consists of 2 packets A and B. We consider two possible storage strategies 1) replication: each packet is replicated two times; 2) coding: ﬁle is encoded using a (4, 2) Maximum- Distance-Separable (MDS) code. It is easy to see that the redundancy factor is 2 in both cases. There is a common dispatcher that queues and schedules the incoming requests. The request process is Poisson and each ﬁle request contains two packet-requests for two packet components. The service time of the storage unit is exponentially distributed.\nQuantifying the exact request delay in a coded system is a challenging task. The main difﬁculty is that the optimal scheduling algorithm needs to remember which SUs served earlier requests to ensure that the two requests of a particular content are always served by distinct SUs. This makes the analysis extremely difﬁcult. Moreover, since data centers can- not afford a redundancy factor of more than a few tens at most, the asymptotic analysis based approach advocated in [3] [10] are not relevant for this setup.\nTo tackle this challenge and to demonstrate the role of codes in delay reduction, we develop a novel scheduling algorithm called Blocking-one Scheduling (BoS) to derive a close upper bound of the delay of the coded system. The idea of BoS is to block subsequent content retrieval requests until the head- of-line request is processed. This helps remove the dependency in the scheduling actions, and allows a clean analysis of the delay performance of the system using codes. Under BoS, we analytically show that the system that uses codes reduces the average request delay by 7% − 17% compared to the system that uses replication, depending upon the replication factor of the ﬁles. The intuition behind this is that in the replication system, the requests for packet A or B can be satisﬁed by speciﬁc designated servers, while in the coded system any two servers are sufﬁcient to serve the ﬁle. Therefore, codes offer more ﬂexibility in data retrieval due to multiplexing gain of available servers.\nRelated Work: The authors in [5] and [12] study the throughput-delay gains of network coding in a single hop wireless downlink with unreliable channels. The authors of [9] consider the network coding gains in throughput when packets have hard delay deadlines. The work of [4] studies the gains in delay and throughput when using network coding\nover a linear network with unreliable links. In [11], the authors use network coding to prevent underﬂow in a multi-media streaming application. While most of the earlier works focus on use of codes to achieve the best effort throughput and or delay over unreliable wireless channels, non-asymptotic and theoretical analysis of queueing delay in coded systems remains largely unaddressed, to the best of our knowledge.\nThe paper is organized as follows. In Section II, we state our system model. In Section III, we present the schemes that will be used in the uncoded and coded systems, and present the Blocking-one Scheduling (BoS) algorithm. We analyze the performance of BoS in Section IV. We then conclude the paper in Section V.\nWe consider an information storage system that consists of n homogeneous storage servers, called storage units (SU). Each SU has a ﬁxed storage capacity, and is capable of serving each incoming request in a time that is exponentially distributed with mean µ = 1. The system stores and serves a set of contents, denoted by C = {1, 2, ..., C}. Each content is striped into k packets of size one. A total of k SU is needed to store a single content. In practice, for reliability and availability purposes, a content is stored redundantly on multiple servers.\nAssume the content retrieval requests for each content c form a Poisson arrival process with rate λ c . Since a content is striped into k packets that are stored on distinct SUs, every request needs to be served at k servers with distinct packets in order to fully retrieve the desired content. We model this behavior by duplicating each arrived content-request into k packet-requests and by making sure that no two packet- requests of a content-request are processed by servers with identical packets.\nThere is a central dispatcher that delegates the incoming requests to the SUs. Upon arrival, the requests are ﬁrst queued at the central dispatcher, and then sent to the servers once they become available. 1 In such a scenario, we are interested in quantifying the reduction in average content delay between a coding-based system and a replication-based system.\nTo further illustrate the problem, consider a simple example depicted in the Fig. 2. The system contains 4 storage units and 1 content. The content is striped into 2 packets A and B, and is stored with a redundancy factor of 2. Each arriving content request brings into the system two packet-requests. In Fig. 2(a) we show a system that uses replication to introduce redundancy, where packets A and B are replicated twice. In contrast, Fig. 2(b) shows a system that uses a MDS code of rate 0.5 to introduce the desired redundancy. The question we aim to answer in this paper is by how much can we reduce the average delay of a coding-based system over a replication- based one?\nIn general, quantifying the delay performance of a system with multiple contents, multiple servers and multiple coded\npackets that are replicated multiple times can be quite chal- lenging. The main difﬁculty lies in the fact that the SUs now store coded data, thus the scheduling algorithm has to ensure that the requests for the same content are not served by a same SU. This means that the scheduling algorithm may need to remember at which SUs all the earlier requests are served. With such a long memory in scheduling, even deﬁning an appropriate system state is very hard, let alone analyzing it.\nTo make the analysis tractable, we focus on a special case with a single content, 2 packets, and n servers. We provide a close theoretical upper bound on the average content retrieval delay performance of the coded system, which we show still performs better than that of a replication-based system with identical amount of SU resources. We believe that the methodology and algorithms developed in this paper will provide useful insights to more general cases, which is part of our ongoing work.\nIn this section, we present the storage and scheduling schemes for both uncoded and coded systems. We assume that both systems have identical resources: each system hosts a single content that is divided into k = 2 packets, and has n = 2r, r ≥ 2 SUs, each capable of storing 1 packet. Each server can serve requests with rate of µ = 1. Here r is a redundancy factor in the system for both content availability as well as reliability of the content, and λ is the arrival rate of the content requests.\nIn this case, due to inherent symmetry of arrivals of sub- requests for packets A, B, r SUs store packet A and the remaining r store packet B. Then, whenever there is an idle SU that stores packet A, the packet A request from the head- of-line request is assigned to this server. The same happens for packet B requests.\nUnder this setting, the uncoded system can be modeled as two M/M/r queueing systems (See Fig. 2 (a)). 2 Now denote by π i the steady-state probability that there are i packet\nrequests in an M/M/r system, and denote ρ \t λ rµ . We note that {π 0 , π 1 , . . . , } can be computed as follows [2].\nr! π 0 , ∀ m ≥ 1. \t (3) The average packet request delay d uncoded packet can be computed by:\nλ \t . \t (4) Although the uncoded system admits an easy analysis of the average packet request delay, we see that ﬁnding the average content request delay can be quite challenging. However, as we will see in the coded system, the average content delay can be easily computed under our algorithm.\nWe now specify our coding and scheduling schemes for the coded system. We have n = 2r SUs and k = 2 packets A, B. We adopt a linear (n, 2) MDS code (any family of MDS code will do) to generate n encoded packets that are stored at each of the SUs. Due to the MDS nature of the code, any content request that is served at any two distinct SUs will be able to retrieve the full content , e.g., see Fig. 2(b). Under such a coding scheme, in order to minimize average packet request delay, the optimal scheduling strategy will be to assign a request to an SU whenever it is idle and can serve a packet- request in the queue. Although the suggested scheme seems simple, it has inherent memory/dependency in scheduling the requests due to the use of codes. For example, if the ﬁrst packet request of the i th content request, denoted by R i1 , is served at SU j, then the scheduler has to remember not to assign the other packet request R i2 to SU j even if it becomes idle. This dependency builds large memory into the system, which makes it very challenging to analyze the average delay performance of the requests in the coded system.\nIn order to resolve this difﬁculty, we propose a novel scheduling scheme called Blocking-one Scheduling (BoS), for the coded system. The main idea of BoS is to break the memory in the scheduler, i.e., the scheduler only has to remember which SU serves the HOL packet-requests, by blocking the requests beyond the HOL request until both packet requests of the HOL request are assigned to servers. 3 The BoS scheduler also corresponds to ﬁrst-come-ﬁrst-serve (FCFS). As we will see, the BoS algorithm not only greatly simpliﬁes the analysis of the packet request delay, but also allows us to directly calculate the average content request delay.\nNote that under BoS, it can happen that there exists an idle SU but no assignment is made even when the queue is non- empty. For example, when the free SU has served the packet\n1) If S Idle = φ, assign the packet requests from the head- of-line request as follows:\n\u2022 If packet request 1 has not yet been assigned, assign it to an idle server.\n\u2022 Else assign packet request 2 to an idle server that did not serve packet request 1.\n2) If an idle server is assigned a packet request, change its state from idle to busy. Remove the server from S Idle and remove the packet request from the queue.\n3) Repeat step 1) until no further assignment can be made.\nrequest 1 of the head-of-line content request and no other SU is idle. In this case, the requests beyond the head-of-line request are \u201cblocked.\u201d Due to this blocking effect, there will be a throughput loss due to the lost scheduling opportunity. Hence, the delay performance of BoS serves as an upper bound of the optimal scheduling scheme in the coded system. Interestingly, we show that such an opportunity loss under BoS decreases as O( 1 r 2 ). For instance, for r = 2, the BoS achieves 96% of the maximum system throughput. Thus, BoS\u2019s performance approaches the optimal scheme as r increases, providing us with a way to analyze the exact delay performance of a coded system.\nWe now analyze the BoS algorithm by ﬁnding the steady- state distribution of the number of packet requests in the coded system. The approach works as follows. We ﬁrst derive the continuous-time Markov chain that captures the system evolution. Then, we analyze the Markov chain by carefully choosing a set of global balance equations that allow us to compute the steady-state distribution.\nIn this section, we present the Markov chain that models the system evolution. Towards that end, we ﬁrst take a closer look at the state evolution of the example system in the Fig.2 (b) with 4 SUs.\nConsider a system in state as shown in Fig. 4 (a). In this case, the 4 SUs are serving packet requests R 11 , R 12 , R 21 , and R 31 . There are three more packet requests R 32 , R 41 and R 42 in the queue. Thus, the total number of packet requests in the system is 7. We denote this state of the system by the total number of packet requests in the system i.e., state is 7.\nNow, if SU 2 completes its service, the packet request R 32 can be assigned to SU 2. This results in a state (6, p) as shown in the Fig. 4 (b), which we refer to as a \u201cperfect-state.\u201d This state is called \u201cperfect\u201d because once in this state, past evolution is irrelevant in determining the future scheduling events and the total service rate is maximum, i.e., 4µ. However, if starting from state 7 but SU 4 completes the service ﬁrst, then we cannot assign request R 32 to SU 4, since R 31 was served by SU 4. In this case BoS will block the requests R 41 , R 42 until R 32 gets assigned to some other SU. This results in a not so perfect state hence we denote it by (6, g), i.e., good state with 6 packet requests in the system (see Fig.4 (c)). Note that in this good state the aggregate service rate is only 3µ.\nAlthough BoS performs sub-optimally as compared to a system with codes that maintains an inﬁnite memory and assigns R 41 to SU 4 when it becomes idle, it permits analytical treatment of average request delay. It can be veriﬁed that this blocking situation appears only when the system has more than 2r packet requests, and the number of packet requests is even. Thus, to characterize the system state, we introduce the sufﬁx \u201cp\u201d and \u201cg,\u201d which stand for \u201cperfect\u201d and \u201cgood\u201d states. When the number of packet requests in the system is less than 2r or when it is odd, we simply use the total number of packet requests in the system to denote the state of the system.\nNow that the system states have been deﬁned, the Markov chain in Fig. 3 explains the evolution of the system under BoS. The chain can be understood as follows:\n\u2022 With rate λ, there is a request arrival event, and two new packet requests are added to the system. If the system was in a good state before the arrival, it remains in the good state after the arrival and likewise for a perfect state.\n\u2022 For any state < 2r, transition rate for service completion is equal to µ times the state.\n\u2022 For any odd state > 2r, there are two outgoing transitions for service completion:\n1) With rate (2r − 1)µ, there is a transition to an even perfect state. This corresponds to the event that an SU\nnot serving the HOL request\u2019s 1st packet request just completes the service (see Fig. 4 (b)).\n2) With rate µ, there is a transition to an even good state (This corresponds to Fig. 4 (c)).\n\u2022 From an even-perfect state ≥ 2r, there is a service completion transition to an odd state with rate 2rµ.\nHere we present the performance results of BoS. To state the theorem, we ﬁrst deﬁne a few notations:\nNow denote by π 0 , ..., π 2r−1 , π p 2r , π g 2r , π 2r+1 , ... the stationary distribution of the Markov chain in Fig. 3, where superscripts \u201cp\u201d and \u201cg\u201d stand for perfect and good states. We have the following theorem:\n(a) The maximum rate the coded system can support is: 0 ≤ λ < rµ 1 − \t 1\n(b) If the system is stable, i.e., (5) is satisﬁed, the steady state probabilities can be computed by the following iterative process:\nπ g 2r and π g 2r+2m can be similarly computed by replacing γ p , β p with γ g , β g . 3\nProof: Omitted due to space limit. Please see [8] for the proofs.\nNote that a system with 2r servers should support an arrival rate of rµ. However, we see from equation (5) of Theorem 1 that, under BoS, there is a loss in throughput of O( 1 r 2 ). This throughput loss is due to blocking and quickly goes to zero as r increases. Thus, BoS indeed ensures high throughput of the coded system even for moderate values of r, and serves as a good approximation of the optimal scheduling scheme for the coded system. Part (b) of Theorem 1 then provides an efﬁcient way for analyzing the system. We emphasize that our results are not asymptotic and can be applied to systems of any sizes.\nUsing the approach in Theorem 1, we can compute the average packet request delay in the system using equation (4). We then compute the delay gain of coding by\nFig. 5 shows the delay gain for different values of r. We see that the gain is signiﬁcant even for small r, e.g., gain is 13% for r = 4, and can be up to 17% when r = 10. The reason why the gain decreases as λ approaches rµ is due to the throughput loss of BoS, which leads to a faster increase of delay when λ → rµ. Finally, we emphasize that Fig. 5 is obtained analytically, computed using results in Theorem 1.\nNotice that the above analysis allows us to derive the average packet request delay. In practice, we care more about the average content request delay, which is deﬁned as the average time it takes for both packet requests of a request to get served. The following lemma shows that under BoS, the average request delay is roughly equal to the packet delay. This is a very desired feature not possessed by the uncoded system.\nLemma 1. Let d coded content and d coded packet be the average content request delay and average packet request delay under BoS, then:\nWe see from the lemma that as the number of servers 2r gets large, the difference between the packet delay and the request delay roughly equals 1 2µ . This is exactly the difference between the average of two request service times and the maximum of them. Hence, it will appear under any scheduling policies regardless of using coding or not. Therefore, Lemma 1 shows that BoS ensures that the average packet latency is almost equal to the average request latency. This is a very important feature of the BoS algorithm.\nIn this paper, we pose a fundamental question of the role of codes in improving the latency of content retrieval in storage systems. The interplay between coding and queueing delay is of complex nature. As a ﬁrst step to make progress on this complex problem we propose and analyze a simpliﬁed setting of a single content divided into two parts and served by multiple servers. We see that even in this simpliﬁed setting the exact analysis of queueing delay for the systems using codes is intractable. As a result we provide a sub-optimal scheduling algorithm called Blocking-one Scheduling that allows us to theoretically quantify the gains in latency achieved by coded system as compared to a system that uses replication. The methodology we developed in this paper is applicable to a more general setting that allows splitting the content into more than 2 parts. Our future work will consider scenarios of serving multiple contents."},"refs":[{"authors":[{"name":"G. Ananthanarayana"},{"name":"S. Agarwa"},{"name":"S. Kandul"},{"name":"I. Stoica"}],"title":{"text":"A Greenberg, and  Scarlett: Coping with skewed content popularity in mapreduce clusters"}},{"authors":[{"name":"D. P. Bertseka"},{"name":"R. G. Gallager"}],"title":{"text":"Data Networks (2nd Edition)"}},{"authors":[{"name":"M. Bramso"},{"name":"Y. L"},{"name":"B. Prabhakar"}],"title":{"text":"Randomized load balancing with general service time distributions"}},{"authors":[{"name":"T. Dikalioti"},{"name":"A. G. Dimaki"},{"name":"T. H"},{"name":"M. Effros"}],"title":{"text":"On the delay of network coding over line networks"}},{"authors":[{"name":"A. Eryilma"},{"name":"A. Ozdagla"},{"name":"M. Medar"}],"title":{"text":"Ebad Ahmed"}},{"authors":[{"name":"A. Gandhi"},{"name":"M. Harchol-Baltera"}],"title":{"text":"How data center size impacts the effectiveness of dynamic power management"}},{"authors":[{"name":"A. Gandhi"},{"name":"M. Harchol-Balter"},{"name":"I. Adan"}],"title":{"text":"Server farms with setup costs"}},{"authors":[{"name":"L. Huan"},{"name":"S. Pawa"},{"name":"H. Zhan"},{"name":"K. Ramchandran"}],"title":{"text":"Codes can reduce queueing delay in data centers"}},{"authors":[{"name":"X. L"},{"name":"C.-C. Wan"},{"name":"X. Lin"}],"title":{"text":"Throughput and delay analysis on uncoded and coded wireless broadcast with hard deadline constraints"}},{"authors":[{"name":"Y. L"},{"name":"Q. Xi"},{"name":"G. Klio"},{"name":"A. Gelle"},{"name":"J. Laru"},{"name":"A. Greenberg"}],"title":{"text":"Join- idle-queue: A novel load balancing algorithm for dynamically scalable web services"}},{"authors":[{"name":"A. ParandehGheib"},{"name":"M. Medar"},{"name":"A. Ozdagla"},{"name":"S. Shakkottai"}],"title":{"text":"Avoid- ing interruptionsa qoe reliability function for streaming media applica- tions"}},{"authors":[{"name":"W. Yeo"},{"name":"A. Hoan"},{"name":"C. Tham"}],"title":{"text":"Minimizing delay for multicast- streaming in wireless networks with network coding"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565613.pdf"},"links":[{"id":"1569566567","weight":3},{"id":"1569565883","weight":2},{"id":"1569565663","weight":4},{"id":"1569564635","weight":3},{"id":"1569565867","weight":2},{"id":"1569561021","weight":2},{"id":"1569564669","weight":7},{"id":"1569565691","weight":27},{"id":"1569559617","weight":4},{"id":"1569566321","weight":2},{"id":"1569565097","weight":10},{"id":"1569566227","weight":11},{"id":"1569566697","weight":2},{"id":"1569566943","weight":4},{"id":"1569565091","weight":2},{"id":"1569566591","weight":2},{"id":"1569552245","weight":2},{"id":"1569565607","weight":19},{"id":"1569565495","weight":29},{"id":"1569564805","weight":3},{"id":"1569566081","weight":4},{"id":"1569566373","weight":11},{"id":"1569558325","weight":5},{"id":"1569565837","weight":2},{"id":"1569566671","weight":3},{"id":"1569566459","weight":2},{"id":"1569563411","weight":2},{"id":"1569565317","weight":3},{"id":"1569566319","weight":2},{"id":"1569558459","weight":4},{"id":"1569565291","weight":2},{"id":"1569564203","weight":3},{"id":"1569566999","weight":2},{"id":"1569565809","weight":15},{"id":"1569566579","weight":4},{"id":"1569564387","weight":14},{"id":"1569566497","weight":2},{"id":"1569566795","weight":21},{"id":"1569566963","weight":2},{"id":"1569566709","weight":2},{"id":"1569564989","weight":2},{"id":"1569566015","weight":2},{"id":"1569565897","weight":2},{"id":"1569566895","weight":4},{"id":"1569566889","weight":8},{"id":"1569566749","weight":2},{"id":"1569566269","weight":2},{"id":"1569561513","weight":2},{"id":"1569567009","weight":2},{"id":"1569566865","weight":2},{"id":"1569564647","weight":22},{"id":"1569566193","weight":11},{"id":"1569564311","weight":2},{"id":"1569565785","weight":4},{"id":"1569566679","weight":2},{"id":"1569565989","weight":14},{"id":"1569563981","weight":2},{"id":"1569559565","weight":2},{"id":"1569566063","weight":5},{"id":"1569558681","weight":3},{"id":"1569566759","weight":3},{"id":"1569566149","weight":2},{"id":"1569559995","weight":8},{"id":"1569558859","weight":2},{"id":"1569565213","weight":2},{"id":"1569566643","weight":4},{"id":"1569565841","weight":13},{"id":"1569566975","weight":8},{"id":"1569567665","weight":2},{"id":"1569561143","weight":2},{"id":"1569566489","weight":11},{"id":"1569565535","weight":8},{"id":"1569562867","weight":4},{"id":"1569566423","weight":3},{"id":"1569564795","weight":2},{"id":"1569566939","weight":2},{"id":"1569553537","weight":9},{"id":"1569565427","weight":3},{"id":"1569566403","weight":2},{"id":"1569566139","weight":3},{"id":"1569554971","weight":2},{"id":"1569566899","weight":2},{"id":"1569566209","weight":2},{"id":"1569566649","weight":31},{"id":"1569566371","weight":2},{"id":"1569566909","weight":4},{"id":"1569566127","weight":2},{"id":"1569565087","weight":2},{"id":"1569566473","weight":2},{"id":"1569564857","weight":2},{"id":"1569564333","weight":2},{"id":"1569566257","weight":2},{"id":"1569565033","weight":6},{"id":"1569566447","weight":2},{"id":"1569566357","weight":5},{"id":"1569565847","weight":3},{"id":"1569564353","weight":2},{"id":"1569566141","weight":2},{"id":"1569566661","weight":22},{"id":"1569565279","weight":8},{"id":"1569566115","weight":3},{"id":"1569565219","weight":2},{"id":"1569558509","weight":2},{"id":"1569564851","weight":7},{"id":"1569565185","weight":23},{"id":"1569565469","weight":17},{"id":"1569566505","weight":2},{"id":"1569565527","weight":2},{"id":"1569566853","weight":2},{"id":"1569567029","weight":3},{"id":"1569565363","weight":3},{"id":"1569566051","weight":3},{"id":"1569561123","weight":3},{"id":"1569566655","weight":2},{"id":"1569565441","weight":4},{"id":"1569565739","weight":2},{"id":"1569565311","weight":4},{"id":"1569566667","weight":2},{"id":"1569564097","weight":4},{"id":"1569563845","weight":2},{"id":"1569566407","weight":4},{"id":"1569565545","weight":2},{"id":"1569566857","weight":4},{"id":"1569565961","weight":2},{"id":"1569566387","weight":3},{"id":"1569566219","weight":3},{"id":"1569565439","weight":2},{"id":"1569566229","weight":3},{"id":"1569565415","weight":2},{"id":"1569564411","weight":6},{"id":"1569565665","weight":2},{"id":"1569566831","weight":3},{"id":"1569565523","weight":2},{"id":"1569566779","weight":18},{"id":"1569566479","weight":3},{"id":"1569565925","weight":3},{"id":"1569566261","weight":8},{"id":"1569565385","weight":2},{"id":"1569565575","weight":2},{"id":"1569565919","weight":2},{"id":"1569565181","weight":5},{"id":"1569566711","weight":2},{"id":"1569565865","weight":7},{"id":"1569566887","weight":5},{"id":"1569564919","weight":3},{"id":"1569561221","weight":3},{"id":"1569566917","weight":4},{"id":"1569566035","weight":20},{"id":"1569566547","weight":2},{"id":"1569565375","weight":2},{"id":"1569565041","weight":14},{"id":"1569566813","weight":2},{"id":"1569566771","weight":4},{"id":"1569564247","weight":5},{"id":"1569563975","weight":2},{"id":"1569564861","weight":4},{"id":"1569565529","weight":2},{"id":"1569556759","weight":2},{"id":"1569566619","weight":3},{"id":"1569566301","weight":7},{"id":"1569566911","weight":2},{"id":"1569564923","weight":10},{"id":"1569565039","weight":2},{"id":"1569565769","weight":2},{"id":"1569566601","weight":24},{"id":"1569565805","weight":2},{"id":"1569559919","weight":2},{"id":"1569566147","weight":2},{"id":"1569565561","weight":6},{"id":"1569555891","weight":4},{"id":"1569565089","weight":2},{"id":"1569565737","weight":2},{"id":"1569564463","weight":2},{"id":"1569565853","weight":6},{"id":"1569564123","weight":3},{"id":"1569566341","weight":3},{"id":"1569566635","weight":23},{"id":"1569551539","weight":2},{"id":"1569561397","weight":2},{"id":"1569556327","weight":2},{"id":"1569565113","weight":2},{"id":"1569564257","weight":2},{"id":"1569565583","weight":2},{"id":"1569565373","weight":2},{"id":"1569566973","weight":10},{"id":"1569561579","weight":2},{"id":"1569566839","weight":2},{"id":"1569565139","weight":9},{"id":"1569566663","weight":3},{"id":"1569565579","weight":39},{"id":"1569566067","weight":2},{"id":"1569566241","weight":2},{"id":"1569566609","weight":3},{"id":"1569566113","weight":6},{"id":"1569566727","weight":2},{"id":"1569565315","weight":5},{"id":"1569566417","weight":14}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S16.T1.2","endtime":"12:10","authors":"Longbo Huang, Sameer Pawar, Hao Zhang, Kannan Ramchandran","date":"1341575400000","papertitle":"Codes Can Reduce Queueing Delay in Data Centers","starttime":"11:50","session":"S16.T1: Coded Storage and Caching","room":"Kresge Rehearsal B (030)","paperid":"1569565613"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"5","spectral43":"21","spectral28":"25","spectral32":"21","spectral14":"11","spectral20":"18","spectral9":"3","spectral25":"9","spectral42":"8","spectral3":"2","spectral47":"7","spectral17":"13","louvain":"269","spectral36":"29","spectral39":"20","spectral10":"0","spectral15":"0","spectral33":"5","spectral5":"4","spectral21":"8","spectral44":"6","spectral26":"21","spectral40":"18","spectral8":"1","spectral11":"6","spectral4":"2","spectral37":"26","spectral48":"21","spectral22":"6","spectral23":"20","spectral12":"6","spectral50":"11","spectral19":"1","spectral34":"20","spectral45":"44","spectral7":"5","spectral49":"15","spectral38":"31","spectral24":"17","spectral13":"8","spectral31":"21","spectral29":"21","spectral35":"29","spectral30":"21","spectral41":"23","spectral27":"9","spectral18":"17","spectral46":"35","spectral2":"1","spectral16":"9"}}

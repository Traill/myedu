{"id":"1569552025","paper":{"title":{"text":"1-bit Hamming Compressed Sensing"},"authors":[{"name":"Tianyi Zhou"},{"name":"Dacheng Tao"}],"abstr":{"text":"Abstract\u2014Compressed sensing (CS) and 1-bit CS cannot directly recover quantized signals preferred in digital systems and require time consuming recovery. In this paper, we introduce 1-bit Hamming compressed sensing (HCS) that directly recovers a k-bit quantized signal of dimension n from its 1-bit measurements via invoking n times of Kullback-Leibler divergence based nearest neighbor search. Compared to CS and 1-bit CS, 1-bit HCS allows the signal to be dense, takes considerably less (linear and non-iterative) recovery time and requires substantially less measurements. Moreover, 1-bit HCS can accelerate 1- bit CS recover. We study a quantized recovery error bound of 1-bit HCS for general signals. Extensive numerical simulations verify the appealing accuracy, robustness, efﬁciency and consistency of 1-bit HCS."},"body":{"text":"Recent results in compressed sensing (CS) [1][2][3] prove that a sparse or compressible signal can be exactly recovered from its linear measurements, rather than uniform samplings, at a rate signiﬁcantly lower than the Nyquist rate. The measurement matrix is required to have the restricted isometry property (RIP) [4][5], or to be incoherent with the bases on which the signal is sparsely represented, for the purpose of ensuring the exact reconstruction via an p (0 ≤ p < 2) penalized/constrained minimization of the measurement error. Random matrix is desirable in CS for its promising incoherence and RIP.\nHowever, CS [6] encounters several problems when applied to practical digital systems, where analog-to-digital converters (ADCs) not only sample, but also quantize each measurement to a ﬁnite number of bits. One key problem is that CS cannot handle the quantized measurements (rounding of coarse quantization leads to large error, while ﬁne quantization requires expensive ADCs). Thus 1-bit CS [7][8] is developed to reconstruct sparse signals from 1- bit measurements, which capture the signs of the CS measurements. The 1-bit measurements signiﬁcantly reduce the costs and strengthen the robustness of hardware implementation. Although the 1-bit mea- surements lead to the loss of scale information, 1-bit CS ensures consistent reconstructions of signals on the unit 2 sphere [9][10]. Similar to RIP in CS, the binary -stable embedding (B SE) [8] of the 1-bit measurement guarantees the theoretical reconstruction and robustness.\nAnother important problem is that digital systems prefer to use the quantized recovery of the original signal, which they can directly process, but the recoveries of both CS [11] and 1-bit CS are real- valued. In order to apply them to digital systems, additional quanti- zation is required. Moreover, the time consuming optimization based and iterative recovery in CS and 1-bit CS limits their applications in practical systems, especially when signals are of high-dimension. In addition, we prefer to control the trade-off between speed and accuracy in signal recovery. Quantized recovery offers a possible solution, because the number of bits in quantization can be adjusted to reach different resolutions. The last problem is that CS or 1-bit CS achieves exact recovery under the Nyquist rate due to replacement of the previous uniform sampling with random linear measurements or\ntheir signs. However, the signal is restricted to be sparse. Quantization is a coarse and irreversible description of the original signal, and to recover it is the same as recovering a box constraint. Thus it is possible to recover the quantization of a dense signal from a small number of measurements. In this paper, we mainly consider the problem of fast quantized recovery of a general signal.\nThe primary contribution of this paper is developing 1-bit Ham- ming compressed sensing (HCS) to recover the quantized signal from its 1-bit measurements with extremely small time cost and without signal sparsity constraint. In compression, we adopt the 1- bit measurements [8] to guarantee consistency (rf. longer version) and B SE but employ them in a different way. In particular, we introduce a bijection between each dimension of the signal and a Bernoulli distribution. The underlying idea of 1-bit HCS is to estimate the Bernoulli distribution for each dimension from the 1-bit measurements, and thus each dimension of the signal can be recov- ered from the corresponding Bernoulli distribution. In recovery, we propose a k-bit HCS quantizer for the signal domain, whose intervals are the mappings of the uniform linear quantization boundaries for the Bernoulli distribution domain. 1-bit HCS searches the nearest neighbor of the estimated Bernoulli distribution among the boundaries and recovers the quantization of the corresponding dimension as the HCS quantizer interval associated with the nearest neighbor. The main signiﬁcance of 1-bit HCS is as follows: 1) it provides a direct and simple recovery of quantized signal for digital systems; 2) it only requires to compute nk Kullback-Leibler (KL) divergences for obtaining k-bit recovery of an n-dimensional signal, and is therefore considerably more efﬁcient than CS and 1-bit CS; 3) successful recovery can be obtained from only O(log n) measurements. Thus 1- bit HCS can be applied to general signals without sparse assumption. We theoretically study a quantized recovery error bound of 1-bit HCS by investigating the precision of the estimation and its impact on the KL divergence based nearest neighbor search. An HCS extension \u201ccompressed labeling\u201d [12] signiﬁcantly reduces the complexity of multi-label learning problem.\n1-bit HCS recovers the quantized signal directly from its quantized measurements, each of which is composed of a ﬁnite number of bits. We consider the extreme case of 1-bit measurements of a signal x ∈ R n , which are given by\nwhere sign(·) is an element-wise sign operator and A(·) maps x from R n to the Boolean cube B M := {−1, 1} M . Since the scale of the signal is lost in 1-bit measurements y (multiplying x with a positive scalar will not change the signs of the measurements), the consistent reconstruction can be obtained by enforcing the signal x ∈ Σ ∗ K := {x ∈ S n−1 : x 0 ≤ K} where S n−1 := {x ∈ R n : x 2 = 1} is the n-dimensional unit hyper-sphere.\nIn contrast to CS and 1-bit CS, 1-bit HCS does not recover the original signal, but reconstructs the quantized signal by recovering each dimension in isolation. In particular, according to Lemma 3.2 in [13], we show that there exists a bijection (cf. Theorem 1) between each dimension of the signal x and a Bernoulli distribution, which can be uniquely estimated from the 1-bit measurements. The underlying idea of 1-bit HCS is to estimate the Bernoulli distribution for each dimension, and recover the quantization of the corresponding dimension as the interval where the Bernoulli distribution\u2019s mapping lies in.\nTheorem 1. (Bijection) For a normalized signal x ∈ R n with x 2 = 1 and a normalized Gaussian random vector φ that is drawn\nuniformly from the unit 2 sphere in R n (i.e., each element of φ is ﬁrstly drawn i.i.d. from the standard Gaussian distribution N (0, 1) and then φ is normalized as φ/ φ 2 ), given the i th dimension of the signal x i and the corresponding coordinate unit vector e i = {0, · · · , 0, 1, 0, · · · , 0}, where 1 appears in the i th dimension, there exists a bijection P : R → P from x i to the Bernoulli distribution of the binary random variable s i = sign ( x, φ ) · sign ( e i , φ ):\nSince the mapping between x i and P (x i ) is bijective, given P (x i ), the i th dimension of x can be uniquely identiﬁed. According to the deﬁnition of s i , P (x i ) can be estimated from the instances of the random variable sign ( x, φ ) (sign ( e i , φ ) is also used but does not depend on x), which are exactly the 1-bit measurements y deﬁned in (1). Therefore, the 1-bit measurements y include sufﬁcient information to reconstruct x i from the estimation of P (x i ), and the recovery accuracy of x i depends on the accuracy of the estimation to P (x i ).\nGiven a signal x, its quantization q = Q(x) by HCS quantizer Q(·), and the quantized recovery q ∗ = R(y) obtained by 1-bit HCS reconstruction R(·) from the 1-bit measurements y = A(x), err H is the quantization error determined by the difference between q and q ∗ . The upper bound of err H will be given in Sections 4.\nThe primary contribution of this paper is the quantized recovery in 1-bit HCS, which reconstructs the quantized signal from its 1- bit measurements (1). Figure 1(a) illustrates 1-bit HCS quantized recovery. To deﬁne the HCS quantizer, we ﬁrstly ﬁnd k boundaries P j (j = 0, · · · , k − 1) (4) in Bernoulli distribution domain by imposing the uniform linear quantizer to the range of P − j . Given an\narbitrary x i , the nearest neighbor of P (x i ) among the k boundaries P j (j = 0, · · · , k−1) indicates the interval q i that x i lies in the signal domain. The k + 1 boundaries S j (j = 0, · · · , k) associated with the k intervals q j (j = 0, · · · , k) are calculated from the k boundaries P j (j = 0, · · · , k − 1) according to the bijection deﬁned in Theorem 1. In 1-bit HCS recovery, P (x i ) is estimated as ˆ P (x i ) from the 1-bit measurements y. Then the nearest neighbor of ˆ P (x i ) among the k boundaries P j (j = 0, · · · , k−1) is determined by comparing the KL- divergences between ˆ P (x i ) and P j . The quantization of x i deﬁned by HCS quantizer is recovered as the interval q i corresponding to the nearest neighbor.\nIn this section, we ﬁrst introduce the HCS quantizer, which is a mapping resulting from the uniform linear quantizer of the Bernoulli distribution domain to the signal domain. The quantized recovery procedure is composed of n times of KL-divergence based nearest neighbor searches. Thus it is a linear algorithm much faster than the conventional reconstruction algorithms of CS and 1-bit CS, which require optimization with the p (0 ≤ p < 2) constraint/penalty, or iterative thresholding/greedy search. We then study the upper bound of the quantized recovery error err H .\nSince 1-bit HCS aims at recovering the quantization of the original signal, we ﬁrstly introduce HCS quantizer, which deﬁnes the intervals and boundaries for quantization in the signal domain. These intervals and boundaries are uniquely derived from a predeﬁned uniform linear quantizer in the Bernoulli distribution domain. Given a signal and the boundaries of HCS quantizer, its k-bit quantization can be identiﬁed. We will show HCS quantizer performs closely to the uniform linear quantizer.\nNote that in the quantized recovery of 1-bit HCS, the reconstruction and quantization are simultaneously accomplished. Thus the HCS quantizer will not play an explicit role in the recovery procedure. However, it is related to and uniquely determined by the quantization of the Bernoulli distribution domain, which plays an important role in the recovery and explains the reconstruction q ∗ . Moreover, it will be applied to the error bound analyses for err H .\nWe introduce the HCS quantizer Q(·) by deﬁning a bijective mapping from the boundaries of the Bernoulli distribution domain to the intervals of the signal domain according to Theorem 1. Assume the range of a signal x is given by:\nBy applying the uniform linear quantizer with the quantization interval ∆ to the Bernoulli distribution domain, we get the corre-\nP + i = Pr (1) = 1 − Pr (−1) . \t , \t (4) i = 0, · · · , k − 1.\nWe deﬁne the k-bit HCS quantizer in the signal domain by computing its k + 1 boundaries as a mapping from the k boundaries P i (i = 0, · · · , k − 1) to R in the Bernoulli domain:\n   \n  \n, i = 1, · · · , k − 1; x sup , \t i = k.\nAlthough the mapping between the boundaries of HCS quantizer S i to the boundaries of the quantizer in the Bernoulli distribution domain P i is bijective, such mapping cannot be explicitly obtained. So it is difﬁcult to derive the corresponding quantizer in the Bernoulli distribution domain from a predeﬁned HCS quantizer. Thus HCS quantizer cannot be ﬁxed as a uniform linear quantizer and has to be computed from a predeﬁned quantizer in the Bernoulli distribution domain. Fortunately, HCS quantizer performs very closely to the uniform linear quantizer, especially when x i is not very close to −1 or 1. Figure 1(b) shows the fact.\nGiven a signal x and the boundaries deﬁned in (6), its k-bit quantization q is:\nThe k + 1 boundaries of the k-bit HCS quantizer in (6) deﬁne k intervals in R. Quantized recovery in 1-bit HCS reconstructs a quan- tized signal by estimating which interval each dimension of the signal x lies in. The estimation is obtained by a nearest neighbor search in the Bernoulli distribution domain. To be speciﬁc, an estimation of P (x i ) given in (2) can be derived from the 1-bit measurements y. For each P (x i ), we ﬁnd its nearest neighbor among the k boundaries P j (j = 0, · · · , k − 1) (4) in the Bernoulli distribution domain. The interval that x i lies in is then estimated as the interval of HCS quantizer corresponding to the nearest neighbor. KL-divergence measures the distance between two Bernoulli distributions in the nearest neighbor search.\nAccording to Theorem 1, the bijection from x i to a particular Bernoulli distribution, i.e., P (x i ) given in (2), has an unbiased estimation from the 1-bit measurements y\n    \n   \n= j : [y · sign (Φ i )] j = −1 /m, ˆ P (x i ) + = ˆ Pr (s i = 1)\nThe quantization of x i can then be recovered by searching the nearest neighbor of ˆ P (x i ) among the k boundary Bernoulli distribu- tions P j (j = 0, · · · , k − 1) in (4). In this paper, the distance between P j and ˆ P (x i ) is measured by the KL-divergence:\nThe interval that x i lies in among the k intervals deﬁned by the boundaries S j (j = 0, · · · , k) in (6) is identiﬁed as the one whose corresponding boundary distribution P j is the nearest neighbor of ˆ P (x i ). Therefore, the quantized recovery of x, i.e., q ∗ , is given by\nThe 1-bit HCS recovery algorithm is fully summarized in (10), which only includes simple computations without iteration and thus can be easily implemented in real systems. According to (10), the quantized recovery in 1-bit HCS requires nk computations of KL- divergence between two Bernoulli distributions. This indicates the high efﬁciency of 1-bit HCS (linear recovery time), and the trade-off between resolution (k) and time cost (nk).\nWe investigate the error bound of the quantized recovery (10) by studying the difference between q i (7) and q ∗ i , which are the quan- tization of x i and its quantized recovery by 1-bit HCS, respectively. The difference between q and q ∗ deﬁnes the error err H , which is the error caused by 1-bit HCS reconstruction (10):\n \nS q i − S q ∗ i +1 ≤ (q i − q ∗ i − 1) ∆ max , q i > q ∗ i ; 0, \t q i = q ∗ i ;\nThe ∆ max denotes the largest interval between neighboring bound- aries of the HCS quantizer, i.e., ∆ max = max j=1,··· ,k (S j − S j−1 ).\nIn order to investigate the difference between q i and q ∗ i , we study the upper bound for the probability of the event that the true quantization of x i is q i = 1 + α, while its recovery by 1-bit HCS is q ∗ i = 1 + β(β = α). According to the HCS quantizer (7) and the 1-bit HCS reconstruction (10), this probability is\nIn order to study the conditional probability in (13), we ﬁrst consider an equivalent event of β = arg min\nD KL P j ˆ P (x i ) , shown in the following Lemma 1.\nLemma 1. (Equivalence) The event that the nearest neighbor of ˆ P (x i ) among P j (j = 0, · · · , k − 1) is P β equals to the event that ˆ P (x i ) is closer to P β than both P β−1 and P β+1 , where the distance between P j and ˆ P (x i ) is measured by KL divergence (9), i.e.,\n \nD KL P β−1 ˆ P (x i ) − D KL P β ˆ P (x i ) > 0, D KL P β+1 ˆ P (x i ) − D KL P β ˆ P (x i ) > 0.\nBy using the equivalence in Lemma 1, the conditional probability given in (13) can be upper bounded by two other conditional probabilities, whose conditions are the two cases of the condition in (13).\nCorollary 1. (Upper bounds in two cases) The conditional proba- bility given in (13) can be upper bounded by\n    \n    \nPr D KL P β−1 ˆ P (x i ) − D KL P β ˆ P (x i ) > 0 | S α ≤ x i ≤ S α+1 ≤ S β ) ,\nPr D KL P β+1 ˆ P (x i ) − D KL P β ˆ P (x i ) > 0 | S β+1 ≤ S α ≤ x i ≤ S α+1 ) .\nHence we bound the conditional probability in (13) by exploring the upper bounds of the two conditional probabilities in Corollary 1. Proposition 1. (Two probabilistic bounds) The two conditional probabilities in (15) are upper bounded by\nBy using Lemma 1, Corollary 1 and Proposition 1, we have the following Theorem about the upper bound of the probability in (13). Theorem 2. (Quantized recovery bound) Given HCS quantizer Q(·) in (7) and 1-bit HCS reconstruction R(·) (10), the probability of the event that the true quantization of x i is q i = 1 + α while its recovery by 1-bit HCS is q ∗ i = 1 + β(q i = q ∗ i ) is upper bounded by\n           \n          \n(19) The minimum amount of 1-bit measurements that ensures the\nsuccessful quantized recovery in 1-bit HCS is then directly obtained from Theorem 2.\nCorollary 2. (Amount of measurements) 1-bit HCS successfully reconstructs x i with probability exceeding 1 − η (0 ≤ η ≤ 1) if the\nMoreover, 1-bit HCS successfully reconstructs the signal x with probability exceeding 1 − η if the number of measurements\nRemark: Corollary 2 states that the quantization of an n- dimensional signal x on the unit sphere can be successfully recovered by 1-bit HCS from m = O(log n) with high probability. Thus 1-bit HCS provides an economical sampling scheme that does not rely on sparse or compressible assumption to the signal.\nA new issue in 1-bit HCS is the trade-off between the measure- ment amount m and the recovery resolution k. According to the deﬁnition of δ i in (21), both the upper bound for the probability of reconstruction failure in (19) and the least number of measurements ensuring reconstruction success in (20) will be reduced if |q i − q ∗ i | increases. This indicates two facts: 1) the interval x i lies in is easier to be mistakenly recovered as its nearest intervals; and 2) when we increase the number of bits k in quantized recovery, x i will become closer to the boundaries S q−1 and S q , which leads to the decreasing of min i δ i in (22). In this case, the number of measurements m has to be increased in order to ensure a successful recovery.\nThis section evaluates 1-bit HCS and compares it with \u201cBIHT [8] (for 1-bit CS)+HCS quantizer\u201d on two groups of numerical experi- ments. We use average quantized recovery error n i=1 |q i − q ∗ i | /nk to measure err H shown in Section 3.3. In each trial, we draw a normalized Gaussian random matrix Φ ∈ R m×n given in Theorem 1 and a signal of length n and cardinality K, whose K nonzero entries drawn uniformly at random on the unit 2 sphere. Please refer to the supplementary material for complete experiments.\nWe ﬁrst study the phase transition properties of 1-bit HCS and 1-bit CS on quantized recovery error and on recovery time in the noiseless case. We conduct 1-bit HCS and \u201cBIHT+HCS quantizer\u201d for 10 5 trials. In particular, given ﬁxed n and k, we uniformly choose 100 different K/n values between 0 and 1, and 100 different m/n values between 0 and 4. For each {K/n, m/n} pair, we conduct 10 trials, i.e., 1-bit HCS recovery and \u201c1-bit CS+HCS quantizer\u201d of 10 n-dimensional signals with cardinality K from their m 1-bit measurements. The average quantized recovery errors and average time costs of the two methods on overall 10 4 {K/n, m/n} pairs are shown in Figure 2.\nIn Figure 2, the phase plots of quantized recovery error show the quantized recovery of 1-bit HCS is accurate if the 1-bit mea- surements are sufﬁcient. Compared to \u201c1-bit CS+HCS quantizer\u201d, 1-bit HCS needs slightly more measurements to reach the same recovery precision, because 1-bit CS recovers the exact signal, while 1-bit HCS recovers its quantization. This is an unavoidable price for direct recovery of quantization. However, the phase plots of quantized\nrecovery time shows that 1-bit HCS takes substantially less time than \u201c1-bit CS+HCS quantizer\u201d. Thus 1-bit HCS can signiﬁcantly improve the efﬁciency of practical digital systems and eliminate the hardware cost for additional quantization. So the trade-off is quite advantageous.\nWe show the trade-off between quantized recovery error and the amount of measurements on 2500 trials for noisy signals of different n, K, k and signal-to-noise ratio (SNR). Given ﬁxed n, K, k and SNR, we uniformly choose 50 values of m between 0 and 16n. For each m value, we conduct 50 trials of 1-bit HCS recovery and \u201c1- bit CS+HCS quantizer\u201d by recovering the quantizations of 50 noisy signals from their m 1-bit measurements. The quantized recovery error and time cost of each trial are shown in Figure 3.\nFigure 3 shows that the quantized recovery error of both 1-bit HCS and \u201c1-bit CS+HCS quantizer\u201d drops drastically with an increase in the number of measurements. For dense signals with large noise, the two methods perform nearly the same on the recovery accuracy. This indicates that 1-bit HCS works well on dense signals and is robust to noise compared to CS and 1-bit CS. In addition, the time taken for 1-bit HCS increases substantially slower than that of \u201c1-bit CS+HCS quantizer\u201d with an increase in the number of measurements.\nWe would like to thank the reviewers for their constructive com- ments. This work is supported by the Australian Research Council discovery project (ARC DP-120103730)."},"refs":[{"authors":[{"name":"D. L. Donoho"}],"title":{"text":"Compressed sensing"}},{"authors":[{"name":"E. J. Cand`es"},{"name":"T. Tao"}],"title":{"text":"Near-optimal signal recovery from random projections: Universal encoding strategies?"}},{"authors":[{"name":"E. Cand`es"},{"name":"M. Rudelson"},{"name":"T. Tao"},{"name":"R. Vershynin"}],"title":{"text":"Error correction via linear programming"}},{"authors":[{"name":"E. J. Cand`es"},{"name":"J. K. Romberg"},{"name":"T. Tao"}],"title":{"text":"Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency informa- tion"}},{"authors":[{"name":"E. J. Cand`es"},{"name":"J. K. Romberg"},{"name":"T. Tao"}],"title":{"text":"Stable signal recovery from incomplete and inaccurate measurements"}},{"authors":[{"name":"A. M. Bruckstein"},{"name":"D. L. Donoho"},{"name":"M. Elad"}],"title":{"text":"From sparse solutions of systems of equations to sparse modeling of signals and images"}},{"authors":[{"name":"P. T. Boufounos"},{"name":"R. G. Baraniuk"}],"title":{"text":"One-bit compressive sensing"}},{"authors":[{"name":"L. Jacques"},{"name":"J. N. Laska"},{"name":"P. T. Boufounos"},{"name":"R. G. Baraniuk"}],"title":{"text":"Robust 1- bit compressive sensing via binary stable embeddings of sparse vectors"}},{"authors":[{"name":"P. T. Boufounos"}],"title":{"text":"Greedy sparse signal reconstruction from sign mea- surements"}},{"authors":[{"name":"J. N. Laska"},{"name":"Z. Wen"},{"name":"W. Yin"},{"name":"R. G. Baraniuk"}],"title":{"text":"Trust, but verify: Fast and accurate signal recovery from 1-bit compressive measurements"}},{"authors":[{"name":"E. J. Cand`es"},{"name":"T. Tao"}],"title":{"text":"The dantzig selector: statistical estimation when p is much larger than n"}},{"authors":[{"name":"T. Zhou"},{"name":"D. Tao"},{"name":"X. Wu"}],"title":{"text":"Compressed labeling on distilled labelsets for multi-label learning"}},{"authors":[{"name":"M. X. Goemans"},{"name":"D. P. Williamson"}],"title":{"text":"Improved approximation algo- rithms for maximum cut and satisﬁability problems using semideﬁnite programming"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569552025.pdf"},"links":[{"id":"1569564843","weight":5},{"id":"1569566527","weight":2},{"id":"1569565383","weight":2},{"id":"1569565883","weight":3},{"id":"1569564889","weight":5},{"id":"1569565377","weight":4},{"id":"1569566385","weight":4},{"id":"1569566799","weight":8},{"id":"1569565067","weight":3},{"id":"1569559665","weight":2},{"id":"1569561021","weight":2},{"id":"1569566815","weight":2},{"id":"1569566875","weight":4},{"id":"1569566981","weight":3},{"id":"1569566433","weight":4},{"id":"1569566321","weight":4},{"id":"1569566683","weight":3},{"id":"1569560629","weight":2},{"id":"1569566869","weight":11},{"id":"1569565097","weight":3},{"id":"1569566091","weight":3},{"id":"1569566697","weight":3},{"id":"1569566597","weight":2},{"id":"1569566943","weight":5},{"id":"1569565091","weight":2},{"id":"1569566591","weight":2},{"id":"1569566571","weight":2},{"id":"1569552245","weight":2},{"id":"1569565495","weight":2},{"id":"1569565227","weight":6},{"id":"1569564481","weight":3},{"id":"1569566415","weight":3},{"id":"1569564805","weight":2},{"id":"1569567005","weight":12},{"id":"1569566469","weight":4},{"id":"1569566081","weight":3},{"id":"1569565355","weight":4},{"id":"1569564469","weight":3},{"id":"1569551535","weight":14},{"id":"1569566765","weight":36},{"id":"1569565547","weight":4},{"id":"1569566871","weight":2},{"id":"1569566653","weight":11},{"id":"1569565461","weight":4},{"id":"1569564731","weight":3},{"id":"1569566207","weight":2},{"id":"1569566671","weight":5},{"id":"1569566303","weight":36},{"id":"1569564233","weight":2},{"id":"1569563411","weight":3},{"id":"1569565317","weight":3},{"id":"1569566941","weight":4},{"id":"1569566033","weight":2},{"id":"1569565291","weight":2},{"id":"1569564203","weight":6},{"id":"1569562685","weight":2},{"id":"1569566467","weight":2},{"id":"1569565771","weight":5},{"id":"1569566999","weight":2},{"id":"1569565859","weight":2},{"id":"1569565809","weight":2},{"id":"1569566843","weight":2},{"id":"1569558483","weight":5},{"id":"1569566563","weight":2},{"id":"1569559221","weight":2},{"id":"1569556091","weight":7},{"id":"1569565347","weight":3},{"id":"1569565455","weight":2},{"id":"1569566497","weight":5},{"id":"1569566963","weight":2},{"id":"1569566523","weight":2},{"id":"1569565897","weight":3},{"id":"1569551763","weight":2},{"id":"1569565953","weight":5},{"id":"1569566895","weight":2},{"id":"1569566749","weight":4},{"id":"1569566269","weight":2},{"id":"1569564189","weight":2},{"id":"1569561513","weight":3},{"id":"1569566985","weight":2},{"id":"1569567009","weight":2},{"id":"1569566865","weight":2},{"id":"1569566193","weight":2},{"id":"1569564337","weight":5},{"id":"1569566343","weight":2},{"id":"1569565785","weight":3},{"id":"1569566167","weight":6},{"id":"1569566679","weight":2},{"id":"1569565989","weight":2},{"id":"1569561085","weight":3},{"id":"1569566617","weight":3},{"id":"1569559565","weight":3},{"id":"1569566905","weight":2},{"id":"1569566733","weight":4},{"id":"1569566311","weight":4},{"id":"1569563307","weight":6},{"id":"1569558681","weight":3},{"id":"1569566759","weight":9},{"id":"1569566149","weight":3},{"id":"1569566657","weight":3},{"id":"1569558859","weight":5},{"id":"1569566643","weight":2},{"id":"1569566511","weight":6},{"id":"1569566719","weight":3},{"id":"1569566991","weight":4},{"id":"1569567665","weight":2},{"id":"1569564611","weight":2},{"id":"1569562867","weight":2},{"id":"1569566845","weight":2},{"id":"1569564795","weight":41},{"id":"1569567015","weight":3},{"id":"1569566437","weight":2},{"id":"1569566811","weight":2},{"id":"1569566851","weight":5},{"id":"1569565735","weight":3},{"id":"1569553909","weight":2},{"id":"1569566687","weight":3},{"id":"1569566939","weight":2},{"id":"1569553537","weight":4},{"id":"1569565427","weight":3},{"id":"1569566403","weight":4},{"id":"1569552251","weight":3},{"id":"1569566139","weight":3},{"id":"1569553519","weight":3},{"id":"1569566231","weight":2},{"id":"1569566513","weight":10},{"id":"1569566425","weight":2},{"id":"1569554971","weight":2},{"id":"1569565501","weight":2},{"id":"1569566899","weight":2},{"id":"1569566209","weight":2},{"id":"1569565559","weight":8},{"id":"1569565655","weight":2},{"id":"1569566127","weight":4},{"id":"1569563763","weight":4},{"id":"1569566473","weight":4},{"id":"1569566913","weight":7},{"id":"1569566809","weight":18},{"id":"1569566629","weight":2},{"id":"1569565033","weight":2},{"id":"1569566447","weight":2},{"id":"1569565847","weight":3},{"id":"1569563897","weight":3},{"id":"1569566141","weight":2},{"id":"1569565633","weight":2},{"id":"1569565279","weight":2},{"id":"1569566115","weight":2},{"id":"1569565219","weight":3},{"id":"1569554759","weight":2},{"id":"1569565185","weight":8},{"id":"1569566223","weight":2},{"id":"1569565029","weight":2},{"id":"1569565393","weight":3},{"id":"1569562207","weight":3},{"id":"1569566191","weight":2},{"id":"1569567033","weight":3},{"id":"1569565527","weight":2},{"id":"1569561123","weight":2},{"id":"1569566673","weight":4},{"id":"1569566233","weight":3},{"id":"1569566893","weight":2},{"id":"1569564097","weight":2},{"id":"1569566407","weight":4},{"id":"1569560349","weight":2},{"id":"1569566275","weight":2},{"id":"1569565545","weight":7},{"id":"1569566857","weight":10},{"id":"1569565961","weight":6},{"id":"1569566245","weight":2},{"id":"1569560503","weight":2},{"id":"1569566219","weight":2},{"id":"1569565439","weight":2},{"id":"1569566229","weight":2},{"id":"1569566949","weight":3},{"id":"1569561623","weight":2},{"id":"1569566383","weight":3},{"id":"1569564485","weight":2},{"id":"1569564411","weight":3},{"id":"1569559199","weight":2},{"id":"1569566831","weight":2},{"id":"1569565549","weight":2},{"id":"1569564175","weight":3},{"id":"1569566983","weight":3},{"id":"1569566097","weight":2},{"id":"1569566873","weight":11},{"id":"1569565215","weight":3},{"id":"1569565181","weight":2},{"id":"1569566711","weight":4},{"id":"1569566927","weight":2},{"id":"1569565661","weight":17},{"id":"1569566887","weight":2},{"id":"1569565273","weight":7},{"id":"1569566267","weight":2},{"id":"1569552037","weight":3},{"id":"1569564919","weight":2},{"id":"1569566737","weight":2},{"id":"1569566429","weight":2},{"id":"1569561221","weight":2},{"id":"1569566917","weight":3},{"id":"1569566035","weight":2},{"id":"1569565353","weight":8},{"id":"1569564683","weight":2},{"id":"1569564305","weight":5},{"id":"1569566691","weight":2},{"id":"1569565421","weight":3},{"id":"1569566651","weight":6},{"id":"1569565349","weight":4},{"id":"1569566137","weight":2},{"id":"1569565645","weight":4},{"id":"1569566715","weight":9},{"id":"1569566755","weight":4},{"id":"1569566819","weight":4},{"id":"1569564703","weight":4},{"id":"1569566713","weight":2},{"id":"1569565293","weight":2},{"id":"1569566771","weight":7},{"id":"1569566641","weight":4},{"id":"1569565425","weight":9},{"id":"1569564437","weight":2},{"id":"1569551905","weight":2},{"id":"1569564861","weight":4},{"id":"1569566487","weight":3},{"id":"1569565529","weight":8},{"id":"1569566619","weight":2},{"id":"1569566397","weight":2},{"id":"1569558779","weight":2},{"id":"1569565233","weight":2},{"id":"1569563721","weight":4},{"id":"1569565593","weight":2},{"id":"1569560235","weight":4},{"id":"1569566817","weight":2},{"id":"1569566435","weight":2},{"id":"1569567483","weight":3},{"id":"1569565367","weight":2},{"id":"1569566299","weight":3},{"id":"1569565769","weight":4},{"id":"1569565805","weight":7},{"id":"1569566933","weight":3},{"id":"1569563919","weight":2},{"id":"1569567691","weight":10},{"id":"1569565389","weight":3},{"id":"1569559919","weight":4},{"id":"1569566147","weight":5},{"id":"1569566057","weight":4},{"id":"1569560785","weight":2},{"id":"1569565561","weight":4},{"id":"1569566847","weight":2},{"id":"1569565035","weight":5},{"id":"1569559597","weight":2},{"id":"1569565737","weight":2},{"id":"1569560459","weight":2},{"id":"1569564463","weight":2},{"id":"1569565853","weight":2},{"id":"1569550425","weight":3},{"id":"1569566341","weight":2},{"id":"1569565889","weight":4},{"id":"1569566635","weight":4},{"id":"1569566611","weight":12},{"id":"1569565565","weight":3},{"id":"1569565635","weight":2},{"id":"1569565731","weight":2},{"id":"1569566797","weight":3},{"id":"1569566125","weight":6},{"id":"1569566413","weight":2},{"id":"1569566375","weight":2},{"id":"1569564257","weight":3},{"id":"1569565373","weight":2},{"id":"1569566973","weight":2},{"id":"1569565031","weight":2},{"id":"1569551541","weight":2},{"id":"1569566839","weight":3},{"id":"1569566663","weight":4},{"id":"1569565579","weight":2},{"id":"1569566067","weight":4},{"id":"1569566825","weight":10},{"id":"1569566241","weight":2},{"id":"1569564807","weight":2},{"id":"1569563007","weight":24},{"id":"1569566443","weight":6},{"id":"1569566727","weight":11}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S10.T9.3","endtime":"12:30","authors":"Tianyi Zhou, Dacheng Tao","date":"1341403800000","papertitle":"1-bit Hamming Compressed Sensing","starttime":"12:10","session":"S10.T9: Compressive Sensing and Algorithms","room":"Stratton West Lounge (201)","paperid":"1569552025"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"3","spectral43":"1","spectral28":"21","spectral32":"17","spectral14":"4","spectral20":"0","spectral9":"6","spectral25":"2","spectral42":"3","spectral3":"1","spectral47":"14","spectral17":"16","louvain":"444","spectral36":"28","spectral39":"33","spectral10":"4","spectral15":"12","spectral33":"3","spectral5":"2","spectral21":"13","spectral44":"26","spectral26":"7","spectral40":"25","spectral8":"1","spectral11":"4","spectral4":"3","spectral37":"21","spectral48":"23","spectral22":"10","spectral23":"14","spectral12":"9","spectral50":"37","spectral19":"0","spectral34":"23","spectral45":"10","spectral7":"5","spectral49":"37","spectral38":"2","spectral24":"16","spectral13":"7","spectral31":"26","spectral29":"18","spectral35":"0","spectral30":"9","spectral41":"5","spectral27":"18","spectral18":"4","spectral46":"4","spectral2":"0","spectral16":"2"}}

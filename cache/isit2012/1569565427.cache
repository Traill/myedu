{"id":"1569565427","paper":{"title":{"text":"Matrix Exponential Learning: Distributed Optimization in MIMO systems"},"authors":[{"name":"Panayotis Mertikopoulos"},{"name":"E. Veronica Belmega"},{"name":"Aris L. Moustakas"}],"abstr":{"text":"Abstract\u2014We analyze the problem of ﬁnding the optimal signal covariance matrix for multiple-input multiple-output (MIMO) multiple access channels by using an approach based on \u201dex- ponential learning\u201d, a novel optimization method which applies more generally to (quasi-)convex problems deﬁned over sets of positive-deﬁnite matrices (with or without trace constraints). If the channels are static, the system users converge to a power allocation proﬁle which attains the sum capacity of the channel exponentially fast (in practice, within a few iterations); otherwise, if the channels ﬂuctuate stochastically over time (following e.g. a stationary ergodic process), users converge to a power proﬁle which attains their ergodic sum capacity instead.\nAn important feature of the algorithm is that its speed can be controlled by tuning the users\u2019 learning rate; correspondingly, the algorithm converges within a few iterations even when the number of users and/or antennas per user in the system is large."},"body":{"text":"mance gains [1], [2], MIMO has become an integral component of numerous state-of-the-art wireless protocols (4G, HSPA+, 802.11n WiFi and WiMAX to name but a few). As a result, con- siderable impetus has been a↵orded to developing distributed algorithms that would allow the users of a MIMO system to attain their performance limits at a network level.\nOn that account, since the actual theoretical limits of MIMO models still elude us even in basic network models (such as the interference channel), it is useful to start instead with the mutual information for Gaussian input and noise, and to optimize the input covariance matrix of each transmitter in the presence of interference from other network users. In this way, one obtains a nonlinear (and possibly non-convex) optimization problem deﬁned over a set of positive-deﬁnite matrices, representing the users\u2019 power allocation policies (i.e. the spread of their symbol distributions over their antennas). However, given the non- explicit nature of the problem\u2019s constraints, standard gradient descent or interior point methods do not apply, so these prob- lems are usually solved by means of the classical waterﬁlling algorithm [3], properly adapted to multi-user environments [4].\nThe most recent and general incarnations of waterﬁlling [5] are fully distributed and thus apply to large, unregulated networks where users cannot be assumed to adhere to central control. Unfortunately however, the convergence of these al- gorithms typically depends on the channel satisfying certain \u201cmild-interference\u201d conditions [6] which, quite often, fail to hold: in fact, in the simple case of a single receiver and several transmitters who communicate over non-overlapping channels (the parallel multiple access channel (PMAC) model), it was shown that these conditions always fail [7]. Furthermore, if the channels are not static but evolve over time following a stationary ergodic process (e.g. due to fading), then the problem becomes signiﬁcantly more di cult (see e.g. [8] for a survey or [9] for some recent results in the asymptotic regime).\nInstead of taking a waterﬁlling approach, we present here an optimization method for problems deﬁned over sets of positive- deﬁnite matrices which works by tracking the gradient of the objective function in an unconstrained space, and then maps the resulting orbits back to the original (constrained) state space via exponentiating (an approach similar to the one used in [10] for learning a positive semidiﬁnite matrix online). In this manner, if the function to be minimized is convex, we show that this method of \u201cexponential learning\u201d converges exponentially fast to a global minimum (Theorem 1).\nObviously, this method applies to a wide array of MIMO problems, but for concreteness, we will focus on the MIMO multiple access channel (MAC) with transmit power con- straints. In this setting, we show that exponential learning converges to the system\u2019s maximum achievable sum rate, and this convergence is independent of whether the channels are static or ergodic (Theorem 2). More importantly, the speed of this convergence can be controlled by a learning rate parameter, allowing the system to equilibrate within a few iterations even for very large numbers of users and/or antennas per user.\nWe begin by considering a vector Gaussian multiple access channel with a ﬁnite set K = {1, . . . , K} of K wireless users, each of whom transmits simultaneously over m k antennas to an n-antenna receiver who decodes individual messages by treat- ing the signals of other users as interference. More speciﬁcally, this corresponds to the familiar baseband signal model:\ny = P k H k x k + z, \t (1) where y 2 C n is the aggregate message reaching the receiver, x k 2 C m k is the individual message transmitted by user k 2 K, H k 2 C n⇥m k is the corresponding n ⇥ m k channel matrix and\nz 2 C n is the channel noise (assumed zero-mean Gaussian, and without loss of generality, with identity covariance matrix).\nIn this context, the total transmit power of user k is simply E ⇥kx k k 2 ⇤ = tr( P k ), where the expectation is taken over the codebook of user k (assumed Gaussian), and P k denotes the covariance matrix P k = cov( x k ) = E ⇥x k x \u2020 k ⇤. As is then cus- tomary, the performance metric that we will be using is the system\u2019s achievable sum rate, i.e. the maximum information transmission rate for a given set of covariance matrices.\nThis objective naturally depends on the variability of the channel matrices H over time, so we will consider two di↵erent (and diametrically opposed) scenarios:\na) Static Channels: the channel matrices H k , k 2 K are drawn randomly at the outset of the transmission but remain ﬁxed for its duration. In this case, the system\u2019s sum rate is [2]:\n( P) = log det ⇣ I + P k H k P k H \u2020 k ⌘ , \t (2) where P denotes the collective proﬁle P = (P 1 , . . . , P K ).\nb) Fast-fading Channels: in the presence of fast fading, the channel matrices H k are stationary ergodic processes with a characteristic time-scale much faster than the typical trans- mission block (for simplicity, we will also assume that they are temporally uncorrelated). 1 Under these assumptions, we have the following expression for the users\u2019 achievable sum rate [12]:\nIn both the static and the ergodic case, higher transmit powers lead to higher sum rates (individually at least), so users can be assumed to saturate their power constraints.In this way, we obtain the optimization problem:\nsubject to P k < 0, tr( P k ) = P k (k = 1, . . . , K), (MP) where F = or F = depending on the channel model, and the power levels P k are non-negative real numbers.\nAs is well-known, and are both concave, so F is convex; furthermore, if we denote each user\u2019s state space by X k = {P k 2 C m k ⇥m k : P k < 0, tr( P k ) = P k }, then it is easy to see that the problem\u2019s state space X ⌘ Q k X k of (MP) is also convex (viewed as a subset of the complex space C Q , Q = P k m 2 k ), making (MP) itself convex. Our goal will thus be to present a general solution method for problems of the type (MP), which when restricted to the objectives (2)-(3), will allow the users of the channel to attain their sum capacity. 2\nIt is important to remark here that any solution of (MP) with respect to the sum rate objectives (2)-(3) is also individually optimal in the sense that users cannot improve their individual rates by unilaterally changing their power matrices P k . Indeed, under the single user decoding (SUD) scheme in which the\nreceiver treats the signal of all other users as interference, the achievable rate of user k 2 K for static channels is just:\nu k ( P) = log det I + H k P k H \u2020 k W 1 k , \t (4) where W k = I + P ` ,k H ` P ` H \u2020 ` is the interference-plus-noise matrix for user k; on the other hand, for ergodically ﬂuctuating channels, we have the expression:\nu k ( P) = E ⇥u k ( P)⇤. \t (5) The sum rate functions (2)\u2013(3) obviously do not correspond\nto the sums of (4)\u2013(5) over all transmitters k 2 K, but, as was shown in [13], any solution Q of (MP) with the static objective (2) (resp. the ergodic objective (3)) will satisfy:\nu k ( Q) u k ( Q 0 k ; Q k ) (resp. u k ( Q) u k ( Q 0 k ; Q k )), (6) for all k 2 K, i.e. it will be at Nash equilibrium. 3 In other words, users are aligned with their global objective in the MIMO multiple access channel, so solving (MP) is both globally and individually optimal: even selﬁsh users have nothing to gain by unilaterally deviating from the global optimum of (MP).\nThe main challenge in solving (MP) is that the positivity constraints P k < 0 cannot be expressed in functional form, so (Lagrangian) descent or interior point methods do not readily apply; instead, the static MIMO problem (2) is usually solved by the well-known method of waterﬁlling [4], [5], [9]. Our aim here will be to overcome this di culty and present an interior point method which does apply to the problem, in both the static and ergodic incarnation of Eqs. (2) and (3) respectively.\nOne special case of (MP) which can be solved by a variant gradient descent method is the so-called \u201cparallel MAC\u201d where the channels do not overlap and the matrices P k are diagonal: P k = diag(p k,1 , . . . , p k,m k ), with p k↵ 0 and P m k = 1 p k = P k . In this setting, the system\u2019s conﬁguration space X is a product of simplices, and if we let v k↵ = @ F @ p k↵ denote (minus) the gradient of F, then all interior orbits of the dynamics\n˙p k↵ = p k↵ ⇣ v k↵ P 1 k P m k = 1 p k v k ⌘ , ↵ = 1, . . . , m k , (7) converge to the minimum of F exponentially fast [11]. 4\nThis dynamical system is known in game theory and biology as the replicator equation, and it is one of the most well- studied models for the evolution of biological populations under natural selection [15], [16]. Regrettably however, this approach cannot be extended to the MIMO case because there is no obvious way to treat (7) as a matrix equation. On the other hand, the replicator dynamics (7) can also be derived from the \u201cexponential learning\u201d scheme [17]:\n˙y k↵ = v k↵ \t (8a) p k↵ = P k exp(y k↵ ) P m k\nIn this learning context (related itself to the inverse logit choice model of [18]), the auxiliary \u201cscore\u201d variable y k↵ 2 R simply measures how well the eigenvalue p k↵ has \u201clearned\u201d the gradient vector v which leads to higher sum rates in the unconstrained y-space. Hence, the only di↵erence between the replicator dynamics and exponential learning is one of view- point: (7) is written directly on the state space of the system (so extra care must be taken in order to satisfy the problem\u2019s constraints), 5 while the otherwise equivalent scheme of (8a) is an unconstrained descent which relies on the Gibbs distribution (8b) to map solutions back to the system\u2019s original state space.\nMotivated by the above, we introduce the following expo- nential learning method for the matrix program (MP):\n˙Y k = V k , \t (9a) P k = P k exp( Y k ) tr(exp( Y\n, \t (9b) where V k ⌘ @F @P ⇤ k is the conjugate derivative of F w.r.t. P k . 6\nNeedless to say, in a MIMO setting, this scheme hinges on users being able to calculate the gradient matrices V k . To that end, a di↵erentiation of the static sum rate of (2) gives:\nwhere W = I + P ` H ` P ` H \u2020 ` is the aggregate signal-plus-noise covariance matrix, assumed to be measured at the receiver end and then made known to the transmitters (e.g. by broadcast- ing) under the same hypotheses used in standard water-ﬁlling schemes [4], [5]. We thus obtain:\nRequire: For all k 2 K, pick Hermitian initial score matrices Y k 2 C m k ⇥m k and positive learning rates k > 0. t   0;\nP k   exp( k Y k ) tr(exp( k Y k )); end for\nRemark 1. We ﬁrst note that exponential learning as deﬁned above has the following desirable properties:\n(P1) It is distributed: users may update the algorithm based on local measurements and information.\n(P2) It is reinforcing: users move along a direction which increases their individual rates (see also Proposition 3).\n(P3) It is stateless: users are oblivious to the state of the algorithm, even to the existence of other users.\nRemark 2. We also see that exponential learning does not di↵erentiate between the static and fast-fading regime: the matrices H k (and obviously W) are simply the ones measured at the t-th iteration of the algorithm. In the next section, we will show that if the channel is static, exponential learning converges to the maximum of the static sum rate ; otherwise, if the channel matrices evolve ergodically over time, the users converge to the maximum of the ergodic sum rate .\nRemark 3. The discrete-time implementation of the exponential learning dynamics has two additional (and important) compo- nents that are not present in (9): i) the users \u201clearning rates\u201d\nk > 0; and ii) the step sequence (t). The time-step se- quence (t) is a standard feature of both deterministic [19] and stochastic [20] optimization algorithms and its role is to ensure convergence when passing from the continuous to the discrete. On the other hand, the role of the learning rate parameter k is much more interesting. Indeed, k can be interpreted as the inverse temperature of the (matrix-valued) Gibbs distribution (9b), and as in simulated annealing, it controls the algorithm\u2019s speed: for small , learning is slower and smoother, while for larger , the algorithm induces rapid changes and then freezes (see also Fig. 1).\nIn this section, we will focus on the behavior and con- vergence properties of the exponential learning dynamics (9) and the corresponding discrete-time algorithm. We thus begin by establishing that the dynamics (9) are consistent, i.e. they respect the structure of the matrix state space X:\nProposition 1. For any Hermitian initialization Y k (0), k 2 K, the corresponding solution P(t) of the exponential learning dynamics (9) remains in X for all t; in particular, P k (t) < 0 and tr( P k (t)) = P k for all k 2 K and for all t 0.\nSketch of proof: Note that V k is Hermitian whenever the Y k are, so if we start with Hermitian initial conditions in (9a), Y k (t) will remain Hermitian for all time. As a result, P k (t) / exp(Y k (t)) will be positive-deﬁnite as well, and the trace condition tr( P k (t)) = P k follows immediately from (9b).\nProposition 1 allows us to overcome the important hurdle of consistence in a surprisingly painless way: instead of specifying the dynamics directly on X (a very hard task given the implicit nature of the positivity constraints P k < 0), the scheme (9) evolves in an unconstrained space and trajectories are then mapped to the original state space via the Gibbs map (9b).\nThat said, it is only natural to ask what are the dynamics that govern the evolution of P(t) in X; to that end, we have:\nProposition 2. Let P(t) be an interior solution orbit of the dy- namics (9). If {p k↵ (t), u k↵ (t)}, ↵ = 1, . . . , m k , is an eigensystem of P k , k 2 K, and we set V k ↵ ⌘ u \u2020 k↵ V k u k , then:\n˙p k↵ = p k↵ ⇣ V k ↵↵ P 1 k P m k = 1 p k V k ⌘ , \t (11a) ˙ u k↵ = X ,↵ V k ↵ ⇣ log p k↵ log p k ⌘ 1 u k . \t (11b)\nFrom a mathematical viewpoint, Proposition 2 (proven by taking the Fr´echet derivative of (9b) in an eigen-decomposition of P) is a reformulation of the exponential learning dynamics\n(9), so its importance lies in that it illuminates the evolution of the actual state variables P k . 7 On the other hand, from a compu- tational standpoint, (11) represents a signiﬁcant simpliﬁcation of the exponential learning algorithm because users can employ it to update their power allocation policies directly, and without ﬁrst having to diagonalize/exponentiate the score matrices Y k .\nThis computational beneﬁt is obviously key when one needs to operate with sizable antenna arrays; putting such compu- tational issues aside however, the key property of exponential learning is that its trajectories always descend the objective F:\nProposition 3. If P(t) is a (non-stationary) solution of the exponential learning dynamics (9), then F( P(t)) is decreasing.\nSketch of proof: By the chain rule for matrices, we obtain dF dt = P k tr @F @ P ⇤ k · ˙P \u2020 k = P k tr V k ˙P k ). It can then be shown that this quantity is negative unless ˙ P k = 0 (in which case dF/dt vanishes), so our assertion follows.\nProposition 3 shows that the system\u2019s sum rate always in- creases along the trajectories of exponential learning; hence, with F convex, we immediately see that exponential learning converges to the minimum set of F (i.e. attains its sum capacity). Nonetheless, F need not be strictly convex (and in the static case it isn\u2019t [7]), so this convergence result does not imply that exponentially learning actually converges to a point.\nFrom a systems point of view, this is an important question because it is crucial to predict the users\u2019 end power allocation policies (and not only the system\u2019s sum capacity). To that end, our next result is that exponential learning does converge to a point, and, in fact, it converges exponentially fast:\nTheorem 1. For any initial Hermitian initialization Y k (0), k 2 K, the exponential learning dynamics (9) converge to a (possibly initialization-dependent) point Q ⇤ which minimizes F. Moreover, there exists a positive constant c > 0 such that:\nkP(t) Q ⇤ k  kP(0) Q ⇤ k e ct . \t (12) Sketch of proof: The basic ingredient for our proof is\nD KL ( Q k P) ⌘ X k tr ⇥ Q k (log Q k log P k )⇤. (13) By Klein\u2019s inequality [21], we have that D KL ( Q k P) is strictly convex in P and positive, except at Q where it vanishes; more importantly, a di↵erentiation of (13) yields the key expression:\ntr ⇥( Q k P k ) V k ⇤ , (14) which shows that d dt D KL ( Q k P(t))  0 if Q is a minimum point of F (recall that F is convex so dF( Q) · Z 0 for all Z tangent to X at a minimum point Q). 9 Thus, given that P(t) converges to the minimum set X ⇤ of F by Proposition 3, it follows (by compactness of X) that the orbit P(t) will have an !-limit Q ⇤ 2 X ⇤ , i.e. P(t n ) ! Q ⇤ for some increasing sequence of times {t n }, t n ! 1. Consequently, we will also have D KL ( Q ⇤ k P(t n )) ! 0, and since the function D KL ( Q ⇤ k P(t)) is itself decreasing, we obtain P(t) ! Q ⇤ , which proves that P(t) converges to a point. The convergence rate (12) can then be proven as in [11] (which essentially covers the diagonal case), the details being omitted for lack of space.\nThe above theorem ensures that the continuous exponential learning dynamics (9) always converge to a (global) minimum point of the program\u2019s objective (MP). Thus, specializing to the discretized version of the exponential learning algorithm and the MIMO sum rates (2) and (3), we obtain:\nTheorem 2. Let Y k (0), k 2 K, be a Hermitian initialization of the exponential learning algorithm with time-steps (t) such that P t (t) = 1 and P t 2 (t) < 1. Then:\n1) In static channels, users converge to a power allocation proﬁle Q which maximizes the sum rate (2).\n2) In ergodic (fast-fading) channels, users converge (a.s.) to the proﬁle Q which maximizes their ergodic sum rate (3).\nSketch of proof: The static case is an Euler approximation scheme with vanishing step size and is thus trivial to dispatch. As for the fast fading regime, recall that exponential learning can be viewed as a dynamical system on X, evolving according\nto the dynamics (11) of Proposition 2. In particular, if the chan- nel matrices H k = H k (t) are time-dependent (but uncorrelated over time), the eigenvalue dynamics (11a) can be written in discrete time as:\np k↵ (t + 1) = p k↵ (t) + (t)p k↵ (t) h V k ↵↵ (t) P 1 k P m k = 1 p k (t)V k (t) i , where the V k ↵↵ depend on t through both P(t) and H(t). Thus, if we set V k ↵ = E h V k ↵ i and R k ↵ = V k ↵ V k ↵ , we will have:\n(t) P 1 k P m k = 1 p k (t)R k (t) i , (15) and similarly for the eigenvector dynamics (11b). Then, by interchanging expectation with di↵erentiation, the resulting ma- trix V k = {V k ↵ } of the previous equation corresponds precisely to (minus) the gradient @ @ P ⇤ k of the ergodic sum rate . Hence, with H k stationary and ergodic, the general theory of stochastic approximation (see e.g. Thm. 2 in Chap. 2 of [20]) shows that (15) will track the mean dynamics:\n˙p k↵ = p k↵ ✓ V ↵↵ P 1 k P m k = 1 p k V k ◆ , \t (16) and similarly for the eigenvectors of P. Consequently, since (16) converges to the (globally) minimum point of by Theorem 1, the exponential algorithm will also converge there (a.s.).\nRemark. A diminishing step size ensures the convergence of the algorithm, but at the expense of convergence speed. With a bit more work however (which we reserve for the future due to space limitations), it can be shown that exponential learning with a constant step still converges, always in static channels and with high probability in ergodic ones (see also Fig. 1).\nIn this paper, we analyzed the power allocation problem in MIMO multiple access channels by means of an approach based on \u201cexponential learning\u201d, a distributed optimization method which applies to general nonlinear problems deﬁned over sets of positive-deﬁnite matrices (where traditional op- timization methods do not apply because of the form of the problem\u2019s constraints). Focusing on the case at hand, we showed that if the system\u2019s channels are static, then exponential learning converges to a power allocation proﬁle which attains the sum capacity of the channel exponentially fast; otherwise, if the channels ﬂuctuate stochastically over time following a stationary ergodic process, users converge to a power proﬁle which maximizes their ergodic sum rate instead. Importantly, the algorithm\u2019s speed can be controlled by tuning the users\u2019 learning rate; as a result, the algorithm converges within a few iterations, even when the number of users and/or antennas per user in the system is very large.\nSince the method of exponential learning is a rather general one, future applications include the interference channel frame- work of [5] where the method can be used to attain the Nash equilibria of the channel (in both static and ergodic channels). Moreover, it can also be shown that via the Gibbs transform (9b), exponential learning induces a Riemannian structure on the space of positive-deﬁnite matrices, and this structure can be further exploited to yield other learning algorithms, possibly with even faster convergence rates."},"refs":[{"authors":[{"name":"G. J. Foschini"},{"name":"M. J. Gans"}],"title":{"text":"On limits of wireless communications in a fading environment when using multiple antennas"}},{"authors":[{"name":"I. E. Telatar"}],"title":{"text":"Capacity of multi-antenna Gaussian channels"}},{"authors":[{"name":"R. S. Cheng"},{"name":"S. Verd´u"}],"title":{"text":"Gaussian multiaccess channels with ISI: capacity region and multiuser water-ﬁlling"}},{"authors":[{"name":"W. Yu"},{"name":"W. Rhee"},{"name":"S. Boyd"},{"name":"J. M. Cio "}],"title":{"text":"Iterative water-ﬁlling for Gaussian vector multiple-access channels"}},{"authors":[{"name":"G. Scutari"},{"name":"D. P. Palomar"},{"name":"S. Barbarossa"}],"title":{"text":"The MIMO iterative waterﬁlling algorithm"}},{"authors":[],"title":{"text":"Competitive design of multiuser MIMO systems based on game theory: a uniﬁed view"}},{"authors":[{"name":"P. Mertikopoulos"},{"name":"E. V. Belmega"},{"name":"A. L. Moustakas"},{"name":"S. Lasaulce"}],"title":{"text":"Dynamic power allocation games in parallel multiple access channels"}},{"authors":[{"name":"A. M. Tulino"},{"name":"S. Verd´u"}],"title":{"text":"Random matrix theory and wireless commu- nications"}},{"authors":[{"name":"F. Dupuy"},{"name":"P. Loubaton"}],"title":{"text":"On the capacity achieving covariance matrix for frequency selective MIMO channels using the asymptotic approach"}},{"authors":[{"name":"K. Tsuda"},{"name":"G. R¨atsch"},{"name":"M. K. Warmuth"}],"title":{"text":"Matrix exponentiated gradient updates for on-line Bregman projection"}},{"authors":[{"name":"P. Mertikopoulos"},{"name":"E. V. Belmega"},{"name":"A. L. Moustakas"},{"name":"S. Lasaulce"}],"title":{"text":"Distributed learning policies for power allication in multiple access channels"}},{"authors":[{"name":"A. Soysal"},{"name":"S. Ulukus"}],"title":{"text":"Optimality of beamforming in fading MIMO multiple access channels"}},{"authors":[{"name":"E. V. Belmega"},{"name":"S. Lasaulce"},{"name":"M. Debbah"},{"name":"A. Hjørungnes"}],"title":{"text":"Distributed power allocation policies in MIMO channels"}},{"authors":[{"name":"X. Lin"},{"name":"T.-M. Lok"}],"title":{"text":"Learning equilibrium play for stochastic parallel Gaussian interference channels"}},{"authors":[{"name":"P. D. Taylor"},{"name":"L. B. Jonker"}],"title":{"text":"Evolutionary stable strategies and game dynamics"}},{"authors":[{"name":"W. H. Sandhol"}],"title":{"text":"Population Games and Evolutionary Dynamics, ser"}},{"authors":[{"name":"P. Mertikopoulos"},{"name":"A. L. Moustakas"}],"title":{"text":"Learning in the presence of noise"}},{"authors":[{"name":"L. E. Blume"}],"title":{"text":"The statistical mechanics of strategic interaction"}},{"authors":[{"name":"J. A. Snyma"}],"title":{"text":"Practical mathematical optimization: an introduction to basic optimization theory and classical and new gradient-based algo- rithms, ser"}},{"authors":[{"name":"V. S. Borka"}],"title":{"text":"Stochastic approximation"}},{"authors":[{"name":"V. Vedral"}],"title":{"text":"The role of relative entropy in quantum information theory"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565427.pdf"},"links":[{"id":"1569566567","weight":3},{"id":"1569564843","weight":4},{"id":"1569566381","weight":2},{"id":"1569566527","weight":6},{"id":"1569566485","weight":6},{"id":"1569565383","weight":4},{"id":"1569565883","weight":4},{"id":"1569564889","weight":16},{"id":"1569565223","weight":10},{"id":"1569566725","weight":10},{"id":"1569565663","weight":5},{"id":"1569565377","weight":9},{"id":"1569566385","weight":9},{"id":"1569567049","weight":8},{"id":"1569564635","weight":5},{"id":"1569565867","weight":5},{"id":"1569566799","weight":7},{"id":"1569565067","weight":18},{"id":"1569559665","weight":9},{"id":"1569561021","weight":6},{"id":"1569564669","weight":3},{"id":"1569565691","weight":8},{"id":"1569566815","weight":5},{"id":"1569566875","weight":4},{"id":"1569559617","weight":3},{"id":"1569566981","weight":13},{"id":"1569566433","weight":3},{"id":"1569566321","weight":6},{"id":"1569566605","weight":3},{"id":"1569565489","weight":4},{"id":"1569566683","weight":6},{"id":"1569566855","weight":5},{"id":"1569560629","weight":3},{"id":"1569566869","weight":7},{"id":"1569565097","weight":9},{"id":"1569566227","weight":5},{"id":"1569566091","weight":8},{"id":"1569559259","weight":11},{"id":"1569566697","weight":4},{"id":"1569566597","weight":7},{"id":"1569565551","weight":3},{"id":"1569565711","weight":6},{"id":"1569566761","weight":5},{"id":"1569566943","weight":6},{"id":"1569565091","weight":5},{"id":"1569566591","weight":6},{"id":"1569556029","weight":5},{"id":"1569566571","weight":10},{"id":"1569552245","weight":6},{"id":"1569565607","weight":5},{"id":"1569559889","weight":2},{"id":"1569565495","weight":18},{"id":"1569559967","weight":5},{"id":"1569567045","weight":6},{"id":"1569565227","weight":9},{"id":"1569564481","weight":9},{"id":"1569560833","weight":6},{"id":"1569566415","weight":7},{"id":"1569564805","weight":7},{"id":"1569567005","weight":8},{"id":"1569566469","weight":5},{"id":"1569566081","weight":6},{"id":"1569565613","weight":3},{"id":"1569565355","weight":7},{"id":"1569564469","weight":3},{"id":"1569565931","weight":7},{"id":"1569566373","weight":13},{"id":"1569566647","weight":8},{"id":"1569551535","weight":15},{"id":"1569566765","weight":10},{"id":"1569564897","weight":4},{"id":"1569565775","weight":3},{"id":"1569565547","weight":4},{"id":"1569566871","weight":5},{"id":"1569566653","weight":3},{"id":"1569565461","weight":5},{"id":"1569564245","weight":3},{"id":"1569564731","weight":9},{"id":"1569565171","weight":3},{"id":"1569566207","weight":8},{"id":"1569564227","weight":7},{"id":"1569558325","weight":5},{"id":"1569557585","weight":6},{"id":"1569565837","weight":4},{"id":"1569566671","weight":6},{"id":"1569566303","weight":9},{"id":"1569566119","weight":10},{"id":"1569564233","weight":5},{"id":"1569566459","weight":9},{"id":"1569567535","weight":3},{"id":"1569563411","weight":9},{"id":"1569560427","weight":2},{"id":"1569564401","weight":5},{"id":"1569564849","weight":4},{"id":"1569559541","weight":6},{"id":"1569565317","weight":2},{"id":"1569566363","weight":4},{"id":"1569566319","weight":9},{"id":"1569565123","weight":5},{"id":"1569566941","weight":13},{"id":"1569566033","weight":15},{"id":"1569566739","weight":2},{"id":"1569555811","weight":5},{"id":"1569558459","weight":13},{"id":"1569565609","weight":2},{"id":"1569565291","weight":7},{"id":"1569564203","weight":6},{"id":"1569566821","weight":13},{"id":"1569556713","weight":8},{"id":"1569562685","weight":3},{"id":"1569566751","weight":8},{"id":"1569566467","weight":12},{"id":"1569565771","weight":4},{"id":"1569566157","weight":7},{"id":"1569560613","weight":3},{"id":"1569566903","weight":17},{"id":"1569566999","weight":5},{"id":"1569565859","weight":5},{"id":"1569565809","weight":3},{"id":"1569566843","weight":11},{"id":"1569566579","weight":5},{"id":"1569558483","weight":5},{"id":"1569566563","weight":6},{"id":"1569566089","weight":3},{"id":"1569564903","weight":3},{"id":"1569566173","weight":9},{"id":"1569559221","weight":6},{"id":"1569556091","weight":3},{"id":"1569565347","weight":8},{"id":"1569566925","weight":6},{"id":"1569564387","weight":5},{"id":"1569565455","weight":7},{"id":"1569566497","weight":6},{"id":"1569566795","weight":7},{"id":"1569566963","weight":6},{"id":"1569561679","weight":3},{"id":"1569566709","weight":5},{"id":"1569564989","weight":5},{"id":"1569560721","weight":4},{"id":"1569566015","weight":3},{"id":"1569566523","weight":4},{"id":"1569565897","weight":11},{"id":"1569551763","weight":5},{"id":"1569565953","weight":5},{"id":"1569566895","weight":3},{"id":"1569566889","weight":4},{"id":"1569565709","weight":3},{"id":"1569566749","weight":3},{"id":"1569566269","weight":8},{"id":"1569564189","weight":11},{"id":"1569564195","weight":2},{"id":"1569561513","weight":15},{"id":"1569566985","weight":7},{"id":"1569564613","weight":2},{"id":"1569565369","weight":4},{"id":"1569567009","weight":14},{"id":"1569566865","weight":8},{"id":"1569565321","weight":4},{"id":"1569558785","weight":8},{"id":"1569564647","weight":15},{"id":"1569566095","weight":4},{"id":"1569566193","weight":6},{"id":"1569564271","weight":11},{"id":"1569564337","weight":4},{"id":"1569565907","weight":8},{"id":"1569566343","weight":3},{"id":"1569564311","weight":3},{"id":"1569565803","weight":4},{"id":"1569565785","weight":5},{"id":"1569566239","weight":6},{"id":"1569566167","weight":3},{"id":"1569566679","weight":9},{"id":"1569565989","weight":8},{"id":"1569566575","weight":9},{"id":"1569563981","weight":6},{"id":"1569561085","weight":5},{"id":"1569566419","weight":5},{"id":"1569566617","weight":3},{"id":"1569559565","weight":13},{"id":"1569566905","weight":9},{"id":"1569566733","weight":5},{"id":"1569566753","weight":9},{"id":"1569566311","weight":3},{"id":"1569563307","weight":13},{"id":"1569566063","weight":8},{"id":"1569558681","weight":10},{"id":"1569555999","weight":2},{"id":"1569566759","weight":16},{"id":"1569565589","weight":3},{"id":"1569559195","weight":5},{"id":"1569566149","weight":14},{"id":"1569559995","weight":6},{"id":"1569566657","weight":14},{"id":"1569558859","weight":3},{"id":"1569565199","weight":3},{"id":"1569565213","weight":5},{"id":"1569565365","weight":3},{"id":"1569566643","weight":4},{"id":"1569566511","weight":19},{"id":"1569566719","weight":4},{"id":"1569566991","weight":3},{"id":"1569565841","weight":6},{"id":"1569566975","weight":3},{"id":"1569566369","weight":3},{"id":"1569566531","weight":11},{"id":"1569567665","weight":7},{"id":"1569561143","weight":5},{"id":"1569566581","weight":5},{"id":"1569565833","weight":4},{"id":"1569566489","weight":7},{"id":"1569564611","weight":14},{"id":"1569565535","weight":14},{"id":"1569562867","weight":9},{"id":"1569566395","weight":4},{"id":"1569565667","weight":3},{"id":"1569561795","weight":3},{"id":"1569566845","weight":3},{"id":"1569566325","weight":7},{"id":"1569566423","weight":4},{"id":"1569565257","weight":2},{"id":"1569564795","weight":10},{"id":"1569567015","weight":13},{"id":"1569559805","weight":4},{"id":"1569566437","weight":6},{"id":"1569566811","weight":13},{"id":"1569566851","weight":5},{"id":"1569558901","weight":2},{"id":"1569565735","weight":5},{"id":"1569553909","weight":2},{"id":"1569559111","weight":6},{"id":"1569566687","weight":6},{"id":"1569564881","weight":3},{"id":"1569566939","weight":10},{"id":"1569553537","weight":8},{"id":"1569566403","weight":6},{"id":"1569565839","weight":7},{"id":"1569565915","weight":12},{"id":"1569552251","weight":6},{"id":"1569566139","weight":2},{"id":"1569553519","weight":3},{"id":"1569567051","weight":5},{"id":"1569566885","weight":4},{"id":"1569564441","weight":16},{"id":"1569566231","weight":6},{"id":"1569564209","weight":2},{"id":"1569554689","weight":3},{"id":"1569566513","weight":13},{"id":"1569566425","weight":4},{"id":"1569554881","weight":5},{"id":"1569554971","weight":24},{"id":"1569565501","weight":5},{"id":"1569566899","weight":7},{"id":"1569566445","weight":4},{"id":"1569566209","weight":18},{"id":"1569562821","weight":2},{"id":"1569566649","weight":10},{"id":"1569566791","weight":3},{"id":"1569565559","weight":5},{"id":"1569566371","weight":10},{"id":"1569565655","weight":6},{"id":"1569566909","weight":12},{"id":"1569566127","weight":7},{"id":"1569565151","weight":3},{"id":"1569558985","weight":9},{"id":"1569563763","weight":6},{"id":"1569565087","weight":2},{"id":"1569566473","weight":11},{"id":"1569564857","weight":6},{"id":"1569564333","weight":10},{"id":"1569566913","weight":9},{"id":"1569566809","weight":9},{"id":"1569566629","weight":8},{"id":"1569566257","weight":9},{"id":"1569565033","weight":8},{"id":"1569566447","weight":7},{"id":"1569566357","weight":7},{"id":"1569565817","weight":3},{"id":"1569565847","weight":9},{"id":"1569564353","weight":5},{"id":"1569563897","weight":5},{"id":"1569557083","weight":2},{"id":"1569565887","weight":4},{"id":"1569565929","weight":5},{"id":"1569566141","weight":11},{"id":"1569553591","weight":2},{"id":"1569565055","weight":9},{"id":"1569564677","weight":2},{"id":"1569563231","weight":5},{"id":"1569565633","weight":6},{"id":"1569566661","weight":3},{"id":"1569565279","weight":5},{"id":"1569555879","weight":14},{"id":"1569565521","weight":2},{"id":"1569566115","weight":6},{"id":"1569565219","weight":6},{"id":"1569558509","weight":9},{"id":"1569554759","weight":3},{"id":"1569564851","weight":6},{"id":"1569565595","weight":9},{"id":"1569565185","weight":6},{"id":"1569566773","weight":5},{"id":"1569566037","weight":2},{"id":"1569564985","weight":4},{"id":"1569565095","weight":8},{"id":"1569566223","weight":4},{"id":"1569558401","weight":4},{"id":"1569566553","weight":8},{"id":"1569564969","weight":8},{"id":"1569566593","weight":5},{"id":"1569566043","weight":4},{"id":"1569565029","weight":7},{"id":"1569565357","weight":3},{"id":"1569561245","weight":13},{"id":"1569566505","weight":8},{"id":"1569565393","weight":5},{"id":"1569565933","weight":7},{"id":"1569562207","weight":9},{"id":"1569565705","weight":2},{"id":"1569566191","weight":12},{"id":"1569567033","weight":11},{"id":"1569565527","weight":14},{"id":"1569566853","weight":11},{"id":"1569566603","weight":6},{"id":"1569567029","weight":3},{"id":"1569565363","weight":8},{"id":"1569566159","weight":5},{"id":"1569566695","weight":4},{"id":"1569566051","weight":7},{"id":"1569561379","weight":7},{"id":"1569565909","weight":10},{"id":"1569561123","weight":5},{"id":"1569555787","weight":4},{"id":"1569565467","weight":5},{"id":"1569566655","weight":13},{"id":"1569566673","weight":4},{"id":"1569567235","weight":3},{"id":"1569565441","weight":6},{"id":"1569565739","weight":5},{"id":"1569565311","weight":5},{"id":"1569566233","weight":7},{"id":"1569566667","weight":6},{"id":"1569566297","weight":2},{"id":"1569566893","weight":9},{"id":"1569566317","weight":6},{"id":"1569564097","weight":12},{"id":"1569560997","weight":11},{"id":"1569563845","weight":4},{"id":"1569566407","weight":6},{"id":"1569560349","weight":19},{"id":"1569566501","weight":9},{"id":"1569565741","weight":18},{"id":"1569566275","weight":5},{"id":"1569566481","weight":7},{"id":"1569565545","weight":8},{"id":"1569566857","weight":14},{"id":"1569565961","weight":4},{"id":"1569566387","weight":3},{"id":"1569566245","weight":8},{"id":"1569560503","weight":5},{"id":"1569565463","weight":5},{"id":"1569564339","weight":3},{"id":"1569566219","weight":6},{"id":"1569565439","weight":5},{"id":"1569566229","weight":7},{"id":"1569566949","weight":7},{"id":"1569566133","weight":4},{"id":"1569562551","weight":2},{"id":"1569566155","weight":2},{"id":"1569563395","weight":3},{"id":"1569566901","weight":4},{"id":"1569551347","weight":5},{"id":"1569565415","weight":7},{"id":"1569555367","weight":3},{"id":"1569561623","weight":11},{"id":"1569566383","weight":9},{"id":"1569564485","weight":9},{"id":"1569565155","weight":2},{"id":"1569566631","weight":6},{"id":"1569565571","weight":5},{"id":"1569565885","weight":10},{"id":"1569566177","weight":10},{"id":"1569565493","weight":3},{"id":"1569557633","weight":5},{"id":"1569564411","weight":4},{"id":"1569566805","weight":3},{"id":"1569559199","weight":6},{"id":"1569566929","weight":7},{"id":"1569566293","weight":14},{"id":"1569565665","weight":5},{"id":"1569566831","weight":7},{"id":"1569565549","weight":7},{"id":"1569565523","weight":14},{"id":"1569565611","weight":10},{"id":"1569557715","weight":8},{"id":"1569564175","weight":6},{"id":"1569566983","weight":12},{"id":"1569566779","weight":3},{"id":"1569566097","weight":10},{"id":"1569566479","weight":8},{"id":"1569556361","weight":2},{"id":"1569566431","weight":5},{"id":"1569565397","weight":10},{"id":"1569566873","weight":6},{"id":"1569565765","weight":5},{"id":"1569565925","weight":4},{"id":"1569565435","weight":9},{"id":"1569557275","weight":4},{"id":"1569565263","weight":2},{"id":"1569566129","weight":3},{"id":"1569566261","weight":5},{"id":"1569565215","weight":4},{"id":"1569565093","weight":2},{"id":"1569565385","weight":5},{"id":"1569565575","weight":5},{"id":"1569565919","weight":11},{"id":"1569565181","weight":6},{"id":"1569566711","weight":10},{"id":"1569565241","weight":4},{"id":"1569566927","weight":12},{"id":"1569565661","weight":3},{"id":"1569565865","weight":3},{"id":"1569566887","weight":5},{"id":"1569565273","weight":7},{"id":"1569566267","weight":4},{"id":"1569564131","weight":11},{"id":"1569552037","weight":6},{"id":"1569564919","weight":5},{"id":"1569565511","weight":6},{"id":"1569566737","weight":7},{"id":"1569566429","weight":5},{"id":"1569561221","weight":4},{"id":"1569564595","weight":2},{"id":"1569566917","weight":18},{"id":"1569566035","weight":4},{"id":"1569566253","weight":3},{"id":"1569565353","weight":4},{"id":"1569564683","weight":6},{"id":"1569564305","weight":8},{"id":"1569564283","weight":4},{"id":"1569564291","weight":2},{"id":"1569566691","weight":4},{"id":"1569565421","weight":11},{"id":"1569566547","weight":3},{"id":"1569566651","weight":6},{"id":"1569565177","weight":2},{"id":"1569566823","weight":8},{"id":"1569566595","weight":18},{"id":"1569566677","weight":12},{"id":"1569565349","weight":2},{"id":"1569552025","weight":3},{"id":"1569566137","weight":8},{"id":"1569565013","weight":3},{"id":"1569565829","weight":2},{"id":"1569566283","weight":4},{"id":"1569565645","weight":5},{"id":"1569566529","weight":4},{"id":"1569565375","weight":7},{"id":"1569566715","weight":4},{"id":"1569565237","weight":4},{"id":"1569566639","weight":6},{"id":"1569566819","weight":4},{"id":"1569565041","weight":7},{"id":"1569564703","weight":9},{"id":"1569566713","weight":6},{"id":"1569565541","weight":2},{"id":"1569565597","weight":3},{"id":"1569566813","weight":7},{"id":"1569565293","weight":10},{"id":"1569566771","weight":16},{"id":"1569564649","weight":2},{"id":"1569564201","weight":4},{"id":"1569562277","weight":5},{"id":"1569566641","weight":5},{"id":"1569565425","weight":5},{"id":"1569559035","weight":3},{"id":"1569566977","weight":3},{"id":"1569564247","weight":5},{"id":"1569564437","weight":8},{"id":"1569566533","weight":3},{"id":"1569563975","weight":2},{"id":"1569551905","weight":4},{"id":"1569564861","weight":7},{"id":"1569565457","weight":8},{"id":"1569564787","weight":10},{"id":"1569566487","weight":9},{"id":"1569565529","weight":12},{"id":"1569556759","weight":7},{"id":"1569566619","weight":11},{"id":"1569565271","weight":3},{"id":"1569561185","weight":6},{"id":"1569566075","weight":2},{"id":"1569566397","weight":3},{"id":"1569566301","weight":3},{"id":"1569558779","weight":6},{"id":"1569565669","weight":6},{"id":"1569565233","weight":3},{"id":"1569563721","weight":7},{"id":"1569566001","weight":4},{"id":"1569565593","weight":7},{"id":"1569560235","weight":5},{"id":"1569566817","weight":3},{"id":"1569564157","weight":9},{"id":"1569566389","weight":12},{"id":"1569566435","weight":5},{"id":"1569567483","weight":5},{"id":"1569566911","weight":6},{"id":"1569564923","weight":4},{"id":"1569565367","weight":5},{"id":"1569566299","weight":6},{"id":"1569564281","weight":3},{"id":"1569565039","weight":4},{"id":"1569564769","weight":8},{"id":"1569565769","weight":3},{"id":"1569566171","weight":7},{"id":"1569566601","weight":2},{"id":"1569565805","weight":8},{"id":"1569561713","weight":5},{"id":"1569566933","weight":5},{"id":"1569563919","weight":9},{"id":"1569566577","weight":4},{"id":"1569557851","weight":5},{"id":"1569565389","weight":15},{"id":"1569559919","weight":27},{"id":"1569565861","weight":2},{"id":"1569566147","weight":11},{"id":"1569565537","weight":7},{"id":"1569559523","weight":2},{"id":"1569566057","weight":3},{"id":"1569562367","weight":6},{"id":"1569560785","weight":4},{"id":"1569565561","weight":3},{"id":"1569560213","weight":3},{"id":"1569566457","weight":5},{"id":"1569555891","weight":3},{"id":"1569566847","weight":30},{"id":"1569565997","weight":5},{"id":"1569563425","weight":14},{"id":"1569565035","weight":7},{"id":"1569559597","weight":13},{"id":"1569564961","weight":7},{"id":"1569559251","weight":7},{"id":"1569565089","weight":5},{"id":"1569567013","weight":7},{"id":"1569566583","weight":7},{"id":"1569561861","weight":6},{"id":"1569565337","weight":12},{"id":"1569564253","weight":2},{"id":"1569565737","weight":4},{"id":"1569560459","weight":3},{"id":"1569566807","weight":5},{"id":"1569564463","weight":6},{"id":"1569565853","weight":9},{"id":"1569550425","weight":5},{"id":"1569566273","weight":8},{"id":"1569564123","weight":9},{"id":"1569566341","weight":5},{"id":"1569565889","weight":6},{"id":"1569566635","weight":6},{"id":"1569566611","weight":7},{"id":"1569551539","weight":4},{"id":"1569564505","weight":4},{"id":"1569565165","weight":3},{"id":"1569565565","weight":5},{"id":"1569565635","weight":6},{"id":"1569561397","weight":8},{"id":"1569565731","weight":3},{"id":"1569556327","weight":4},{"id":"1569566797","weight":7},{"id":"1569566125","weight":5},{"id":"1569566413","weight":5},{"id":"1569565707","weight":8},{"id":"1569565113","weight":3},{"id":"1569566375","weight":8},{"id":"1569565143","weight":2},{"id":"1569564257","weight":6},{"id":"1569565583","weight":4},{"id":"1569566555","weight":3},{"id":"1569564931","weight":2},{"id":"1569565373","weight":3},{"id":"1569564141","weight":2},{"id":"1569566973","weight":19},{"id":"1569561579","weight":8},{"id":"1569566449","weight":14},{"id":"1569566987","weight":11},{"id":"1569565031","weight":6},{"id":"1569564755","weight":4},{"id":"1569551541","weight":10},{"id":"1569565619","weight":2},{"id":"1569566839","weight":8},{"id":"1569551751","weight":5},{"id":"1569558697","weight":2},{"id":"1569565139","weight":8},{"id":"1569565895","weight":3},{"id":"1569566663","weight":10},{"id":"1569564419","weight":6},{"id":"1569565579","weight":3},{"id":"1569566067","weight":17},{"id":"1569566825","weight":10},{"id":"1569566615","weight":2},{"id":"1569566241","weight":5},{"id":"1569564807","weight":4},{"id":"1569566609","weight":6},{"id":"1569563007","weight":7},{"id":"1569566113","weight":6},{"id":"1569566443","weight":5},{"id":"1569566727","weight":10},{"id":"1569565315","weight":7},{"id":"1569565515","weight":2},{"id":"1569566417","weight":2},{"id":"1569560581","weight":4},{"id":"1569559233","weight":3}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S17.T3.2","endtime":"15:40","authors":"Panayotis Mertikopoulos, Elena Veronica Belmega, Aris Moustakas","date":"1341588000000","papertitle":"Matrix Exponential Learning: Distributed Optimization in MIMO systems","starttime":"15:20","session":"S17.T3: MIMO Equalization, Diversity and Optimization","room":"Stratton S. de P. Rico (202)","paperid":"1569565427"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"0","spectral43":"15","spectral28":"5","spectral32":"5","spectral14":"13","spectral20":"7","spectral9":"7","spectral25":"7","spectral42":"13","spectral3":"0","spectral47":"6","spectral17":"3","louvain":"386","spectral36":"24","spectral39":"0","spectral10":"8","spectral15":"5","spectral33":"31","spectral5":"1","spectral21":"12","spectral44":"36","spectral26":"18","spectral40":"37","spectral8":"4","spectral11":"1","spectral4":"3","spectral37":"30","spectral48":"26","spectral22":"14","spectral23":"3","spectral12":"7","spectral50":"18","spectral19":"11","spectral34":"27","spectral45":"4","spectral7":"5","spectral49":"24","spectral38":"22","spectral24":"19","spectral13":"1","spectral31":"8","spectral29":"24","spectral35":"8","spectral30":"8","spectral41":"19","spectral27":"8","spectral18":"10","spectral46":"26","spectral2":"1","spectral16":"11"}}

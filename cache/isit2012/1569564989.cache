{"id":"1569564989","paper":{"title":{"text":"Universal communication over unknown vector channels"},"authors":[{"name":"Yuval Lomnitz"},{"name":"Meir Feder"}],"abstr":{"text":"Abstract\u2014Consider communication over a channel whose probabilistic model is completely unknown vector-wise and is not assumed to be stationary. Communication over such channels is challenging because knowing the past does not indicate anything about the future. The existence of reliable feedback and common randomness is assumed. In a previous paper it was shown that the Shannon capacity cannot be attained, in general, if the channel is not known. An alternative notion of \u201ccapacity\u201d was deﬁned, as the maximum rate of reliable communication by any block-coding system used over consecutive blocks. This rate was shown to be achievable for the modulo-additive channel with an individual, unknown noise sequence, and not achievable for some channels with memory. In this paper this \u201ccapacity\u201d is shown to be achievable for general channel models possibly including memory, as long as this memory fades with time. In other words, there exists a system with feedback and common randomness that, without knowledge of the channel, asymptotically performs as well as any block-coding system, which may be designed knowing the channel. For non-fading memory channels a weaker type of \u201ccapacity\u201d is shown to be achievable."},"body":{"text":"Consider communication over a channel which has a general probabilistic structure. In other words, the inﬁnite length out- put Y ∞ 1 depends on the inﬁnite length input X ∞ 1 through an arbitrary vector-wise probability function P Y|X (Y n 1 |X ∞ 1 ), n = 1, 2, . . ., which is unknown to the transmitter and the receiver. Particular cases of such a channel include any unknown functional relation between the input and output sequences, as well as arbitrarily varying channels, compound channels [1] and channels with an individual state sequence [2][3][4][5]. In the current paper, an attempt is made to keep the model as general as possible, i.e. minimize any assumptions on P Y|X , except for causality. Without feedback, communication over such a channel is limited, as the communication rate, and the codebook would have to be selected in advance. Therefore, the existence of a reliable feedback link is assumed.\nTwo traditional models, which relate to particular cases of the current problem, are the arbitrarily varying channel (AVC) model [1] and the compound ﬁnite state channel (compound- FSC) model [6]. In the AVC model, the channel is assumed to be controlled by a sequence of states which is arbitrary and unknown to the transmitter and the receiver. In the com- pound channel model, the channel is assumed to be arbitrarily selected from a family of possible channels. In both models, the capacity is the maximum rate of reliable communication that can be guaranteed. Both models do not give a satisfying answer to the current problem: the fundamental reason is\nthat these models focus on capacity, i.e. before knowing the channel, one is required to ﬁnd a rate of reliable transmission which can be guaranteed a-priori. Clearly, if the channel is completely general, the compound/AVC capacity is zero, as it is possible, for example, that a channel with zero capacity will be selected. In both models mentioned, constraints on the family of channels, or on the possible state sequences need to be deﬁned, and these constraints do not seem suitable for natural channels. In addition to this fundamental gap, the models considered under the AVC and compound-FSC frameworks are quite limited, in a way that does not seem to capture the possible complexity of an unknown natural channel. For example, most papers on AVC consider only memoryless channels, and the compound-FSC is stationary.\nUsing feedback, the communication rate can be adapted, so that one does not have to commit to a communication rate a-priori. Several works by us and other authors considered the gains from such adaptation [2][3][7][5]. The ﬁrst question to ask is, how the target communication rate should be deﬁned? The sought rate R(P Y|X ) can be a function of the channel, but should be universally attainable without prior knowledge of the channel, and should have an operational meaning.\nIn a previous paper [4], the problem of determining such a communication rate was addressed. In general, the Shannon capacity [8] of the channel, C(P Y|X ) is not attainable uni- versally with feedback, when the channel is unknown. The problem of determining a universally-achievable rate is similar to the source coding problem of setting a compression rate for an individual sequence. Following the spirit of the \u201cﬁnite state compressibility\u201d of Lempel and Ziv [9], we proposed to set as a target, the best rate that can be reliably attained by a system employing ﬁnite block encoding (successively) over the inﬁnite channel. The supremum of these rates is termed the Iterative-Finite-Block (IFB) capacity and denoted C IFB (P Y|X ). When the channel is stationary and ergodic, then the IFB capacity equals the Shannon capacity. As in the universal source coding problem, due to the richness of the model family, there is a large gap between the performance that can be attained universally and the performance that can be attained without constraints, when knowing the speciﬁc model (the Shannon capacity) and this gap requires limiting the abilities of the reference system. This motivates considering the IFB capacity as a goal.\nIt is easy to see that the IFB capacity is not universally achievable for completely general models. The counter ex- ample in [4] is of a family consisting of only two binary\nchannels, termed \u201cpassword\u201d channels, where the ﬁrst input bit X 1 determines whether the channel becomes \u201cgood\u201d or \u201cbad\u201d for eternity, and where the values of X 1 matching each state are opposite in the two channels. There is no way for the universal system to correctly guess X 1 with high probability. The conclusion is that the IFB capacity is not universally attainable for some channels with inﬁnite memory. On the other hand, the IFB capacity was shown to be asymptotically attainable for the class of modulo-additive channels with an individual, unknown noise sequence. In this case, it was further shown, that the IFB capacity is related to the ﬁnite state compressibility of the noise sequence, and the scheme attaining it uses the Lempel-Ziv source encoder [9] to generate decoding metrics. The result in [4] relies crucially on two properties of the modulo additive channel:\n1) The channel is memoryless with respect to the input x i (i.e. current behavior is not affected by previous values of the input).\n2) The capacity achieving input distribution is ﬁxed (uni- form i.i.d.) regardless of the noise sequence.\nTo avoid these assumptions it is required to address the memory of the channel and the setting of the communication prior. The second limitation, raises the question, how the input distribution should be adapted, if the channel changes arbitrarily over time? This question was the center of [5], where universal prediction methods were used to set the com- munication prior. The focus of that paper is on channels which are memoryless in the input, and therefore can be deﬁned by an unknown sequence of memoryless channels P Y|X (Y n 1 |X n 1 ) =\nW i (Y i |X i ). It is shown there that the capacity of the time-averaged channel W (y|x) = 1 n n i=1 W i (y|x) can be universally attained using feedback and common randomness without knowing {W i }, and that this value is the maximum rate that can be achieved universally and does not depend on the order of the channels in the sequence. The notion of universality used in [5] is different and weaker than the IFB universality, since the rate is only compared with other rates that could have been universally attained.\nIn the current paper, ideas from [4] and [5] are combined to generalize the previous results. It is shown that the IFB ca- pacity is asymptotically universally attainable for any channel with a fading memory, i.e. where the effect of the channel history on the far future is vanishing. In this sense, the two assumptions used in the previous paper [4] are avoided as much as possible, and the assumptions made on the channel are signiﬁcantly minimized. The fading memory condition includes as particular cases memoryless arbitrarily varying channels as well as compound indecomposable ﬁnite state channels [10]. Here, an example is given of a class of ﬁnite state channels where the state is a non-homogenous Markov chain, which satisfy the fading memory condition.\nConsidering channels where memory of the past is not necessarily fading, it may still be possible to communicate universally over the channel, if it is not maliciously designed like the password channel described above. The advantage of the IFB reference class which enables it to win over any uni- versal system is its ability to determine such a codebook that\nwill not only enable reliable transmission, but will also keep the channel in a favorable state, whereas the universal system does not know the long term effects of certain input symbols or distributions. An alternative formulation is proposed, where the reference system is crippled, so that it cannot enjoy the ability to shape the past: the encoder and decoder operate over ﬁnite blocks, however the error probability is required to be small in the worst case channel state (history) prior to each block, and average over blocks. This models a situation where the reference encoder and decoder are \u201cthrown\u201d each time into a different location in time, where the past state might have been arbitrary. It is not required to have good performance in each of these events, but only on average. This alternative reference system is termed \u201carbitrary-ﬁnite-block\u201d (AFB) and the same universal system is shown to asymptotically approach the respective AFB capacity, without requiring that the channel memory is fading. This reference class is less natural than the IFB, yet it enables releasing constraints on the channel.\nNote that there are several alternative deﬁnitions of a limited reference class for the universal communication problem [4]. Most notably, Misra and Weissman [11] generalized the main results of [4] to ﬁnite-state communication systems with feedback. For the sake of simplicity the current paper focuses on the basic model of reference systems using block coding. Although the current result is purely theoretical, it supplies motivation for using competitive universality in communica- tion.\nVectors are denoted by boldface letters. Sub-vectors are de- ﬁned by superscripts and subscripts: x i j [x j , x j+1 , . . . , x i ]. x i j equals the empty string if i < j. The subscript is sometimes removed when it equals 1, i.e. x i \t x i 1 . For a vector x, x [k] i \t x (i−1)k+k (i−1)k+1 denotes the i-th block of length k in the vector. For brevity, vectors with similar ranges are sometimes joined together, for example, the notation (xy) k 1 is used instead of x k 1 y k 1 . Exponents and logs are base 2. Random variables are distinguished from their sample values by capital letters.\nLet x and y be inﬁnite sequences denoting the input and the output respectively, where each letter is chosen from the alphabets X , Y respectively, x i ∈ X , y i ∈ Y. Throughout the current paper the input and output alphabets are assumed to be ﬁnite. A channel P Y|X is deﬁned through the probabilistic relations P Y|X (y n |x ∞ ) = Pr(Y n = y n |X ∞ = x ∞ ) for n = 1, 2, ...∞. A ﬁnite length output sequence is considered in order to make the probability well deﬁned.\nDeﬁnition 1. The channel deﬁned by Pr(Y n 1 |X ∞ 1 ) is termed causal if for all n:\nAll the deﬁnitions below (including IFB/AFB capacity) pertain to causal channels. This characterization of a causal\nchannel is similar to the deﬁnition used by Han and Verd´u [8] (and references therein).\nDeﬁnition 2. The channel is termed a fading memory channel if for any h > 0 there exists L and a sequence of causal conditional vector distribution functions {P n (·|·)}, such that for all n and m ≥ n:\nwhere the L 1 norm is calculated over Y m n , and deﬁned by g(Y|·) 1 \t y |g(y|·)|\nThe difference between the terms on the LHS of (2) is that P n does not include (XY) n−L−1 1 \t (see Fig.1), and thus the fading memory condition asserts that the dependence of the conditional distribution of future outputs, on the channel state at the far past, decays.\nThe fading memory condition does not imply stationarity or ergodicity. The memoryless arbitrary varying channel model considered in [5] is fading memory, and so are the FSC [10, §4.6] or compound-FSC models [6], if the underlying FSC is indecomposable. An example of a non-homogeneous ﬁnite state channel with fading memory is presented in the full paper.\nDeﬁnition 3 (Reference encoder and decoder). A ﬁnite length encoder E with block length k and a rate R is a mapping E : {1, . . . , M } → X k from a set of M ≥ exp(kR) messages to a set of input sequences X k . A respective ﬁnite length decoder D is a mapping D : Y k → {1, . . . , M } from the set of output sequences to the set of messages.\nDeﬁnition 4 (IFB error probability). The average error prob- ability in iterative mapping of the k length encoder E and decoder D to b blocks over the channel P Y|X is deﬁned as follows: b messages m 1 , . . . , m b are chosen as i.i.d. uni- formly distributed random variables m i ∼ U {1, . . . , M }, i = 1, . . . , b. The channel input is set to X [k] i = E(m i ), i = 1, . . . , b, and the decoded message is ˆ m i = D(Y [k] i ) where Y is the channel output. The iterative mapping is illustrated in Fig.2. The average error probability is P e = 1 b b i=1 Pr( ˆ m i = m i ).\nDeﬁnition 5 (AFB error probability). The average error probability in arbitrary mapping of the k length encoder E\nand decoder D to b blocks over the channel P Y|X is deﬁned as P e = 1 b b i=1 P e (i). P e (i) is the worst case per-block error probability, deﬁned as:\nDeﬁnition 6 (IFB/AFB achievability). A rate R is iterated- ﬁnite-block (IFB) / arbitrary-ﬁnite-block (AFB) achievable over the channel P Y|X , if for any > 0 there exist k, b ∗ > 0 such that for any b > b ∗ there exist an encoder E and a decoder D with block length k and rate R for which the average error probability in iterative / arbitrary mapping (resp.) of E, D to b blocks is at most .\nDeﬁnition 7 (IFB/AFB capacity). The IFB/AFB capacity of the channel P Y|X is the supremum of the set of IFB/AFB achievable rates, and is denoted C IFB /C AFB (resp.).\nNotice that by deﬁnition, the AFB error probability is at least as large as the IFB error probability, and as a result, the AFB capacity is smaller than, or equal to the IFB capacity.\nIn the following, the properties of the adaptive system with feedback, and IFB/AFB-universality are deﬁned. A random- ized rate-adaptive transmitter and receiver for block length n with feedback are deﬁned as follows: the transmitter is presented with a message expressed by an inﬁnite bit se- quence, and following the reception of n symbols, the decoder announces the achieved rate R, and decodes the ﬁrst nR\nbits. An error means any of these bits differs from the bits of the original message sequence. Both encoder and decoder have access to a random variable S (the common randomness) distributed over a chosen alphabet, and a causal feedback link allows the transmitted symbols to depend on previously sent feedback from the receiver. See formal deﬁnitions in our previous paper [7].\nThe following deﬁnition states formally the notion of IFB/AFB-universality for rate adaptive systems:\nDeﬁnition 8 (IFB/AFB universality). With respect to a set of channels {P (θ) Y|X }, θ ∈ Θ (not necessarily ﬁnite or countable), a rate-adaptive communication system (possibly using feedback and common randomness) is called IFB/AFB universal if for\nevery channel in the family and any , δ > 0 there is n large enough such that when the system is operated over n channel uses, then in probability 1− , the message is correctly decoded and the rate is at least C IFB (P Y|X )−δ or C AFB (P Y|X )−δ (resp.).\nTheorem 1. For any \t > 0 there exists a sequence of adaptive rate systems over a block of size N with feedback and common randomness, for growing values of N , such that with a probability of at least 1 − the message is received correctly with a rate of:\n0 for any causal fading memory channel. Furthermore, this can be attained with any positive rate of the feedback link.\nThis implies that the system is IFB universal over the set of causal fading memory channels, and AFB universal over the set of causal channels, according to Deﬁnition 8. While the system does not depend on the channel, the convergence rate of δ IFB N , δ AFB N does.\nIn [5], a communication scheme for adapting the prior over an arbitrarily varying channel which is memoryless in the input was described. Combining Theorem 3 and Lemma 9 of [5] yields:\nLemma 1. [Lemma 9 of [5]] For every ˜, ˜ δ > 0 there exists n ∗ and a constant c ∆ , such that for any n ≥ n ∗ there is an adaptive rate system with feedback and common randomness, such that for any channel Pr(Y n 1 |X n 1 ):\n2) The rate satisﬁes R ≥ C(W SUBJ ) − ˜ ∆ C with probability at least 1 − ˜ δ\n˜ ∆ C = c ∆ · ln 2 (n) n\nThe universal communication scheme for attaining the claims of Theorem 1 is as follows. The inﬁnite time is divided into epochs of increasing length, numbered m = 1, 2, . . .. In the ﬁrst epoch, the scheme of [5] is operated over N 1 symbols. In the second epoch, the channel inputs and outputs are joined into pairs, i.e. super-symbols of dimension 2, and the scheme is operated over N 2 such super-symbols. In epoch m, the scheme is operated over N m super-symbols of dimension 2 m−1 (Fig.3). Since all N m are ﬁnite, the dimension of the super-symbols used grows indeﬁnitely with time.\nThe parameters of the scheme are chosen as follows. Let > 0 the chosen error probability. Choose any ∆ C > 0, and\nlet m = 1 2 · 2 −m . The length of the m-th epoch, N m , is chosen such that:\n1) It is equal to or larger than the value of n ∗ given by Lemma 1 for the parameters ˜ = ˜ δ = m .\n2) The value of ˜ ∆ C given by Lemma 1 for n = N m is not larger than ∆ C (the chosen value).\n3) If the end of the next epoch N m+1 would occur beyond symbol N , then the current epoch N m is extended to reach symbol N .\nThe second requirement makes sure that there is no more than a constant loss from capacity per epoch, while the dimension of the super-symbol of each epoch is growing, and therefore the loss per symbol tends to 0. The values of ˜ δ and ˜ chosen per epoch, guarantee that the overall probability of error is not larger than ∞ m=1 m = 1 2 and similarly the overall probability that at any epoch the rate falls below the rate declared in the lemma is at most 1 2 . This way the overall probability of having an error or falling below the guaranteed rate is at most . Note that the epoch durations N m are ﬁxed and do not depend on the message or received signal.\nThe scheme does not need to know the IFB/AFB block length, rate and error probability, and the exact relation be- tween L, h given by the fading memory condition. Its only parameters are the input and output alphabets, the number of symbols N , and .\nThe claim of Theorem 1, that any positive feedback rate is sufﬁcient, simply follows from the fact [5] that this is true for the scheme of Lemma 1.\nFollowing is the outline of the proof. The value Pr(Y i = y|X i = x, X i−1 , Y i−1 ) appearing in the deﬁnition of W SUBJ (5) is the probability of a certain output symbol to appear given a certain input symbol at time i, where the history of the chan- nel (XY) i−1 attains the speciﬁc value that occurred during the universal system\u2019s operation. Pr(Y i = y|X i = x, X i−1 , Y i−1 ) is a random variable and depends both on the channel and on the universal communication system behavior. As a result, the rate W SUBJ guaranteed by Lemma 1 is also a random variable and depends on the joint input-output distribution induced by the universal communication scheme.\nThe baseline for comparison with the reference system is the \u201cpessimistic average channel capacity\u201d, obtained by replacing the history (XY) i−1 by an arbitrary state, and taking the worst-case state sequence (worst case history), i.e. the one\nthat yields the minimum capacity. The rate attained by the universal system (for a particular state sequence) would be at least as large. For super-symbols, the averaged channel relates to the joint distribution over the super-symbol, where the state (XY) (i−1)q refers to the input and output sequences before the start of the super-symbol. The universal system is shown to asymptotically attain a rate which is at least the weighted average of the pessimistic average channel capacities measured over the epochs .\nNext, the reference system with block size k is compared to the universal system during epoch m, where the super-symbol length is q = 2 m−1 . Consider a set of super-symbols in hops of k (l·k+j : l ∈ Z + , j = 1, . . . , k). Since the number of symbols between the start of two successive super-symbols in each of these \u201calignment\u201d sets divides by k, in each of these super- symbols, the reference system\u2019s blocks and the super-symbols align, i.e. the IFB/AFB blocks begin at the same location with respect to the beginning of the super-symbol.\nTherefore, there is an equivalence between the average error probability of the reference system over these super- symbols, and the error probability that would be attained for the \u201ccollapsed\u201d channel, generated by randomly and uniformly drawing one of the super-symbols in the set and operating the reference system over this channel. Due to this equivalence, the reference system\u2019s rate, for a given average error proba- bility, is limited by the capacity of the \u201ccollapsed\u201d channel.\nFor the IFB case, this \u201ccollapsed\u201d channel is induced not only by the channel law, but also by the behavior of the reference system in previous blocks. When replacing the collapsed channel with a similar channel, where the history (XY) (i−1)q before each super-symbol is forced to a speciﬁc value, then due to the fading memory assumption, from some point in the block onward, the two channels become similar (in L 1 sense). Due to this similarity, the increase in error probability, when exchanging the original \u201ccollapsed channel\u201d with the new one, is small. For the AFB case, this transition is not needed, as the desired relation stems immediately from the deﬁnition.\nUsing a variant of Fano\u2019s inequality, the rate of the IFB/AFB system is related to the capacity of the pessimistic average channel measured over each of the k alignment sets of super- symbols. The pessimistic average channel over the epoch, is the average of the k average channels measured over the alignment sets. Averaging k channels may induce a loss of at most log k in capacity. This results in a bound on the pessimistic average channel during each epoch, as a function of the IFB/AFB capacity, and the IFB/AFB error probability during the epoch. Note that at this stage, the error probability of the reference system cannot be dismissed as being small, since only the average (over growing intervals in time) is guaranteed to be small. Taking the weighted average of the pessimistic capacities over the epochs enables relating the rate of the universal system to the rate and the average error probability of the reference system, where the latter tends to zero. All overheads, such as the ones related to alignment of the blocks to the super-symbols, the time it takes the channel memory to fade, the log k penalty for mixing k channels, vanish asymptotically as the super-symbol length increases\nindeﬁnitely with time. The full proof can be found in the full version of this paper [12].\nThe universal communication system presented does not require any modeling of the channel, and does not assume the channel is stationary in any way, and still achieves com- petitive rates. Although this result is pleasing in terms of the asymptotical rate, it is theoretical in the sense that the convergence rate was not optimized and is expected to be slow. The best convergence rate, and more efﬁcient schemes are left for further study (see comments in the full paper).\nEven if the scheme is improved, the issue remains that in the setting considered here and in [4], the transmission lengths N required to obtain a small redundancy compared to a reference system of block size k, grow exponentially with k (see the lower bounds on redundancy [4]). Although a similar issue occurs with Lempel-Ziv universality compared to ﬁnite state encoders [9] (see also [12]), this makes the current result theoretical. Complementary results that present faster convergence rates under simpler models or reference systems are required in order to show universal communication schemes can have gains that are realizable in practice (such is the result of [5], for example).\nSee the discussion section in the full paper [12] for ad- ditional commentary on convergence rates and alternative models."},"refs":[{"authors":[{"name":"A. Lapidoth"},{"name":"P. Narayan"}],"title":{"text":"Reliable communication under channel uncertainty"}},{"authors":[{"name":"O. Shayevitz"},{"name":"M. Feder"}],"title":{"text":"Achieving the empirical capacity using feedback: Memoryless additive models"}},{"authors":[{"name":"K. Eswaran"},{"name":"A. Sarwate"},{"name":"A. Sahai"},{"name":"M. Gastpar"}],"title":{"text":"Zero-rate feedback can achieve the empirical capacity"}},{"authors":[{"name":"Y. Lomnit"},{"name":"M. Feder"}],"title":{"text":"Dec"}},{"authors":[],"title":{"text":""}},{"authors":[{"name":"A. Lapidoth"},{"name":"I. Telatar"}],"title":{"text":"The compound channel capacity of a class of ﬁnite-state channels"}},{"authors":[{"name":"Y. Lomnitz"},{"name":"M. Feder"}],"title":{"text":"Communication over individual channels"}},{"authors":[{"name":"S. Verd´u"},{"name":"T. Han"}],"title":{"text":"A general formula for channel capacity"}},{"authors":[{"name":"J. Ziv"},{"name":"A. Lempel"}],"title":{"text":"Compression of individual sequences via variable-rate coding"}},{"authors":[{"name":"R. Gallage"}],"title":{"text":"Information Theory and Reliable Communication"}},{"authors":[{"name":"V. Misra"},{"name":"T. Weissman"}],"title":{"text":"The porosity of additive noise sequences"}},{"authors":[{"name":"Y. Lomnit"},{"name":"M. Feder"}],"title":{"text":"Jan"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569564989.pdf"},"links":[{"id":"1569566567","weight":9},{"id":"1569564843","weight":6},{"id":"1569566381","weight":2},{"id":"1569566527","weight":2},{"id":"1569566485","weight":4},{"id":"1569565383","weight":4},{"id":"1569565883","weight":5},{"id":"1569564889","weight":3},{"id":"1569565223","weight":2},{"id":"1569566725","weight":6},{"id":"1569565663","weight":4},{"id":"1569565377","weight":4},{"id":"1569566385","weight":8},{"id":"1569567049","weight":2},{"id":"1569564635","weight":5},{"id":"1569565867","weight":7},{"id":"1569565067","weight":4},{"id":"1569559665","weight":2},{"id":"1569561021","weight":2},{"id":"1569565691","weight":3},{"id":"1569566875","weight":4},{"id":"1569559617","weight":5},{"id":"1569566981","weight":4},{"id":"1569566321","weight":4},{"id":"1569566605","weight":2},{"id":"1569565489","weight":3},{"id":"1569566683","weight":4},{"id":"1569566855","weight":2},{"id":"1569560629","weight":2},{"id":"1569566869","weight":4},{"id":"1569565097","weight":3},{"id":"1569566227","weight":3},{"id":"1569566091","weight":2},{"id":"1569559259","weight":3},{"id":"1569566697","weight":3},{"id":"1569566597","weight":7},{"id":"1569565551","weight":5},{"id":"1569566761","weight":2},{"id":"1569566943","weight":5},{"id":"1569565091","weight":4},{"id":"1569566591","weight":3},{"id":"1569556029","weight":3},{"id":"1569566571","weight":9},{"id":"1569552245","weight":3},{"id":"1569565607","weight":2},{"id":"1569565495","weight":3},{"id":"1569567045","weight":3},{"id":"1569565227","weight":2},{"id":"1569564481","weight":3},{"id":"1569566415","weight":4},{"id":"1569564805","weight":3},{"id":"1569567005","weight":3},{"id":"1569566469","weight":5},{"id":"1569566081","weight":6},{"id":"1569565613","weight":2},{"id":"1569565355","weight":7},{"id":"1569564469","weight":2},{"id":"1569565931","weight":8},{"id":"1569566373","weight":2},{"id":"1569566647","weight":2},{"id":"1569551535","weight":7},{"id":"1569566765","weight":3},{"id":"1569565775","weight":2},{"id":"1569565547","weight":2},{"id":"1569566871","weight":3},{"id":"1569565461","weight":5},{"id":"1569564731","weight":3},{"id":"1569565171","weight":2},{"id":"1569566207","weight":2},{"id":"1569564227","weight":6},{"id":"1569558325","weight":3},{"id":"1569566671","weight":5},{"id":"1569566303","weight":3},{"id":"1569566119","weight":2},{"id":"1569564233","weight":18},{"id":"1569566459","weight":5},{"id":"1569563411","weight":6},{"id":"1569564401","weight":2},{"id":"1569564849","weight":4},{"id":"1569559541","weight":2},{"id":"1569565459","weight":2},{"id":"1569566319","weight":3},{"id":"1569565123","weight":2},{"id":"1569566941","weight":7},{"id":"1569566033","weight":3},{"id":"1569555811","weight":3},{"id":"1569558459","weight":7},{"id":"1569565291","weight":8},{"id":"1569564203","weight":5},{"id":"1569566821","weight":7},{"id":"1569556713","weight":5},{"id":"1569562685","weight":5},{"id":"1569566467","weight":5},{"id":"1569565771","weight":3},{"id":"1569566157","weight":3},{"id":"1569566903","weight":5},{"id":"1569566999","weight":4},{"id":"1569565859","weight":3},{"id":"1569564249","weight":2},{"id":"1569566843","weight":10},{"id":"1569566579","weight":3},{"id":"1569558483","weight":2},{"id":"1569566563","weight":2},{"id":"1569566089","weight":2},{"id":"1569566173","weight":3},{"id":"1569559221","weight":2},{"id":"1569556091","weight":2},{"id":"1569565347","weight":4},{"id":"1569566925","weight":2},{"id":"1569564387","weight":2},{"id":"1569565455","weight":12},{"id":"1569566497","weight":2},{"id":"1569566963","weight":3},{"id":"1569561679","weight":2},{"id":"1569566709","weight":8},{"id":"1569565897","weight":4},{"id":"1569551763","weight":5},{"id":"1569565953","weight":2},{"id":"1569566895","weight":4},{"id":"1569566889","weight":3},{"id":"1569566749","weight":4},{"id":"1569566269","weight":3},{"id":"1569564189","weight":6},{"id":"1569564195","weight":3},{"id":"1569561513","weight":2},{"id":"1569566985","weight":4},{"id":"1569564613","weight":2},{"id":"1569567009","weight":4},{"id":"1569566865","weight":2},{"id":"1569565321","weight":2},{"id":"1569558785","weight":2},{"id":"1569564647","weight":2},{"id":"1569566095","weight":3},{"id":"1569566193","weight":3},{"id":"1569564271","weight":6},{"id":"1569565907","weight":8},{"id":"1569566343","weight":2},{"id":"1569564311","weight":2},{"id":"1569565803","weight":2},{"id":"1569565785","weight":3},{"id":"1569566239","weight":5},{"id":"1569566679","weight":7},{"id":"1569565989","weight":3},{"id":"1569566575","weight":5},{"id":"1569563981","weight":4},{"id":"1569561085","weight":2},{"id":"1569566419","weight":4},{"id":"1569566617","weight":5},{"id":"1569559565","weight":5},{"id":"1569566905","weight":6},{"id":"1569566753","weight":2},{"id":"1569563307","weight":7},{"id":"1569566063","weight":3},{"id":"1569558681","weight":13},{"id":"1569566759","weight":6},{"id":"1569565589","weight":3},{"id":"1569559195","weight":4},{"id":"1569566149","weight":6},{"id":"1569559995","weight":6},{"id":"1569566657","weight":4},{"id":"1569558859","weight":4},{"id":"1569565199","weight":2},{"id":"1569565213","weight":5},{"id":"1569565365","weight":2},{"id":"1569566643","weight":2},{"id":"1569566511","weight":7},{"id":"1569565841","weight":4},{"id":"1569566531","weight":4},{"id":"1569567665","weight":18},{"id":"1569561143","weight":5},{"id":"1569566581","weight":4},{"id":"1569565833","weight":4},{"id":"1569566489","weight":2},{"id":"1569564611","weight":8},{"id":"1569565535","weight":4},{"id":"1569562867","weight":3},{"id":"1569566395","weight":3},{"id":"1569561795","weight":2},{"id":"1569566845","weight":3},{"id":"1569566325","weight":6},{"id":"1569566423","weight":4},{"id":"1569564795","weight":4},{"id":"1569567015","weight":4},{"id":"1569559805","weight":2},{"id":"1569566437","weight":6},{"id":"1569566811","weight":3},{"id":"1569566851","weight":2},{"id":"1569558901","weight":2},{"id":"1569565735","weight":2},{"id":"1569553909","weight":3},{"id":"1569559111","weight":3},{"id":"1569566687","weight":2},{"id":"1569566939","weight":4},{"id":"1569553537","weight":7},{"id":"1569565427","weight":5},{"id":"1569566403","weight":3},{"id":"1569565915","weight":6},{"id":"1569552251","weight":5},{"id":"1569553519","weight":3},{"id":"1569567051","weight":4},{"id":"1569566885","weight":4},{"id":"1569564441","weight":2},{"id":"1569566231","weight":3},{"id":"1569564209","weight":2},{"id":"1569566513","weight":4},{"id":"1569566425","weight":5},{"id":"1569554881","weight":5},{"id":"1569554971","weight":5},{"id":"1569565501","weight":6},{"id":"1569566899","weight":2},{"id":"1569566445","weight":2},{"id":"1569566209","weight":10},{"id":"1569562821","weight":2},{"id":"1569565559","weight":2},{"id":"1569566371","weight":3},{"id":"1569565655","weight":4},{"id":"1569566909","weight":5},{"id":"1569566127","weight":5},{"id":"1569565151","weight":2},{"id":"1569558985","weight":3},{"id":"1569563763","weight":3},{"id":"1569565087","weight":2},{"id":"1569566473","weight":8},{"id":"1569564857","weight":2},{"id":"1569564333","weight":2},{"id":"1569566913","weight":3},{"id":"1569566809","weight":2},{"id":"1569566629","weight":4},{"id":"1569566257","weight":2},{"id":"1569565033","weight":9},{"id":"1569566447","weight":5},{"id":"1569566357","weight":2},{"id":"1569565847","weight":4},{"id":"1569564353","weight":2},{"id":"1569563897","weight":10},{"id":"1569565887","weight":5},{"id":"1569565929","weight":2},{"id":"1569566141","weight":3},{"id":"1569566721","weight":3},{"id":"1569565055","weight":2},{"id":"1569565633","weight":4},{"id":"1569555879","weight":5},{"id":"1569566115","weight":2},{"id":"1569565219","weight":5},{"id":"1569558509","weight":6},{"id":"1569554759","weight":3},{"id":"1569564851","weight":2},{"id":"1569565185","weight":2},{"id":"1569556671","weight":2},{"id":"1569566037","weight":2},{"id":"1569565095","weight":2},{"id":"1569566223","weight":2},{"id":"1569566553","weight":9},{"id":"1569564973","weight":6},{"id":"1569564969","weight":6},{"id":"1569566043","weight":2},{"id":"1569565029","weight":4},{"id":"1569561245","weight":6},{"id":"1569566505","weight":2},{"id":"1569565393","weight":4},{"id":"1569565933","weight":2},{"id":"1569562207","weight":5},{"id":"1569566191","weight":3},{"id":"1569567033","weight":5},{"id":"1569565527","weight":5},{"id":"1569566853","weight":2},{"id":"1569566603","weight":4},{"id":"1569567029","weight":2},{"id":"1569565363","weight":3},{"id":"1569566159","weight":2},{"id":"1569566695","weight":2},{"id":"1569566051","weight":4},{"id":"1569561379","weight":5},{"id":"1569565909","weight":2},{"id":"1569561123","weight":7},{"id":"1569565467","weight":4},{"id":"1569566655","weight":5},{"id":"1569566673","weight":3},{"id":"1569565739","weight":3},{"id":"1569565311","weight":4},{"id":"1569566233","weight":6},{"id":"1569566667","weight":3},{"id":"1569566893","weight":5},{"id":"1569566317","weight":3},{"id":"1569564097","weight":3},{"id":"1569560997","weight":6},{"id":"1569563845","weight":5},{"id":"1569566407","weight":6},{"id":"1569560349","weight":3},{"id":"1569566501","weight":4},{"id":"1569565741","weight":3},{"id":"1569566275","weight":2},{"id":"1569566481","weight":3},{"id":"1569565545","weight":3},{"id":"1569566857","weight":2},{"id":"1569566387","weight":2},{"id":"1569566245","weight":5},{"id":"1569560503","weight":9},{"id":"1569565463","weight":3},{"id":"1569566219","weight":9},{"id":"1569565439","weight":4},{"id":"1569566229","weight":4},{"id":"1569566949","weight":2},{"id":"1569566133","weight":2},{"id":"1569562551","weight":3},{"id":"1569563395","weight":3},{"id":"1569566901","weight":3},{"id":"1569551347","weight":3},{"id":"1569565415","weight":13},{"id":"1569555367","weight":2},{"id":"1569566383","weight":5},{"id":"1569564485","weight":3},{"id":"1569565155","weight":2},{"id":"1569566631","weight":2},{"id":"1569565571","weight":5},{"id":"1569565885","weight":5},{"id":"1569566177","weight":4},{"id":"1569557633","weight":2},{"id":"1569564411","weight":5},{"id":"1569566805","weight":3},{"id":"1569559199","weight":2},{"id":"1569566929","weight":2},{"id":"1569566293","weight":4},{"id":"1569565665","weight":2},{"id":"1569566831","weight":6},{"id":"1569565549","weight":4},{"id":"1569565523","weight":2},{"id":"1569565611","weight":3},{"id":"1569557715","weight":2},{"id":"1569564175","weight":4},{"id":"1569566983","weight":2},{"id":"1569566779","weight":3},{"id":"1569566097","weight":2},{"id":"1569566479","weight":3},{"id":"1569566431","weight":2},{"id":"1569565397","weight":2},{"id":"1569566873","weight":5},{"id":"1569565765","weight":6},{"id":"1569565925","weight":3},{"id":"1569565435","weight":2},{"id":"1569557275","weight":7},{"id":"1569565263","weight":2},{"id":"1569565215","weight":5},{"id":"1569565385","weight":3},{"id":"1569565575","weight":2},{"id":"1569565919","weight":6},{"id":"1569565181","weight":3},{"id":"1569566711","weight":11},{"id":"1569565241","weight":2},{"id":"1569566927","weight":5},{"id":"1569565661","weight":3},{"id":"1569565865","weight":6},{"id":"1569566887","weight":3},{"id":"1569565273","weight":2},{"id":"1569566267","weight":8},{"id":"1569564131","weight":4},{"id":"1569564919","weight":6},{"id":"1569565511","weight":2},{"id":"1569566737","weight":8},{"id":"1569561221","weight":4},{"id":"1569564595","weight":2},{"id":"1569566917","weight":3},{"id":"1569566035","weight":2},{"id":"1569566253","weight":3},{"id":"1569565353","weight":2},{"id":"1569564683","weight":2},{"id":"1569564305","weight":6},{"id":"1569564291","weight":2},{"id":"1569566691","weight":4},{"id":"1569565421","weight":6},{"id":"1569566547","weight":3},{"id":"1569566651","weight":3},{"id":"1569566823","weight":5},{"id":"1569566677","weight":4},{"id":"1569565349","weight":2},{"id":"1569566137","weight":4},{"id":"1569565013","weight":2},{"id":"1569565829","weight":2},{"id":"1569565645","weight":2},{"id":"1569565375","weight":6},{"id":"1569566715","weight":2},{"id":"1569566639","weight":2},{"id":"1569565041","weight":3},{"id":"1569564703","weight":3},{"id":"1569566713","weight":3},{"id":"1569565597","weight":2},{"id":"1569566813","weight":6},{"id":"1569565293","weight":2},{"id":"1569566771","weight":4},{"id":"1569564201","weight":2},{"id":"1569562277","weight":2},{"id":"1569566641","weight":3},{"id":"1569565425","weight":2},{"id":"1569559035","weight":3},{"id":"1569564247","weight":5},{"id":"1569564437","weight":6},{"id":"1569566533","weight":2},{"id":"1569551905","weight":3},{"id":"1569564861","weight":2},{"id":"1569565457","weight":6},{"id":"1569564787","weight":2},{"id":"1569566487","weight":3},{"id":"1569565529","weight":5},{"id":"1569556759","weight":6},{"id":"1569566619","weight":3},{"id":"1569561185","weight":2},{"id":"1569566075","weight":3},{"id":"1569566397","weight":2},{"id":"1569558779","weight":2},{"id":"1569565669","weight":3},{"id":"1569565233","weight":4},{"id":"1569565593","weight":3},{"id":"1569560235","weight":5},{"id":"1569566817","weight":3},{"id":"1569564157","weight":3},{"id":"1569566389","weight":3},{"id":"1569566435","weight":2},{"id":"1569567483","weight":2},{"id":"1569566911","weight":3},{"id":"1569564923","weight":5},{"id":"1569565367","weight":3},{"id":"1569566299","weight":8},{"id":"1569564281","weight":2},{"id":"1569565039","weight":2},{"id":"1569564769","weight":7},{"id":"1569565769","weight":3},{"id":"1569565805","weight":5},{"id":"1569566933","weight":4},{"id":"1569563919","weight":4},{"id":"1569566577","weight":6},{"id":"1569557851","weight":5},{"id":"1569565389","weight":3},{"id":"1569559919","weight":5},{"id":"1569566147","weight":7},{"id":"1569565537","weight":4},{"id":"1569562367","weight":2},{"id":"1569560785","weight":5},{"id":"1569565561","weight":3},{"id":"1569565631","weight":2},{"id":"1569555891","weight":10},{"id":"1569566847","weight":3},{"id":"1569565997","weight":2},{"id":"1569563425","weight":2},{"id":"1569565035","weight":4},{"id":"1569559597","weight":3},{"id":"1569564961","weight":2},{"id":"1569559251","weight":3},{"id":"1569565089","weight":2},{"id":"1569567013","weight":3},{"id":"1569566583","weight":3},{"id":"1569561861","weight":7},{"id":"1569565337","weight":5},{"id":"1569565737","weight":4},{"id":"1569560459","weight":3},{"id":"1569564463","weight":4},{"id":"1569565853","weight":5},{"id":"1569550425","weight":5},{"id":"1569566273","weight":2},{"id":"1569564123","weight":5},{"id":"1569566341","weight":5},{"id":"1569565889","weight":5},{"id":"1569566635","weight":5},{"id":"1569566611","weight":3},{"id":"1569551539","weight":3},{"id":"1569564505","weight":3},{"id":"1569565165","weight":4},{"id":"1569565565","weight":4},{"id":"1569565635","weight":4},{"id":"1569561397","weight":3},{"id":"1569565731","weight":2},{"id":"1569556327","weight":2},{"id":"1569566797","weight":3},{"id":"1569566125","weight":2},{"id":"1569566413","weight":5},{"id":"1569565707","weight":2},{"id":"1569565113","weight":9},{"id":"1569566375","weight":4},{"id":"1569564257","weight":4},{"id":"1569565583","weight":2},{"id":"1569566555","weight":3},{"id":"1569564931","weight":4},{"id":"1569565373","weight":4},{"id":"1569564141","weight":2},{"id":"1569566973","weight":3},{"id":"1569561579","weight":7},{"id":"1569566449","weight":7},{"id":"1569566987","weight":3},{"id":"1569565031","weight":4},{"id":"1569564509","weight":3},{"id":"1569551541","weight":6},{"id":"1569565619","weight":2},{"id":"1569566839","weight":3},{"id":"1569551751","weight":4},{"id":"1569565139","weight":4},{"id":"1569566663","weight":5},{"id":"1569564419","weight":2},{"id":"1569565579","weight":2},{"id":"1569566067","weight":8},{"id":"1569566825","weight":5},{"id":"1569566241","weight":2},{"id":"1569564807","weight":4},{"id":"1569566609","weight":3},{"id":"1569563007","weight":2},{"id":"1569566113","weight":7},{"id":"1569566443","weight":5},{"id":"1569566727","weight":8},{"id":"1569565315","weight":3},{"id":"1569560581","weight":3},{"id":"1569559233","weight":4}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S14.T7.3","endtime":"17:40","authors":"Yuval Lomnitz, Meir Feder","date":"1341508800000","papertitle":"Universal communication over unknown vector channels","starttime":"17:20","session":"S14.T7: Topics in Shannon Theory","room":"Stratton (407)","paperid":"1569564989"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"3","spectral43":"9","spectral28":"9","spectral32":"22","spectral14":"5","spectral20":"14","spectral9":"8","spectral25":"4","spectral42":"32","spectral3":"0","spectral47":"25","spectral17":"0","louvain":"385","spectral36":"18","spectral39":"37","spectral10":"4","spectral15":"8","spectral33":"15","spectral5":"4","spectral21":"10","spectral44":"9","spectral26":"2","spectral40":"21","spectral8":"2","spectral11":"8","spectral4":"2","spectral37":"34","spectral48":"5","spectral22":"21","spectral23":"5","spectral12":"5","spectral50":"17","spectral19":"13","spectral34":"22","spectral45":"0","spectral7":"5","spectral49":"27","spectral38":"6","spectral24":"21","spectral13":"11","spectral31":"4","spectral29":"28","spectral35":"33","spectral30":"4","spectral41":"12","spectral27":"24","spectral18":"15","spectral46":"40","spectral2":"0","spectral16":"1"}}

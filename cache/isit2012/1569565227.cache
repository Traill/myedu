{"id":"1569565227","paper":{"title":{"text":"Combinatorial Selection and Least Absolute Shrinkage via the C LASH Algorithm"},"authors":[{"name":"Anastasios Kyrillidis"},{"name":"Volkan Cevher"}],"abstr":{"text":"Abstract\u2014The least absolute shrinkage and selection operator (LASSO) for linear regression exploits the geometric interplay of the ` 2 -data error objective and the ` 1 -norm constraint to arbitrarily select sparse models. Guiding this uninformed selec- tion process with sparsity models has been precisely the center of attention over the last decade in order to improve learning performance. To this end, we alter the selection process of LASSO to explicitly leverage combinatorial sparsity models (CSMs) via the combinatorial selection and least absolute shrinkage (C LASH ) operator. We provide concrete guidelines how to leverage com- binatorial constraints within C LASH , and characterize C LASH \u2019s guarantees as a function of the set restricted isometry constants of the sensing matrix. Finally, our experimental results show that C LASH can outperform both LASSO and model-based compressive sensing in sparse estimation."},"body":{"text":"The least absolute shrinkage and selection operator (LASSO) is the de facto standard algorithm for regression [1]. LASSO estimates sparse linear models by minimizing the empirical data error via:\n, (1) where k · k r is the ` r -norm. In (1), 2 R m ⇥n is the sensing matrix, y 2 R m are the responses (or observations), x 2 R n is the loading vector and 2 R ++ governs the sparsity of the solution. Along with many efﬁcient algorithms for its solution, the LASSO formulation is now backed with a rather mature theory for the generalization of its solutions as well as its variable selection consistency [2]\u2013[5].\nWhile the long name attributed to (1) is apropos, 1 it does not capture the LASSO\u2019s arbitrariness in subset selection via shrinkage to best explain the responses. In fact, this uninformed selection process not only prevents interpretability of results in many problems, but also fails to exploit key prior information that could radically improve learning per- formance. Based on this premise, approaches to guide the selection process of the LASSO are now aplenty.\nSurprisingly, while the prior information in many regression problems generate fundamentally discrete constraints (e.g., on the sparsity patterns or the support of the LASSO solution),\nthe majority of the existing approaches that enforce such constraints in selection are inherently continuous. For instance, a prevalent approach is to tailor a sparsity inducing norm to the constraints on the support set (c.f., [6]). That is, we create a structured convex norm by mixing basic norms with weights over pre-deﬁned groups or using the Lov´asz extension of non- decreasing submodular set functions of the support. As many basic norms have well-understood behavior in sparse selection, reverse engineering such norms is quite intuitive.\nWhile such structure inducing, convex norm-based ap- proaches on the LASSO are impressive, our contention in this paper is that, in order to truly make an impact in structured sparsity problems, we must fully leverage explicitly combinatorial approaches to guide LASSO\u2019s subset selection process. To achieve this, we show how Euclidean projections with structured sparsity constraints correspond to an integer linear program (ILP), which can be exactly or approximately solved subject to matroid (via the greedy algorithm), and certain linear inequality constraints (via convex relaxation or multi-knapsack solvers). A key actor in this process is a polynomial-time combinatorial algorithm that goes beyond simple selection heuristics towards provable solution quality as well as runtime/space bounds.\nFurthermore, we introduce our combinatorial selection and least absolute shrinkage (C LASH ) operator and theoretically characterize its estimation guarantees. C LASH enhances the model-based compressive sensing (model-CS) framework [7] by combining ` 1 -norm and combinatorial constraints on the regression vector. Therefore, C LASH uses a combination of shrinkage and hard thresholding operations to signiﬁcantly outperform the model-CS approach, LASSO, or continuous structured sparsity approaches in learning performance of sparse linear models. Furthermore, C LASH establishes a re- gression framework where the underlying tractability of ap- proximation in combinatorial selection is directly reﬂected in the algorithm\u2019s estimation and convergence guarantees.\nThe organization of the paper is as follows. In Sections II and III, we set up the notation and the exact projections with structured sparsity constraints. We develop C LASH in Section IV and highlight the key components of its convergence proof in Section V. We present numerical results in Section VI. We provide our conclusions in Section VII.\nNotation: We use [x] j to denote the j-th element of x, and let x i represent the i-th iterate of C LASH . The index set of n\ndimensions is denoted as N = {1, 2, . . . , n}. Given S ✓ N , we deﬁne the complement set S c = N \\ S. Moreover, given a set S ✓ N and a vector x 2 R n , (x) S 2 R n denotes a vector with the following properties: [(x) S ] S = [x] S and [(x) S ] S c = 0 . The support set of x is deﬁned as supp(x) = {i : [x] i 6= 0}. We use |S| to denote the cardinality of the set S. The empirical data error is denoted as f(x) , ky x k 2 2 , with gradient deﬁned as rf(x) , 2 T (y x) , where T is the transpose operation. The notation r S f (x) is shorthand for (rf(x)) S . I represents the identity matrix. Combinatorial notions of sparsity: We provide some deﬁnitions on combinatorial sparse models, and elaborate on a subset of interesting models with algorithmic implications.\nDeﬁnition 1 (Combinatorial sparsity models (CSMs)). We deﬁne a combinatorial sparsity model C k = {S m : 8m, S m ✓ N , |S m |  k} with the sparsity parameter k as a collection of distinct index subsets S\nThroughout the paper, we assume that any CSM C k is downward compatible, i.e., removing any subset of indices of any given element in C k , it is still in C k .\nProperties of the regression matrix: Deriving approx- imation guarantees for C LASH behooves us to assume the restricted isometry property (RIP) (deﬁned below) on the regression matrix [8]. While the RIP and other similar conditions for deriving consistency properties of LASSO and its variants, such as the unique/exact representation property or the irrepresentable condition [5], [6], [9]\u2013[11], are unveriﬁable a priori without exhaustive search, many random matrices satisfy them with high probability.\nDeﬁnition 2 (RIP [7], [8]). The regression matrix has the k- RIP with an isometry constant k when\n(1 k ) kxk 2 2  k xk 2 2  (1 + k ) kxk 2 2 , \t (2) 8supp(x) 2 C k , where k = max S2C k T S S I 2 !2 , and\nHere, we also comment on the scaling of (k, m, n) for the desired level of isometry. When the entries of can be modeled as independent and identically distributed (iid) with respect to a sub-Gaussian distribution, we can show that m = O 2 k (log(2M ) + k log(12 1 k )) with overwhelming probability [7]. Here, M is the minimum number of subspaces covering C k . While m explicitly depends on n, for certain restricted CSMs, such as the rooted connected tree of [7], this dependence can be quite weak, e.g., m = O(k).\nThe workhorse of the model-CS approach is the following non-convex projection problem onto CSMs, as deﬁned by C k , which is a basic subset selection problem:\nkw x k 2 2 : supp(w) 2 C k   , (3) where P C k (x) is the projection operator. [7] shows that as long as P C k ( ·) is exactly computed in polynomial time for a CSM,\ntheir sparse recovery algorithms inherit strong approximation guarantees for that CSM. To better identify the CSMs that live within the model-CS assumptions, we ﬁrst state the following key observation\u2014the proof can be found in [12].\nLemma 1 (Euclidean projections onto CSMs). The support of the Euclidean projection onto C k in (3) can be obtained as a solution to the following discrete optimization problem:\nwhere F (S; x) = kxk 2 2 k(x) S x k 2 2 = P i 2S |[x] i | 2 is the modular, variance reduction set function. Moreover, let b S 2 C k be the minimizer of the discrete problem. Then, it holds that P C k (x) = (x) b S , which corresponds to hard thresholding.\nThe following proposition reﬁnes this observation to further accentuate the algorithmic implications for CSMs:\nProposition 1 (CSM projections via ILP\u2019s). The problem (4) is equivalent to the following integer linear program (ILP):\nThe proof of Proposition 1 is straightforward and is omitted. Regardless of whether we use a dynamic program, a greedy\ncombinatorial algorithm, or an ILP solver, the formulations (4) or (5) make the underlying tractability of the combinatorial selection explicit. We highlight this notion via the polynomial- time modular ✏-approximation property (PMAP ✏ ):\nDeﬁnition 3 (PMAP ✏ [12]) . A CSM has the PMAP ✏ with constant ✏, if the modular subset selection problem (4) or the ILP (5) admit an ✏-approximation scheme with polynomial or pseudo-polynomial time complexity as a function of n, 8x 2 R n . Denoting the ✏-approximate solution of (4) or (5) as b S ✏ , this means F ( b S ✏ ; x) (1 ✏) max S2C k F ( S; x).\nMatroids: By matroid, we mean that C k = ( N , I) is a ﬁnite collection of subsets of N that satisﬁes three conditions: (i) {;} 2 I, (ii) if S is in I, then any subset of S is also in I, and (iii) for S 1 , S 2 2 I and |S 1 | > |S 2 |, there is an element s 2 S 1 \\ S 2 such that S 2 [ {s} is in I. As a simple example, the unstructured sparsity model (i.e., x is k-sparse) forms a uniform matroid as it is deﬁned as the union of all subsets of N with cardinality k or less. When C k forms a matroid, the greedy basis algorithm can efﬁciently compute (3) by solving (4) [13] where sorting and selecting the k largest elements in absolute value is suffcient to obtain the exact projection.\nMoreover, it turns out that this particular perspective pro- vides a principled and tractable approach to encode an inter- esting class of matroid-structured sparsity models. The recipe is quite simple: we seek the intersection of a structure provider\nInput: y, , , P C k , Tolerance ⌘, MaxIterations Initialize: x 0   0, X 0   {;}, i   0 repeat\n5: X i+1   supp(x i+1 ) i   i + 1.\nmatroid (e.g., partition, cographic/graphic, disjoint path, or matching matroid) with the sparsity provider uniform matroid. While the intersection of two matroids is not a matroid in general, we can prove that the intersection of the uniform matroid with any other matroid satisﬁes the conditions above.\nLinear support constraints: Many interesting CSMs C k can be encoded using linear support constraints of the form:\nwhere [A, b] is an integral matrix, and the ﬁrst row of A is all 1\u2019s and [b] 1 = k . As a basic example, the neuronal spike model of [14] is based on linear support constraints where each spike respects a minimum refractory distance to each other.\nA key observation is that if each of the nonempty faces of Z contains an integral point (i.e., forming an integral polyhedra), then convex relaxation methods can exactly obtain the correct integer solutions in polynomial time. In general, checking the integrality of Z is NP-Hard. However, if Z is integral and non-empty for all integral b, then a necessary condition is that A be a totally unimodular (TU) matrix [13]. A matrix is totally unimodular if the determinant of each square submatrix is equal to 0,1, or -1. Example TU matrices include interval, perfect, and network matrices [13]. As expected, the constraint matrix A of [14] is TU. Moreover, it is easy to verify that the sparse disjoint group model of [15] also deﬁnes a TU constraint, where groups have individual sparsity budgets.\nFor completeness and due to lack of space, we only mention PMAP ✏ , which extends the breath of the model-CS approach. For a detailed treatment of PMAP ✏ and C LASH , c.f. [12], which describes multi-knapsack CSMs as a concrete example. Moreover, for many of the PMAP 0 examples above, we can employ ✏-approximate\u2014randomized\u2014techniques to reduce computational cost.\nThe new C LASH algorithm obtains approximate solutions to the LASSO problem in (1) with the added twist that the solution must live within the CSM, as deﬁned by C k :\nWhen available, using the CSM constraint C k in addition to the ` 1 -norm constraint enhances learning in two important ways. First, the combinatorial constraints restricts the LASSO solution to exhibit true model-based supports, increasing the interpretability of the solution without relaxing C k into a convex norm. Second, it empirically requires much fewer number of samples to obtain the true solution than both the LASSO and the model-CS approaches.\nWe provide a pseudo-code of an example implementation of C LASH in Algorithm 1. One can think of alternative ways of implementing C LASH , such as single gradient updates in Step 2, or removing Step 4 altogether. While such changes may lead to different\u2014possibly better\u2014approximation guar- antees for the solution of (6), we observe degradation in the empirical performance of the algorithm as compared to this implementation, whose guarantees are as follows:\nand c 1 ( 2k , 3k ) is a constant deﬁned in [16]. The iterations contract when 3k < 0.3658 .\nTheorem 1 shows that the isometry requirements of C LASH are competitive with the mainstream hard thresholding meth- ods, such as CoSaMP [17] and Subspace Pursuit [18], even though it incorporates the ` 1 -norm constraints, which, as Section VI illustrates, improves learning performance.\nRemark 1. [Model mismatch and selection] Let us assume a generative model y = + ˜ \" . Let x ⇤ be the best ap- proximation of in C k within ` 1 -ball of radius . Then, we can show that the iteration invariant of Theorem 1 still holds with SNR = kx ⇤ k 2 k\"k 2 , where k\"k 2  k˜\"k 2 + k ( x ⇤ ) k 2 , where the latter quantity (the impact of mismatch) can be analyzed using the restricted ampliﬁcation property of [7]. For instance, when C k is the uniform sparsity model, then k ( x ⇤ ) k 2  p 1 + k\n⌘ , which should presumably be small if the model is selected correctly.\nIn the absence of prior information, we automate the pa- rameter selection by using the Donoho-Tanner phase transition [19] to choose the maximum k allowed for a given (m, n)-pair, and then by using cross validation to pick [20].\nWe sketch the proof of Theorem 1 a l´a [17] and [21] assuming the general case of PMAP ✏ . The details of the proof can be found in an extended version of the paper [16].\nLemma 2 (Active set expansion - Step 1). The support set S i , where |S i |  2k, identiﬁes a subspace in C 2k such that:\nk(x i x ⇤ ) S c i k 2  ( 3k + 2k + p ✏(1 + 2k )) kx i x ⇤ k 2 +\nLemma 2 states that, at each iteration, Step 1 of C LASH identiﬁes a 2k support set such that the unrecovered energy of x ⇤ is bounded. For ✏ = 0, C LASH exactly identiﬁes the support where the projected gradient onto C k can make most impact on the loading vector in the support complement of its current solution, which are subsequently merged together.\nLemma 3 (Greedy descent with least absolute shrinkage - Step 2) . Let S i be a 2k-sparse support set. Then, the least squares solution v i in step 2 of Algorithm 1 satisﬁes\nWe borrow the proof of Lemma 3 from [21]. This step improves the objective function f(x) as much as possible on the active set in order to arbitrate the active set. The solution simultaneously satisﬁes the ` 1 -norm constraint.\nStep 3 projects the solution onto C k , whose action is characterized by the following lemma. Here, we show the ✏- approximate projection explicitly:\nLemma 4 (Combinatorial selection - Step 3). Let v i be a 2k- sparse proxy vector with indices in support set S i , C k be a CSM and i the projection of v i under C k . Then:\nCorollary 1 (De-bias - Step 4). Let i be the support set of a proxy vector i where | i |  k. Then, the least squares solution x i+1 in Step 4 satisﬁes\nStep 4 de-biases the current result on the putative solution support. Its characterization connects Lemmas 3 and 4:\nLemma 5. Let v i be the least squares solution of the greedy descent step (step 5) and i be a proxy vector to v i after applying Combinatorial selection step. Then, k i x ⇤ k 2 can be expressed in terms of the distance from v i to x ⇤ as follows:\n+ 2 3k p ✏ + ✏ · kv i x ⇤ k 2 + D 1 k\"k 2 + D 2 kx ⇤ k 2 + D 3\n(8) where D 1 , D 2 , D 3 are constants depending on ✏, 2k , 3k .\nFinally, the proof of Theorem 1 follows by concatenating Corollary 1 with Lemmas 2, 3, and 5, and setting ✏ = 0.\nIn the following experiments, we compare algorithms from the following list: (i) the LASSO algorithm [1], (ii) the Basis Pursuit DeNoising (BPDN) [22], (iii) the sparse-C LASH algorithm, where C k is the index set of k-sparse signals, (iv) the model-C LASH algorithm 2 , which explicitly carries C k , and\n(v) Subspace Pursuit (SP) algorithm [18], as integrated with the model-CS approach. We emphasize here that when ! 1 in (6), C LASH must converge to the model-based SP solution.\nThe LASSO algorithm ﬁnds a solution to the problem deﬁned in (1), where we use a Nesterov accelerated projected gradient algorithm. The BPDN algorithm in turn solves the following optimization problem:\nb x BPDN = arg min {kxk 1 : k x y k 2  } , (9) where represents prior knowledge on the energy of the additive noise term. To solve (9), we use the spectral projected gradient method SPGL1 algorithm [23].\nIn the experiments below, the nonzero coefﬁcients of x ⇤ are generated iid according to the standard normal distribution with kx ⇤ k 2 = 1 . The BPDN algorithm is given the true values. While C\nLASH is given the true value of k for the experiments below, additional experiments (not shown) shows that our phase transition heuristics is quite good and the mismatch is graceful as indicated in Remark 1. All the algorithms use a high precision stopping tolerance ⌘ = 10 5 .\nExperiment 1: Improving simple sparse recovery. In this experiment, we generate random realizations of the model y =\nx ⇤ +\" for n = 800. Here, is a dense random matrix whose entries are iid Gaussian with zero mean and variance 1/m. We consider two distinct generative model settings: (i) with additive Gaussian white noise with k\"k 2 = 0.05 , m = 240 and k = 89 , and (ii) the noiseless model (k\"k 2 = 0 ), m = 250 and sparsity parameter k = 93. For this experiment, we perform 500 Monte Carlo model realizations.\nWe sweep and illustrate the recovery performance of C LASH (6). Figures 1(a)-(b) illustrate that the combination of hard thresholding with norm constraints can improve the sig- nal recovery performance signiﬁcantly over convex-only and hard thresholding-only methods\u2014both in noisy and noiseless problem settings. For k\"k = 0, C LASH perfectly recovers the signal when is close to the true value. When ⌧ kx ⇤ k 1 , the performance degrades due to the large norm mismatch.\nExperiment 2: Improving structured sparse recovery We consider two signal CSMs: in the ﬁrst model, we assume k- sparse signals that admit clustered sparsity with coefﬁcients in C-contiguous blocks on an undirected, acyclic chain graph [24]. Without loss of generality, we use C = 5 (Figure 1(c)). The second model corresponds to a TU system where we partition the k-sparse signals into uniform blocks and force sparsity constraints on individual blocks; in this case, we solve the set optimization problem optimally via linear programming relaxation (Figure 1(d)). Here, the noise energy level satisﬁes k\"k 2 = 0.05 , and n = 500, m = 125, and k = 50. In both cases, we conduct 100 Monte Carlo iterations and perform sparse estimation for a range of values.\nIn Figure 1(c), we observe that clustered sparsity structure provides a distinct advantage in reconstruction compared to LASSO formulation and the sparse-C LASH algorithm. Further- more, note that when is large, norm constraints have no effect and the model-C LASH provides essentially the same results as the model-CS approach [7]. On the other hand, the sparse-\nC LASH improves signiﬁcantly beyond the LASSO solution thanks to the ` 1 -norm constraint.\nIn Figure 1(d) however, the situation is radically changed: while the TU constraints enhance the reconstruction of model- CS approach over simple sparse recovery, the improvement becomes quite large as the ` 1 -norm constraint kicks in. We also observe the improvement in sparse-C LASH but it is not as accentuated as the model-C LASH .\nC LASH establishes a regression framework where efﬁcient algorithms from combinatorial and convex optimization can interface for interpretable sparse solutions. Our experiments demonstrate that, while the model-based selection can greatly improve sparse recovery over the approaches based on uniform sparsity alone, the ` 1 -norm shrinkage operations have an undeniable, positive impact on the learning performance. Un- derstanding the tradeoffs between the complexity of approxi- mation and the recovery guarantees of C LASH is a promising theoretical and practical direction. Finally, joint combinatorial and norm constrained scenarios appear in many problems of interest such as portfolio optimization."},"refs":[{"authors":[{"name":"R. Tibshirani"},{"name":"J. Royal"}],"title":{"text":"Regression shrinkage and selection via the lasso"}},{"authors":[{"name":"J. Duch"},{"name":"S. Shalev-Shwart"},{"name":"Y. Singe"},{"name":"T. Chandra"}],"title":{"text":"Efﬁcient projections onto the ` 1 -ball for learning in high dimensions"}},{"authors":[{"name":"J. Bicke"},{"name":"Y. Rito"},{"name":"B. Tsybakov"}],"title":{"text":"P"}},{"authors":[{"name":"J. Wainwright"}],"title":{"text":"M"}},{"authors":[{"name":"P. Zha"},{"name":"B. Yu"}],"title":{"text":"On model selection consistency of lasso"}},{"authors":[{"name":"R. Jenatto"},{"name":"J.-Y. Audiber"},{"name":"F. Bach"}],"title":{"text":"Structured variable selection with sparsity-inducing norms"}},{"authors":[{"name":"G. Baraniu"},{"name":"V. Cevhe"},{"name":"F. Duart"},{"name":"C. Hegde"}],"title":{"text":"R"}},{"authors":[{"name":"E. J. Cand`e"},{"name":"T. Tao"}],"title":{"text":"Near optimal signal recovery from random projections: Universal encoding strategies?"}},{"authors":[{"name":"A. Tropp"}],"title":{"text":"J"}},{"authors":[{"name":"L. Jaco"},{"name":"G. Obozinsk"},{"name":"P. Vert"}],"title":{"text":"J"}},{"authors":[{"name":"F. Bach"}],"title":{"text":"Structured sparsity-inducing norms through submodular func- tions"}},{"authors":[{"name":"A. Kyrillidi"},{"name":"V. Cevher"}],"title":{"text":"Sublinear time, approximate model-based sparse recovery for all"}},{"authors":[{"name":"L. Nemhause"},{"name":"A. Wolsey"}],"title":{"text":"G"}},{"authors":[{"name":"C. Hegd"},{"name":"F. Duart"},{"name":"V. Cevher"}],"title":{"text":"M"}},{"authors":[{"name":"J. Friedma"},{"name":"T. Hasti"},{"name":"R. Tibshirani"}],"title":{"text":"A note on the group lasso and a sparse group lasso"}},{"authors":[{"name":"A. Kyrillidi"},{"name":"V. Cevher"}],"title":{"text":"Combinatorial selection and least absolute shrinkage via the C LASH algorithm"}},{"authors":[{"name":"D. Needel"},{"name":"J. Tropp"}],"title":{"text":"CoSaMP: Iterative signal recovery from in- complete and inaccurate samples"}},{"authors":[{"name":"W. Da"},{"name":"O. Milenkovic"}],"title":{"text":"Subspace pursuit for compressive sensing signal reconstruction"}},{"authors":[{"name":"D. L. Donoh"},{"name":"J. Tanner"}],"title":{"text":"Neighborliness of randomly projected simplices in high dimensions"}},{"authors":[{"name":"R. Ward"}],"title":{"text":"Compressed sensing with cross validation"}},{"authors":[{"name":"S. Foucart"}],"title":{"text":"Sparse recovery algorithms: sufﬁcient conditions in terms of restricted isometry constants"}},{"authors":[{"name":"S. S. Che"},{"name":"D. L. Donoh"},{"name":"M. A. Saunders"}],"title":{"text":"Atomic decomposition by basis pursuit"}},{"authors":[{"name":"E. van den Ber"},{"name":"M. P. Friedlander"}],"title":{"text":"Probing the pareto frontier for basis pursuit solutions"}},{"authors":[{"name":"V. Cevhe"},{"name":"P. Indy"},{"name":"C. Hegd"},{"name":"G. Baraniuk"}],"title":{"text":"R"}}]},"file":{"jsonClass":"File","file":"/home/arnfred/Code/trailhead/resources/isit2012/1569565227.pdf"},"links":[{"id":"1569566567","weight":2},{"id":"1569564843","weight":3},{"id":"1569566527","weight":3},{"id":"1569565383","weight":5},{"id":"1569565883","weight":2},{"id":"1569564889","weight":4},{"id":"1569565223","weight":2},{"id":"1569566725","weight":3},{"id":"1569565663","weight":2},{"id":"1569565377","weight":6},{"id":"1569566385","weight":4},{"id":"1569564635","weight":3},{"id":"1569565867","weight":2},{"id":"1569566799","weight":13},{"id":"1569565067","weight":2},{"id":"1569559665","weight":2},{"id":"1569561021","weight":5},{"id":"1569564669","weight":2},{"id":"1569566815","weight":5},{"id":"1569566875","weight":2},{"id":"1569566981","weight":2},{"id":"1569566321","weight":16},{"id":"1569566605","weight":2},{"id":"1569565489","weight":2},{"id":"1569566683","weight":3},{"id":"1569566855","weight":3},{"id":"1569560629","weight":4},{"id":"1569566869","weight":13},{"id":"1569565097","weight":3},{"id":"1569566091","weight":4},{"id":"1569559259","weight":14},{"id":"1569566697","weight":4},{"id":"1569566597","weight":3},{"id":"1569565551","weight":3},{"id":"1569565711","weight":3},{"id":"1569566761","weight":3},{"id":"1569566943","weight":2},{"id":"1569565091","weight":2},{"id":"1569566591","weight":2},{"id":"1569556029","weight":2},{"id":"1569566571","weight":4},{"id":"1569552245","weight":2},{"id":"1569565607","weight":2},{"id":"1569565495","weight":3},{"id":"1569559967","weight":2},{"id":"1569567045","weight":3},{"id":"1569564481","weight":7},{"id":"1569560833","weight":4},{"id":"1569566415","weight":2},{"id":"1569564805","weight":5},{"id":"1569567005","weight":10},{"id":"1569566081","weight":3},{"id":"1569565355","weight":3},{"id":"1569564469","weight":5},{"id":"1569565931","weight":2},{"id":"1569566373","weight":2},{"id":"1569566647","weight":3},{"id":"1569551535","weight":4},{"id":"1569566765","weight":2},{"id":"1569564897","weight":2},{"id":"1569566871","weight":4},{"id":"1569566653","weight":6},{"id":"1569565461","weight":8},{"id":"1569564731","weight":3},{"id":"1569565171","weight":3},{"id":"1569566207","weight":4},{"id":"1569564227","weight":2},{"id":"1569558325","weight":2},{"id":"1569565837","weight":5},{"id":"1569566671","weight":3},{"id":"1569566303","weight":4},{"id":"1569564233","weight":3},{"id":"1569566459","weight":3},{"id":"1569567535","weight":2},{"id":"1569563411","weight":4},{"id":"1569560427","weight":2},{"id":"1569564401","weight":4},{"id":"1569564849","weight":3},{"id":"1569559541","weight":3},{"id":"1569566363","weight":3},{"id":"1569565123","weight":3},{"id":"1569566033","weight":4},{"id":"1569566739","weight":2},{"id":"1569555811","weight":2},{"id":"1569558459","weight":2},{"id":"1569565609","weight":2},{"id":"1569565291","weight":3},{"id":"1569564203","weight":2},{"id":"1569566821","weight":3},{"id":"1569556713","weight":2},{"id":"1569566751","weight":2},{"id":"1569566467","weight":2},{"id":"1569566903","weight":3},{"id":"1569566999","weight":2},{"id":"1569565859","weight":2},{"id":"1569566843","weight":4},{"id":"1569558483","weight":3},{"id":"1569566563","weight":3},{"id":"1569566089","weight":2},{"id":"1569559221","weight":4},{"id":"1569556091","weight":3},{"id":"1569565347","weight":7},{"id":"1569566925","weight":3},{"id":"1569564387","weight":6},{"id":"1569565455","weight":2},{"id":"1569566497","weight":10},{"id":"1569566795","weight":2},{"id":"1569566963","weight":2},{"id":"1569561679","weight":22},{"id":"1569566709","weight":2},{"id":"1569564989","weight":2},{"id":"1569560721","weight":4},{"id":"1569566015","weight":2},{"id":"1569565897","weight":4},{"id":"1569565953","weight":2},{"id":"1569566895","weight":2},{"id":"1569565709","weight":2},{"id":"1569566749","weight":2},{"id":"1569566269","weight":7},{"id":"1569564189","weight":4},{"id":"1569561513","weight":6},{"id":"1569566985","weight":3},{"id":"1569567009","weight":9},{"id":"1569565321","weight":2},{"id":"1569558785","weight":2},{"id":"1569564647","weight":6},{"id":"1569566193","weight":4},{"id":"1569564271","weight":2},{"id":"1569564337","weight":6},{"id":"1569565907","weight":3},{"id":"1569566343","weight":2},{"id":"1569564311","weight":2},{"id":"1569565785","weight":3},{"id":"1569566239","weight":2},{"id":"1569566167","weight":9},{"id":"1569566679","weight":8},{"id":"1569565989","weight":3},{"id":"1569566575","weight":4},{"id":"1569563981","weight":3},{"id":"1569561085","weight":3},{"id":"1569566419","weight":2},{"id":"1569559565","weight":4},{"id":"1569566905","weight":2},{"id":"1569566753","weight":2},{"id":"1569566311","weight":2},{"id":"1569563307","weight":3},{"id":"1569566063","weight":2},{"id":"1569558681","weight":2},{"id":"1569566759","weight":3},{"id":"1569559195","weight":3},{"id":"1569566149","weight":3},{"id":"1569559995","weight":2},{"id":"1569566657","weight":8},{"id":"1569558859","weight":2},{"id":"1569565199","weight":2},{"id":"1569566511","weight":4},{"id":"1569566991","weight":12},{"id":"1569565841","weight":2},{"id":"1569566975","weight":3},{"id":"1569566369","weight":3},{"id":"1569566531","weight":2},{"id":"1569567665","weight":3},{"id":"1569561143","weight":2},{"id":"1569566581","weight":2},{"id":"1569565833","weight":2},{"id":"1569566489","weight":2},{"id":"1569564611","weight":2},{"id":"1569565535","weight":3},{"id":"1569562867","weight":6},{"id":"1569565667","weight":2},{"id":"1569561795","weight":2},{"id":"1569566845","weight":8},{"id":"1569566325","weight":2},{"id":"1569565257","weight":2},{"id":"1569564795","weight":4},{"id":"1569567015","weight":4},{"id":"1569559805","weight":4},{"id":"1569566437","weight":3},{"id":"1569565735","weight":3},{"id":"1569553909","weight":2},{"id":"1569559111","weight":2},{"id":"1569566687","weight":2},{"id":"1569562285","weight":4},{"id":"1569564881","weight":2},{"id":"1569566939","weight":3},{"id":"1569553537","weight":3},{"id":"1569565427","weight":9},{"id":"1569566403","weight":3},{"id":"1569565839","weight":2},{"id":"1569552251","weight":3},{"id":"1569566139","weight":3},{"id":"1569553519","weight":2},{"id":"1569567051","weight":4},{"id":"1569566885","weight":3},{"id":"1569564441","weight":3},{"id":"1569566231","weight":2},{"id":"1569564209","weight":2},{"id":"1569566513","weight":2},{"id":"1569554881","weight":2},{"id":"1569554971","weight":5},{"id":"1569565501","weight":2},{"id":"1569566899","weight":3},{"id":"1569566445","weight":2},{"id":"1569566209","weight":3},{"id":"1569566649","weight":2},{"id":"1569566791","weight":2},{"id":"1569565559","weight":13},{"id":"1569566371","weight":2},{"id":"1569565655","weight":2},{"id":"1569566127","weight":2},{"id":"1569558985","weight":3},{"id":"1569563763","weight":3},{"id":"1569566473","weight":4},{"id":"1569564857","weight":2},{"id":"1569564333","weight":2},{"id":"1569566913","weight":11},{"id":"1569566809","weight":3},{"id":"1569566629","weight":2},{"id":"1569566257","weight":3},{"id":"1569565033","weight":3},{"id":"1569566447","weight":5},{"id":"1569565847","weight":6},{"id":"1569564353","weight":4},{"id":"1569557083","weight":2},{"id":"1569565929","weight":3},{"id":"1569566141","weight":4},{"id":"1569566721","weight":2},{"id":"1569563231","weight":3},{"id":"1569565633","weight":2},{"id":"1569566661","weight":2},{"id":"1569565279","weight":2},{"id":"1569566115","weight":4},{"id":"1569565219","weight":2},{"id":"1569558509","weight":2},{"id":"1569564851","weight":2},{"id":"1569565595","weight":4},{"id":"1569565185","weight":4},{"id":"1569566773","weight":5},{"id":"1569564985","weight":2},{"id":"1569566223","weight":2},{"id":"1569566553","weight":2},{"id":"1569564969","weight":2},{"id":"1569565029","weight":3},{"id":"1569566505","weight":3},{"id":"1569565393","weight":2},{"id":"1569565933","weight":5},{"id":"1569562207","weight":2},{"id":"1569565705","weight":2},{"id":"1569566191","weight":3},{"id":"1569567033","weight":4},{"id":"1569565527","weight":5},{"id":"1569566853","weight":2},{"id":"1569566603","weight":3},{"id":"1569567029","weight":2},{"id":"1569565363","weight":4},{"id":"1569566159","weight":2},{"id":"1569566695","weight":2},{"id":"1569566051","weight":2},{"id":"1569561123","weight":3},{"id":"1569566655","weight":2},{"id":"1569566673","weight":3},{"id":"1569567235","weight":3},{"id":"1569565441","weight":3},{"id":"1569565311","weight":2},{"id":"1569566233","weight":2},{"id":"1569566667","weight":2},{"id":"1569566893","weight":3},{"id":"1569566317","weight":3},{"id":"1569564097","weight":2},{"id":"1569560997","weight":3},{"id":"1569563845","weight":2},{"id":"1569565741","weight":3},{"id":"1569566275","weight":4},{"id":"1569566481","weight":2},{"id":"1569565545","weight":8},{"id":"1569566857","weight":7},{"id":"1569565961","weight":2},{"id":"1569566387","weight":2},{"id":"1569566245","weight":4},{"id":"1569560503","weight":3},{"id":"1569565463","weight":2},{"id":"1569564339","weight":2},{"id":"1569565439","weight":2},{"id":"1569566229","weight":3},{"id":"1569566133","weight":3},{"id":"1569562551","weight":2},{"id":"1569566901","weight":2},{"id":"1569551347","weight":2},{"id":"1569565415","weight":2},{"id":"1569561623","weight":4},{"id":"1569566383","weight":3},{"id":"1569564485","weight":2},{"id":"1569566631","weight":3},{"id":"1569565571","weight":3},{"id":"1569565885","weight":2},{"id":"1569566177","weight":4},{"id":"1569565493","weight":2},{"id":"1569557633","weight":3},{"id":"1569564411","weight":2},{"id":"1569566805","weight":2},{"id":"1569559199","weight":2},{"id":"1569566293","weight":3},{"id":"1569565665","weight":3},{"id":"1569566831","weight":3},{"id":"1569565549","weight":2},{"id":"1569565523","weight":2},{"id":"1569565611","weight":2},{"id":"1569557715","weight":3},{"id":"1569564175","weight":4},{"id":"1569566983","weight":8},{"id":"1569566097","weight":4},{"id":"1569566479","weight":2},{"id":"1569556361","weight":5},{"id":"1569566431","weight":2},{"id":"1569565397","weight":3},{"id":"1569566873","weight":8},{"id":"1569565765","weight":3},{"id":"1569565435","weight":6},{"id":"1569557275","weight":2},{"id":"1569565263","weight":2},{"id":"1569565385","weight":3},{"id":"1569565575","weight":3},{"id":"1569565919","weight":3},{"id":"1569565181","weight":4},{"id":"1569566711","weight":3},{"id":"1569565241","weight":3},{"id":"1569566927","weight":2},{"id":"1569565661","weight":2},{"id":"1569566887","weight":3},{"id":"1569565273","weight":3},{"id":"1569564131","weight":2},{"id":"1569552037","weight":5},{"id":"1569564919","weight":3},{"id":"1569566737","weight":3},{"id":"1569566429","weight":4},{"id":"1569561221","weight":2},{"id":"1569566917","weight":5},{"id":"1569566253","weight":2},{"id":"1569565353","weight":7},{"id":"1569564683","weight":3},{"id":"1569564305","weight":2},{"id":"1569564283","weight":4},{"id":"1569566691","weight":2},{"id":"1569565421","weight":5},{"id":"1569566547","weight":3},{"id":"1569566651","weight":4},{"id":"1569566823","weight":2},{"id":"1569566595","weight":4},{"id":"1569566677","weight":2},{"id":"1569565349","weight":2},{"id":"1569552025","weight":6},{"id":"1569565013","weight":2},{"id":"1569566237","weight":3},{"id":"1569566283","weight":2},{"id":"1569566529","weight":3},{"id":"1569565375","weight":2},{"id":"1569566715","weight":10},{"id":"1569565237","weight":3},{"id":"1569566639","weight":5},{"id":"1569566755","weight":13},{"id":"1569566819","weight":3},{"id":"1569565041","weight":7},{"id":"1569564703","weight":10},{"id":"1569565541","weight":2},{"id":"1569566813","weight":3},{"id":"1569566771","weight":2},{"id":"1569562277","weight":3},{"id":"1569566641","weight":4},{"id":"1569565425","weight":14},{"id":"1569564437","weight":3},{"id":"1569566533","weight":2},{"id":"1569551905","weight":2},{"id":"1569564861","weight":5},{"id":"1569565457","weight":3},{"id":"1569564787","weight":5},{"id":"1569566487","weight":4},{"id":"1569565529","weight":8},{"id":"1569556759","weight":2},{"id":"1569566619","weight":3},{"id":"1569565271","weight":2},{"id":"1569561185","weight":3},{"id":"1569566397","weight":2},{"id":"1569565669","weight":2},{"id":"1569563721","weight":2},{"id":"1569566001","weight":7},{"id":"1569565593","weight":3},{"id":"1569560235","weight":2},{"id":"1569566817","weight":2},{"id":"1569564157","weight":2},{"id":"1569566389","weight":2},{"id":"1569566435","weight":3},{"id":"1569567483","weight":3},{"id":"1569565367","weight":2},{"id":"1569566299","weight":2},{"id":"1569564281","weight":3},{"id":"1569565769","weight":2},{"id":"1569566171","weight":4},{"id":"1569565805","weight":3},{"id":"1569561713","weight":2},{"id":"1569566933","weight":3},{"id":"1569563919","weight":3},{"id":"1569567691","weight":11},{"id":"1569565389","weight":4},{"id":"1569559919","weight":4},{"id":"1569565861","weight":5},{"id":"1569566147","weight":12},{"id":"1569565537","weight":2},{"id":"1569566057","weight":3},{"id":"1569562367","weight":4},{"id":"1569565561","weight":2},{"id":"1569560213","weight":3},{"id":"1569555891","weight":2},{"id":"1569566847","weight":2},{"id":"1569565997","weight":3},{"id":"1569563425","weight":4},{"id":"1569565035","weight":3},{"id":"1569559597","weight":6},{"id":"1569564961","weight":5},{"id":"1569559251","weight":2},{"id":"1569567013","weight":2},{"id":"1569566583","weight":2},{"id":"1569565337","weight":2},{"id":"1569565737","weight":3},{"id":"1569560459","weight":2},{"id":"1569564463","weight":2},{"id":"1569565853","weight":7},{"id":"1569550425","weight":4},{"id":"1569566273","weight":3},{"id":"1569564123","weight":5},{"id":"1569566341","weight":2},{"id":"1569565889","weight":3},{"id":"1569566635","weight":4},{"id":"1569566611","weight":13},{"id":"1569563725","weight":5},{"id":"1569551539","weight":2},{"id":"1569564505","weight":3},{"id":"1569565565","weight":3},{"id":"1569565635","weight":3},{"id":"1569561397","weight":2},{"id":"1569565731","weight":2},{"id":"1569556327","weight":2},{"id":"1569566797","weight":7},{"id":"1569566125","weight":8},{"id":"1569566413","weight":2},{"id":"1569565707","weight":5},{"id":"1569566375","weight":4},{"id":"1569564257","weight":3},{"id":"1569565583","weight":2},{"id":"1569566555","weight":2},{"id":"1569566973","weight":6},{"id":"1569561579","weight":3},{"id":"1569566449","weight":2},{"id":"1569566987","weight":2},{"id":"1569565031","weight":2},{"id":"1569564755","weight":3},{"id":"1569551541","weight":3},{"id":"1569566839","weight":4},{"id":"1569565139","weight":3},{"id":"1569566663","weight":3},{"id":"1569565579","weight":2},{"id":"1569566067","weight":4},{"id":"1569566825","weight":6},{"id":"1569566615","weight":2},{"id":"1569566241","weight":3},{"id":"1569564807","weight":3},{"id":"1569563007","weight":2},{"id":"1569566113","weight":2},{"id":"1569566443","weight":19},{"id":"1569566727","weight":3},{"id":"1569565315","weight":2},{"id":"1569560581","weight":3},{"id":"1569559233","weight":2}],"meta":{"jsonClass":"HashMap$HashTrieMap","sessionid":"S12.T9.2","endtime":"12:10","authors":"Anastasios Kyrillidis, Volkan Cevher","date":"1341489000000","papertitle":"Combinatorial Selection and Least Absolute Shrinkage via the Clash Algorithm","starttime":"11:50","session":"S12.T9: L1-Regularized Least Squares and Sparsity","room":"Stratton West Lounge (201)","paperid":"1569565227"},"cluster":{"jsonClass":"HashMap$HashTrieMap","spectral6":"3","spectral43":"24","spectral28":"12","spectral32":"16","spectral14":"4","spectral20":"0","spectral9":"6","spectral25":"2","spectral42":"34","spectral3":"1","spectral47":"8","spectral17":"16","louvain":"250","spectral36":"4","spectral39":"33","spectral10":"4","spectral15":"12","spectral33":"10","spectral5":"2","spectral21":"13","spectral44":"11","spectral26":"22","spectral40":"20","spectral8":"1","spectral11":"4","spectral4":"3","spectral37":"35","spectral48":"3","spectral22":"12","spectral23":"1","spectral12":"9","spectral50":"9","spectral19":"0","spectral34":"4","spectral45":"42","spectral7":"5","spectral49":"1","spectral38":"34","spectral24":"3","spectral13":"7","spectral31":"22","spectral29":"5","spectral35":"19","spectral30":"27","spectral41":"7","spectral27":"15","spectral18":"4","spectral46":"34","spectral2":"0","spectral16":"2"}}
